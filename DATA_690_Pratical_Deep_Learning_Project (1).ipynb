{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA 690 Pratical Deep Learning Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGvtj3l2Jg9I",
        "outputId": "6a4951ff-5aab-4aa9-fb94-73e28ddc3087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision \n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import torchtext.data as textdata\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import torchtext\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import time\n",
        "import sys, os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7ACiPY6TxVT"
      },
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIc3v6_0Ub3v"
      },
      "source": [
        "#HYPERPARAMETERS\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr = 2e-4,weight_decay=1e-5)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "vocab_Max = 50000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCo9B05PJhGN",
        "outputId": "481487b1-8738-4a91-ac81-2acb0f2ff4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7nIYM5XLSra",
        "outputId": "135a2a95-676d-4cd3-b970-0d215bb5d9c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fwOwE_WEJsj"
      },
      "source": [
        "New_Process_Data = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjh7_9F2LtGu"
      },
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "#test_df = pd.read_csv('test.csv')\n",
        "#test_labels_df = pd.read_csv('test_labels.csv')\n",
        "process_train_path = 'processtrain.csv'\n",
        "process_test_path = 'processtest.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH7sd8EyTgZG"
      },
      "source": [
        "#print('Null values in training set?')\n",
        "#print(df.isnull().sum())\n",
        "#print('')\n",
        "#print('Null values in test set?')\n",
        "#print(test_df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T49c8teyV-xo"
      },
      "source": [
        "#df['toxic'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdvQKNQ9Wm6d"
      },
      "source": [
        "#df['obscene'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UKdDysjWmV3"
      },
      "source": [
        "#df['comment_text'][36]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3nvCLyNApIW"
      },
      "source": [
        "#Inspiration for this function gotten from https://www.kaggle.com/prabhatkumarsahu/toxic-comment-classification-thecaffeinedev\n",
        "punctuations = string.punctuation\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "spacy_tokenizer = torchtext.data.utils.get_tokenizer('spacy')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def processing(text):\n",
        "  \n",
        "    def tokenizer(text):\n",
        "        text = str.split(text)\n",
        "        return text\n",
        "    \n",
        "    def remove_punctuations(sentence):\n",
        "        result = \"\".join([i if i not in punctuations and not i.isdigit() else \" \" for i in sentence])\n",
        "        return result\n",
        "    \n",
        "    def word_lemmatizer(sentence):\n",
        "        result = lemmatizer.lemmatize(sentence)\n",
        "        return result\n",
        "    \n",
        "    def word_lowercase(sentence):\n",
        "        return sentence.lower()\n",
        "    \n",
        "    def remove_URL(text):\n",
        "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        html = re.compile(r'<.*?>')\n",
        "        text = html.sub(r'',text)\n",
        "        text = url.sub(r'',str(text))\n",
        "        return text\n",
        "  \n",
        "    def remove_newline(text):\n",
        "        return text.rstrip(\"\\n\")\n",
        "    \n",
        "    def clean_comment(sentence):\n",
        "        result = []\n",
        "        sentence = remove_newline(sentence)\n",
        "        sentence = remove_URL(sentence)\n",
        "        sentence = word_lowercase(sentence)\n",
        "        sentence = word_lemmatizer(sentence)\n",
        "        sentence = remove_punctuations(sentence)\n",
        "        sentence = tokenizer(sentence)\n",
        "\n",
        "        result = \" \".join(sentence)\n",
        "        return result\n",
        "     \n",
        "    text = clean_comment(text)\n",
        "    if text == \"\":\n",
        "        text = \"None\"\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcE7a7tRD_z1",
        "outputId": "ef2d324c-f41d-4309-8475-1196037b351f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "if New_Process_Data:\n",
        "    with open('train.csv', \"r\", encoding=\"utf8\") as in_csv, open('processtrain.csv', \"w\", newline=\"\", encoding=\"utf8\") as out_csv:\n",
        "        read = csv.reader(in_csv)\n",
        "        write = csv.writer(out_csv)\n",
        "        next(read, None) # Skip header\n",
        "        for i in tqdm(read):\n",
        "            i[1] = processing(i[1])\n",
        "            try:\n",
        "                write.writerow(i)\n",
        "            except Exception as e:\n",
        "                print(e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159571it [00:14, 11045.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KCbDRqtH_Bj",
        "outputId": "eda6af40-6d75-4e8f-f948-28174a5f061e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "if New_Process_Data:\n",
        "    with open('test.csv', \"r\", encoding=\"utf8\") as in_csv, open('processtest.csv', \"w\", newline=\"\", encoding=\"utf8\") as out_csv:\n",
        "        read = csv.reader(in_csv)\n",
        "        write = csv.writer(out_csv)\n",
        "        next(read, None) # Skip header\n",
        "        for i in tqdm(read):\n",
        "            i[1] = processing(i[1])\n",
        "            try:\n",
        "                write.writerow(i)\n",
        "            except Exception as e:\n",
        "               print(e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "153164it [00:13, 11681.65it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBSzy8hsI33a"
      },
      "source": [
        "#df['comment_text'] = df['comment_text'].apply(processing)\n",
        "\n",
        "#test_df['comment_text'] = test_df['comment_text'].apply(processing)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYUrkktOJI3Y",
        "outputId": "4de614ff-9be8-4255-a332-bd2f107cfcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(df['comment_text'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explanation\n",
            "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZgjB1Dj7Mg_"
      },
      "source": [
        "#df.to_csv('traindata.csv')\n",
        "#test_df.to_csv('testdata.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOz90-FC07yD"
      },
      "source": [
        "import torchtext.data as textdata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_-sRweMbZTK"
      },
      "source": [
        "text = textdata.Field(batch_first = True,\n",
        "                  tokenize = spacy_tokenizer,\n",
        "                  stop_words = stopwords_list)\n",
        "\n",
        "label = textdata.LabelField(dtype = torch.float)\n",
        "\n",
        "id1 = textdata.LabelField()\n",
        "\n",
        "id2 = textdata.Field(sequential=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY8_jFFscV0w"
      },
      "source": [
        "Features = [[\"id\",id1], [\"text\", text], [\"toxic\",label],[\"severe_toxic\",label],\n",
        "          [\"obscene\",label],[\"threat\",label],[\"insult\",label],[\"identity_hate\",label]]\n",
        "\n",
        "Test_Features = [[\"id\",id2], [\"text\", text]]\n",
        "\n",
        "train_data = textdata.TabularDataset('processtrain.csv', format = 'csv', fields=Features, skip_header=True)\n",
        "\n",
        "test_data = textdata.TabularDataset('processtest.csv', format = 'csv', fields=Test_Features, skip_header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlI7PHJSerIT"
      },
      "source": [
        "train_data, val_data = train_data.split(split_ratio=0.75,random_state=random.seed(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82p-MJ_JxmiC"
      },
      "source": [
        "#Had to try different petrained vectors, as I kept getting weird HTTP errors with some of them.\n",
        "text.build_vocab(train_data,\n",
        "                 min_freq = 3,\n",
        "                 max_size = vocab_Max,\n",
        "                 vectors = \"glove.6B.50d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "label.build_vocab(train_data)\n",
        "\n",
        "id1.build_vocab(train_data)\n",
        "id2.build_vocab(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46vAV-lZ0SFR",
        "outputId": "77ead520-0427-4d87-9517-5ea49dfe14fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "vocab_list = (text.vocab.freqs.most_common(10))\n",
        "print('The 10 most common words in my vocabulary are: ')\n",
        "for i in range(len(vocab_list)):\n",
        "  print(str(i+1), \": '\", vocab_list[i][0],\"'.\", \"It occured \", vocab_list[i][1], \" times!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 10 most common words in my vocabulary are: \n",
            "1 : ' article '. It occured  43118  times!\n",
            "2 : ' page '. It occured  34858  times!\n",
            "3 : ' wikipedia '. It occured  34773  times!\n",
            "4 : ' talk '. It occured  28026  times!\n",
            "5 : ' please '. It occured  22280  times!\n",
            "6 : ' would '. It occured  21951  times!\n",
            "7 : ' one '. It occured  21849  times!\n",
            "8 : ' like '. It occured  20953  times!\n",
            "9 : ' see '. It occured  16164  times!\n",
            "10 : ' also '. It occured  15563  times!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnvFyMzC3Wrv",
        "outputId": "2b97f753-23fc-40de-f9c6-4a39e06824b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"The number of unique words in the training set is: \", str(len(text.vocab)))\n",
        "print(\"The number of unique labels in the training set is: \", str(len(label.vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of unique words in the training set is:  50002\n",
            "The number of unique labels in the training set is:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk8-AYG_CLTp"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_i, val_i = textdata.BucketIterator.splits((train_data, val_data),\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  device = device)\n",
        "\n",
        "test_i = textdata.BucketIterator(test_data,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False,\n",
        "                                device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HObUKU36NlJj"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout,pad_idx):\n",
        "        super(ConvNet, self).__init__() \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1, out_channels = n_filters, kernel_size = (fs, embed_dim)) for fs in filter_sizes])\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.embedding(text)          \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]       \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUllaI0HQxbq",
        "outputId": "825b1208-9928-490c-c96f-156d0e2e6f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "input_dim = len(text.vocab)\n",
        "embed_dim = 100\n",
        "n_filters = 32\n",
        "filter_sizes = [2,3,4]\n",
        "#For the 6 labels we have.\n",
        "output_dim = 6\n",
        "dropout = 0.5\n",
        "pad_idx = text.vocab.stoi[text.pad_token]\n",
        "#UNK_IDX = text.vocab.stoi[text.unk_token]\n",
        "\n",
        "model = ConvNet(input_dim, embed_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx)\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 2e-5, weight_decay=1e-4)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet(\n",
            "  (embedding): Embedding(50002, 100)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv2d(1, 32, kernel_size=(2, 100), stride=(1, 1))\n",
            "    (1): Conv2d(1, 32, kernel_size=(3, 100), stride=(1, 1))\n",
            "    (2): Conv2d(1, 32, kernel_size=(4, 100), stride=(1, 1))\n",
            "  )\n",
            "  (fc): Linear(in_features=96, out_features=6, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1qTTLuIScMr"
      },
      "source": [
        "def get_label(batch):\n",
        "    toxic = batch.toxic.unsqueeze(1)\n",
        "    severe_toxic = batch.severe_toxic.unsqueeze(1)\n",
        "    obscene = batch.obscene.unsqueeze(1)\n",
        "    threat = batch.threat.unsqueeze(1)\n",
        "    insult = batch.insult.unsqueeze(1)\n",
        "    identity_hate = batch.identity_hate.unsqueeze(1)\n",
        "    labels = torch.cat((toxic,severe_toxic,obscene,\n",
        "                        threat,insult,identity_hate),dim=1)\n",
        "    return labels\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_pace(model, optimizer, criterion, batch):\n",
        "    batch_size = len(batch)\n",
        "    model.train()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    text = batch.text.view(batch_size, -1)\n",
        "    labels = get_label(batch)\n",
        "\n",
        "    outputs = model(text)\n",
        "    loss = criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU9oeo1dTO_K",
        "outputId": "e9ca531a-9d33-44d5-cb54-eb25bfc584ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#To decide if you want to retrain the model or load a previously saved version.\n",
        "train_model = True\n",
        "batch_size = 128\n",
        "if train_model:\n",
        "    EPOCHS = 2\n",
        "    loss_list = []\n",
        "    print(\"Training now...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        for i, batch in enumerate(train_i):\n",
        "            train_loss = train_pace(model,optimizer, criterion, batch)\n",
        "            loss_list.append(train_loss)\n",
        "            print(f\"Epoch: [{epoch+1}/{EPOCHS}] | Iterations: [{i+1}/{len(train_i)}] | Training loss: {train_loss:.3f}\")\n",
        "    torch.save(model.state_dict(), \"modelConvNet.pt\")\n",
        "    print(\"Training done.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training now...\n",
            "Epoch: [1/2] | Iterations: [1/1870] | Training loss: 0.687\n",
            "Epoch: [1/2] | Iterations: [2/1870] | Training loss: 0.712\n",
            "Epoch: [1/2] | Iterations: [3/1870] | Training loss: 0.691\n",
            "Epoch: [1/2] | Iterations: [4/1870] | Training loss: 0.691\n",
            "Epoch: [1/2] | Iterations: [5/1870] | Training loss: 0.703\n",
            "Epoch: [1/2] | Iterations: [6/1870] | Training loss: 0.658\n",
            "Epoch: [1/2] | Iterations: [7/1870] | Training loss: 0.641\n",
            "Epoch: [1/2] | Iterations: [8/1870] | Training loss: 0.680\n",
            "Epoch: [1/2] | Iterations: [9/1870] | Training loss: 0.719\n",
            "Epoch: [1/2] | Iterations: [10/1870] | Training loss: 0.709\n",
            "Epoch: [1/2] | Iterations: [11/1870] | Training loss: 0.656\n",
            "Epoch: [1/2] | Iterations: [12/1870] | Training loss: 0.691\n",
            "Epoch: [1/2] | Iterations: [13/1870] | Training loss: 0.679\n",
            "Epoch: [1/2] | Iterations: [14/1870] | Training loss: 0.699\n",
            "Epoch: [1/2] | Iterations: [15/1870] | Training loss: 0.647\n",
            "Epoch: [1/2] | Iterations: [16/1870] | Training loss: 0.664\n",
            "Epoch: [1/2] | Iterations: [17/1870] | Training loss: 0.708\n",
            "Epoch: [1/2] | Iterations: [18/1870] | Training loss: 0.676\n",
            "Epoch: [1/2] | Iterations: [19/1870] | Training loss: 0.703\n",
            "Epoch: [1/2] | Iterations: [20/1870] | Training loss: 0.682\n",
            "Epoch: [1/2] | Iterations: [21/1870] | Training loss: 0.662\n",
            "Epoch: [1/2] | Iterations: [22/1870] | Training loss: 0.658\n",
            "Epoch: [1/2] | Iterations: [23/1870] | Training loss: 0.662\n",
            "Epoch: [1/2] | Iterations: [24/1870] | Training loss: 0.677\n",
            "Epoch: [1/2] | Iterations: [25/1870] | Training loss: 0.660\n",
            "Epoch: [1/2] | Iterations: [26/1870] | Training loss: 0.678\n",
            "Epoch: [1/2] | Iterations: [27/1870] | Training loss: 0.672\n",
            "Epoch: [1/2] | Iterations: [28/1870] | Training loss: 0.674\n",
            "Epoch: [1/2] | Iterations: [29/1870] | Training loss: 0.619\n",
            "Epoch: [1/2] | Iterations: [30/1870] | Training loss: 0.624\n",
            "Epoch: [1/2] | Iterations: [31/1870] | Training loss: 0.682\n",
            "Epoch: [1/2] | Iterations: [32/1870] | Training loss: 0.659\n",
            "Epoch: [1/2] | Iterations: [33/1870] | Training loss: 0.614\n",
            "Epoch: [1/2] | Iterations: [34/1870] | Training loss: 0.648\n",
            "Epoch: [1/2] | Iterations: [35/1870] | Training loss: 0.669\n",
            "Epoch: [1/2] | Iterations: [36/1870] | Training loss: 0.639\n",
            "Epoch: [1/2] | Iterations: [37/1870] | Training loss: 0.641\n",
            "Epoch: [1/2] | Iterations: [38/1870] | Training loss: 0.625\n",
            "Epoch: [1/2] | Iterations: [39/1870] | Training loss: 0.650\n",
            "Epoch: [1/2] | Iterations: [40/1870] | Training loss: 0.630\n",
            "Epoch: [1/2] | Iterations: [41/1870] | Training loss: 0.608\n",
            "Epoch: [1/2] | Iterations: [42/1870] | Training loss: 0.633\n",
            "Epoch: [1/2] | Iterations: [43/1870] | Training loss: 0.618\n",
            "Epoch: [1/2] | Iterations: [44/1870] | Training loss: 0.617\n",
            "Epoch: [1/2] | Iterations: [45/1870] | Training loss: 0.632\n",
            "Epoch: [1/2] | Iterations: [46/1870] | Training loss: 0.644\n",
            "Epoch: [1/2] | Iterations: [47/1870] | Training loss: 0.629\n",
            "Epoch: [1/2] | Iterations: [48/1870] | Training loss: 0.657\n",
            "Epoch: [1/2] | Iterations: [49/1870] | Training loss: 0.615\n",
            "Epoch: [1/2] | Iterations: [50/1870] | Training loss: 0.649\n",
            "Epoch: [1/2] | Iterations: [51/1870] | Training loss: 0.622\n",
            "Epoch: [1/2] | Iterations: [52/1870] | Training loss: 0.625\n",
            "Epoch: [1/2] | Iterations: [53/1870] | Training loss: 0.613\n",
            "Epoch: [1/2] | Iterations: [54/1870] | Training loss: 0.661\n",
            "Epoch: [1/2] | Iterations: [55/1870] | Training loss: 0.592\n",
            "Epoch: [1/2] | Iterations: [56/1870] | Training loss: 0.618\n",
            "Epoch: [1/2] | Iterations: [57/1870] | Training loss: 0.639\n",
            "Epoch: [1/2] | Iterations: [58/1870] | Training loss: 0.611\n",
            "Epoch: [1/2] | Iterations: [59/1870] | Training loss: 0.635\n",
            "Epoch: [1/2] | Iterations: [60/1870] | Training loss: 0.622\n",
            "Epoch: [1/2] | Iterations: [61/1870] | Training loss: 0.608\n",
            "Epoch: [1/2] | Iterations: [62/1870] | Training loss: 0.615\n",
            "Epoch: [1/2] | Iterations: [63/1870] | Training loss: 0.565\n",
            "Epoch: [1/2] | Iterations: [64/1870] | Training loss: 0.604\n",
            "Epoch: [1/2] | Iterations: [65/1870] | Training loss: 0.664\n",
            "Epoch: [1/2] | Iterations: [66/1870] | Training loss: 0.603\n",
            "Epoch: [1/2] | Iterations: [67/1870] | Training loss: 0.642\n",
            "Epoch: [1/2] | Iterations: [68/1870] | Training loss: 0.610\n",
            "Epoch: [1/2] | Iterations: [69/1870] | Training loss: 0.593\n",
            "Epoch: [1/2] | Iterations: [70/1870] | Training loss: 0.561\n",
            "Epoch: [1/2] | Iterations: [71/1870] | Training loss: 0.619\n",
            "Epoch: [1/2] | Iterations: [72/1870] | Training loss: 0.592\n",
            "Epoch: [1/2] | Iterations: [73/1870] | Training loss: 0.596\n",
            "Epoch: [1/2] | Iterations: [74/1870] | Training loss: 0.576\n",
            "Epoch: [1/2] | Iterations: [75/1870] | Training loss: 0.586\n",
            "Epoch: [1/2] | Iterations: [76/1870] | Training loss: 0.567\n",
            "Epoch: [1/2] | Iterations: [77/1870] | Training loss: 0.595\n",
            "Epoch: [1/2] | Iterations: [78/1870] | Training loss: 0.599\n",
            "Epoch: [1/2] | Iterations: [79/1870] | Training loss: 0.601\n",
            "Epoch: [1/2] | Iterations: [80/1870] | Training loss: 0.597\n",
            "Epoch: [1/2] | Iterations: [81/1870] | Training loss: 0.582\n",
            "Epoch: [1/2] | Iterations: [82/1870] | Training loss: 0.589\n",
            "Epoch: [1/2] | Iterations: [83/1870] | Training loss: 0.570\n",
            "Epoch: [1/2] | Iterations: [84/1870] | Training loss: 0.593\n",
            "Epoch: [1/2] | Iterations: [85/1870] | Training loss: 0.616\n",
            "Epoch: [1/2] | Iterations: [86/1870] | Training loss: 0.605\n",
            "Epoch: [1/2] | Iterations: [87/1870] | Training loss: 0.563\n",
            "Epoch: [1/2] | Iterations: [88/1870] | Training loss: 0.567\n",
            "Epoch: [1/2] | Iterations: [89/1870] | Training loss: 0.608\n",
            "Epoch: [1/2] | Iterations: [90/1870] | Training loss: 0.577\n",
            "Epoch: [1/2] | Iterations: [91/1870] | Training loss: 0.591\n",
            "Epoch: [1/2] | Iterations: [92/1870] | Training loss: 0.556\n",
            "Epoch: [1/2] | Iterations: [93/1870] | Training loss: 0.599\n",
            "Epoch: [1/2] | Iterations: [94/1870] | Training loss: 0.575\n",
            "Epoch: [1/2] | Iterations: [95/1870] | Training loss: 0.628\n",
            "Epoch: [1/2] | Iterations: [96/1870] | Training loss: 0.579\n",
            "Epoch: [1/2] | Iterations: [97/1870] | Training loss: 0.607\n",
            "Epoch: [1/2] | Iterations: [98/1870] | Training loss: 0.555\n",
            "Epoch: [1/2] | Iterations: [99/1870] | Training loss: 0.624\n",
            "Epoch: [1/2] | Iterations: [100/1870] | Training loss: 0.589\n",
            "Epoch: [1/2] | Iterations: [101/1870] | Training loss: 0.550\n",
            "Epoch: [1/2] | Iterations: [102/1870] | Training loss: 0.580\n",
            "Epoch: [1/2] | Iterations: [103/1870] | Training loss: 0.566\n",
            "Epoch: [1/2] | Iterations: [104/1870] | Training loss: 0.555\n",
            "Epoch: [1/2] | Iterations: [105/1870] | Training loss: 0.552\n",
            "Epoch: [1/2] | Iterations: [106/1870] | Training loss: 0.544\n",
            "Epoch: [1/2] | Iterations: [107/1870] | Training loss: 0.573\n",
            "Epoch: [1/2] | Iterations: [108/1870] | Training loss: 0.526\n",
            "Epoch: [1/2] | Iterations: [109/1870] | Training loss: 0.543\n",
            "Epoch: [1/2] | Iterations: [110/1870] | Training loss: 0.514\n",
            "Epoch: [1/2] | Iterations: [111/1870] | Training loss: 0.542\n",
            "Epoch: [1/2] | Iterations: [112/1870] | Training loss: 0.532\n",
            "Epoch: [1/2] | Iterations: [113/1870] | Training loss: 0.542\n",
            "Epoch: [1/2] | Iterations: [114/1870] | Training loss: 0.538\n",
            "Epoch: [1/2] | Iterations: [115/1870] | Training loss: 0.542\n",
            "Epoch: [1/2] | Iterations: [116/1870] | Training loss: 0.549\n",
            "Epoch: [1/2] | Iterations: [117/1870] | Training loss: 0.550\n",
            "Epoch: [1/2] | Iterations: [118/1870] | Training loss: 0.540\n",
            "Epoch: [1/2] | Iterations: [119/1870] | Training loss: 0.513\n",
            "Epoch: [1/2] | Iterations: [120/1870] | Training loss: 0.538\n",
            "Epoch: [1/2] | Iterations: [121/1870] | Training loss: 0.528\n",
            "Epoch: [1/2] | Iterations: [122/1870] | Training loss: 0.566\n",
            "Epoch: [1/2] | Iterations: [123/1870] | Training loss: 0.538\n",
            "Epoch: [1/2] | Iterations: [124/1870] | Training loss: 0.538\n",
            "Epoch: [1/2] | Iterations: [125/1870] | Training loss: 0.539\n",
            "Epoch: [1/2] | Iterations: [126/1870] | Training loss: 0.545\n",
            "Epoch: [1/2] | Iterations: [127/1870] | Training loss: 0.548\n",
            "Epoch: [1/2] | Iterations: [128/1870] | Training loss: 0.537\n",
            "Epoch: [1/2] | Iterations: [129/1870] | Training loss: 0.545\n",
            "Epoch: [1/2] | Iterations: [130/1870] | Training loss: 0.546\n",
            "Epoch: [1/2] | Iterations: [131/1870] | Training loss: 0.527\n",
            "Epoch: [1/2] | Iterations: [132/1870] | Training loss: 0.503\n",
            "Epoch: [1/2] | Iterations: [133/1870] | Training loss: 0.556\n",
            "Epoch: [1/2] | Iterations: [134/1870] | Training loss: 0.501\n",
            "Epoch: [1/2] | Iterations: [135/1870] | Training loss: 0.476\n",
            "Epoch: [1/2] | Iterations: [136/1870] | Training loss: 0.488\n",
            "Epoch: [1/2] | Iterations: [137/1870] | Training loss: 0.493\n",
            "Epoch: [1/2] | Iterations: [138/1870] | Training loss: 0.475\n",
            "Epoch: [1/2] | Iterations: [139/1870] | Training loss: 0.503\n",
            "Epoch: [1/2] | Iterations: [140/1870] | Training loss: 0.525\n",
            "Epoch: [1/2] | Iterations: [141/1870] | Training loss: 0.513\n",
            "Epoch: [1/2] | Iterations: [142/1870] | Training loss: 0.529\n",
            "Epoch: [1/2] | Iterations: [143/1870] | Training loss: 0.523\n",
            "Epoch: [1/2] | Iterations: [144/1870] | Training loss: 0.486\n",
            "Epoch: [1/2] | Iterations: [145/1870] | Training loss: 0.525\n",
            "Epoch: [1/2] | Iterations: [146/1870] | Training loss: 0.489\n",
            "Epoch: [1/2] | Iterations: [147/1870] | Training loss: 0.510\n",
            "Epoch: [1/2] | Iterations: [148/1870] | Training loss: 0.515\n",
            "Epoch: [1/2] | Iterations: [149/1870] | Training loss: 0.502\n",
            "Epoch: [1/2] | Iterations: [150/1870] | Training loss: 0.472\n",
            "Epoch: [1/2] | Iterations: [151/1870] | Training loss: 0.502\n",
            "Epoch: [1/2] | Iterations: [152/1870] | Training loss: 0.496\n",
            "Epoch: [1/2] | Iterations: [153/1870] | Training loss: 0.508\n",
            "Epoch: [1/2] | Iterations: [154/1870] | Training loss: 0.524\n",
            "Epoch: [1/2] | Iterations: [155/1870] | Training loss: 0.493\n",
            "Epoch: [1/2] | Iterations: [156/1870] | Training loss: 0.508\n",
            "Epoch: [1/2] | Iterations: [157/1870] | Training loss: 0.509\n",
            "Epoch: [1/2] | Iterations: [158/1870] | Training loss: 0.515\n",
            "Epoch: [1/2] | Iterations: [159/1870] | Training loss: 0.510\n",
            "Epoch: [1/2] | Iterations: [160/1870] | Training loss: 0.492\n",
            "Epoch: [1/2] | Iterations: [161/1870] | Training loss: 0.495\n",
            "Epoch: [1/2] | Iterations: [162/1870] | Training loss: 0.476\n",
            "Epoch: [1/2] | Iterations: [163/1870] | Training loss: 0.489\n",
            "Epoch: [1/2] | Iterations: [164/1870] | Training loss: 0.470\n",
            "Epoch: [1/2] | Iterations: [165/1870] | Training loss: 0.480\n",
            "Epoch: [1/2] | Iterations: [166/1870] | Training loss: 0.482\n",
            "Epoch: [1/2] | Iterations: [167/1870] | Training loss: 0.511\n",
            "Epoch: [1/2] | Iterations: [168/1870] | Training loss: 0.494\n",
            "Epoch: [1/2] | Iterations: [169/1870] | Training loss: 0.505\n",
            "Epoch: [1/2] | Iterations: [170/1870] | Training loss: 0.451\n",
            "Epoch: [1/2] | Iterations: [171/1870] | Training loss: 0.491\n",
            "Epoch: [1/2] | Iterations: [172/1870] | Training loss: 0.485\n",
            "Epoch: [1/2] | Iterations: [173/1870] | Training loss: 0.443\n",
            "Epoch: [1/2] | Iterations: [174/1870] | Training loss: 0.485\n",
            "Epoch: [1/2] | Iterations: [175/1870] | Training loss: 0.457\n",
            "Epoch: [1/2] | Iterations: [176/1870] | Training loss: 0.541\n",
            "Epoch: [1/2] | Iterations: [177/1870] | Training loss: 0.456\n",
            "Epoch: [1/2] | Iterations: [178/1870] | Training loss: 0.437\n",
            "Epoch: [1/2] | Iterations: [179/1870] | Training loss: 0.489\n",
            "Epoch: [1/2] | Iterations: [180/1870] | Training loss: 0.488\n",
            "Epoch: [1/2] | Iterations: [181/1870] | Training loss: 0.502\n",
            "Epoch: [1/2] | Iterations: [182/1870] | Training loss: 0.465\n",
            "Epoch: [1/2] | Iterations: [183/1870] | Training loss: 0.421\n",
            "Epoch: [1/2] | Iterations: [184/1870] | Training loss: 0.455\n",
            "Epoch: [1/2] | Iterations: [185/1870] | Training loss: 0.464\n",
            "Epoch: [1/2] | Iterations: [186/1870] | Training loss: 0.465\n",
            "Epoch: [1/2] | Iterations: [187/1870] | Training loss: 0.427\n",
            "Epoch: [1/2] | Iterations: [188/1870] | Training loss: 0.526\n",
            "Epoch: [1/2] | Iterations: [189/1870] | Training loss: 0.473\n",
            "Epoch: [1/2] | Iterations: [190/1870] | Training loss: 0.485\n",
            "Epoch: [1/2] | Iterations: [191/1870] | Training loss: 0.482\n",
            "Epoch: [1/2] | Iterations: [192/1870] | Training loss: 0.445\n",
            "Epoch: [1/2] | Iterations: [193/1870] | Training loss: 0.418\n",
            "Epoch: [1/2] | Iterations: [194/1870] | Training loss: 0.485\n",
            "Epoch: [1/2] | Iterations: [195/1870] | Training loss: 0.469\n",
            "Epoch: [1/2] | Iterations: [196/1870] | Training loss: 0.459\n",
            "Epoch: [1/2] | Iterations: [197/1870] | Training loss: 0.472\n",
            "Epoch: [1/2] | Iterations: [198/1870] | Training loss: 0.422\n",
            "Epoch: [1/2] | Iterations: [199/1870] | Training loss: 0.476\n",
            "Epoch: [1/2] | Iterations: [200/1870] | Training loss: 0.432\n",
            "Epoch: [1/2] | Iterations: [201/1870] | Training loss: 0.439\n",
            "Epoch: [1/2] | Iterations: [202/1870] | Training loss: 0.437\n",
            "Epoch: [1/2] | Iterations: [203/1870] | Training loss: 0.472\n",
            "Epoch: [1/2] | Iterations: [204/1870] | Training loss: 0.420\n",
            "Epoch: [1/2] | Iterations: [205/1870] | Training loss: 0.498\n",
            "Epoch: [1/2] | Iterations: [206/1870] | Training loss: 0.430\n",
            "Epoch: [1/2] | Iterations: [207/1870] | Training loss: 0.432\n",
            "Epoch: [1/2] | Iterations: [208/1870] | Training loss: 0.436\n",
            "Epoch: [1/2] | Iterations: [209/1870] | Training loss: 0.434\n",
            "Epoch: [1/2] | Iterations: [210/1870] | Training loss: 0.431\n",
            "Epoch: [1/2] | Iterations: [211/1870] | Training loss: 0.477\n",
            "Epoch: [1/2] | Iterations: [212/1870] | Training loss: 0.435\n",
            "Epoch: [1/2] | Iterations: [213/1870] | Training loss: 0.464\n",
            "Epoch: [1/2] | Iterations: [214/1870] | Training loss: 0.423\n",
            "Epoch: [1/2] | Iterations: [215/1870] | Training loss: 0.474\n",
            "Epoch: [1/2] | Iterations: [216/1870] | Training loss: 0.392\n",
            "Epoch: [1/2] | Iterations: [217/1870] | Training loss: 0.433\n",
            "Epoch: [1/2] | Iterations: [218/1870] | Training loss: 0.415\n",
            "Epoch: [1/2] | Iterations: [219/1870] | Training loss: 0.431\n",
            "Epoch: [1/2] | Iterations: [220/1870] | Training loss: 0.405\n",
            "Epoch: [1/2] | Iterations: [221/1870] | Training loss: 0.450\n",
            "Epoch: [1/2] | Iterations: [222/1870] | Training loss: 0.405\n",
            "Epoch: [1/2] | Iterations: [223/1870] | Training loss: 0.411\n",
            "Epoch: [1/2] | Iterations: [224/1870] | Training loss: 0.488\n",
            "Epoch: [1/2] | Iterations: [225/1870] | Training loss: 0.440\n",
            "Epoch: [1/2] | Iterations: [226/1870] | Training loss: 0.420\n",
            "Epoch: [1/2] | Iterations: [227/1870] | Training loss: 0.419\n",
            "Epoch: [1/2] | Iterations: [228/1870] | Training loss: 0.437\n",
            "Epoch: [1/2] | Iterations: [229/1870] | Training loss: 0.400\n",
            "Epoch: [1/2] | Iterations: [230/1870] | Training loss: 0.429\n",
            "Epoch: [1/2] | Iterations: [231/1870] | Training loss: 0.407\n",
            "Epoch: [1/2] | Iterations: [232/1870] | Training loss: 0.428\n",
            "Epoch: [1/2] | Iterations: [233/1870] | Training loss: 0.413\n",
            "Epoch: [1/2] | Iterations: [234/1870] | Training loss: 0.415\n",
            "Epoch: [1/2] | Iterations: [235/1870] | Training loss: 0.405\n",
            "Epoch: [1/2] | Iterations: [236/1870] | Training loss: 0.422\n",
            "Epoch: [1/2] | Iterations: [237/1870] | Training loss: 0.409\n",
            "Epoch: [1/2] | Iterations: [238/1870] | Training loss: 0.463\n",
            "Epoch: [1/2] | Iterations: [239/1870] | Training loss: 0.431\n",
            "Epoch: [1/2] | Iterations: [240/1870] | Training loss: 0.387\n",
            "Epoch: [1/2] | Iterations: [241/1870] | Training loss: 0.424\n",
            "Epoch: [1/2] | Iterations: [242/1870] | Training loss: 0.411\n",
            "Epoch: [1/2] | Iterations: [243/1870] | Training loss: 0.401\n",
            "Epoch: [1/2] | Iterations: [244/1870] | Training loss: 0.416\n",
            "Epoch: [1/2] | Iterations: [245/1870] | Training loss: 0.415\n",
            "Epoch: [1/2] | Iterations: [246/1870] | Training loss: 0.406\n",
            "Epoch: [1/2] | Iterations: [247/1870] | Training loss: 0.401\n",
            "Epoch: [1/2] | Iterations: [248/1870] | Training loss: 0.391\n",
            "Epoch: [1/2] | Iterations: [249/1870] | Training loss: 0.446\n",
            "Epoch: [1/2] | Iterations: [250/1870] | Training loss: 0.431\n",
            "Epoch: [1/2] | Iterations: [251/1870] | Training loss: 0.408\n",
            "Epoch: [1/2] | Iterations: [252/1870] | Training loss: 0.403\n",
            "Epoch: [1/2] | Iterations: [253/1870] | Training loss: 0.434\n",
            "Epoch: [1/2] | Iterations: [254/1870] | Training loss: 0.365\n",
            "Epoch: [1/2] | Iterations: [255/1870] | Training loss: 0.369\n",
            "Epoch: [1/2] | Iterations: [256/1870] | Training loss: 0.423\n",
            "Epoch: [1/2] | Iterations: [257/1870] | Training loss: 0.390\n",
            "Epoch: [1/2] | Iterations: [258/1870] | Training loss: 0.351\n",
            "Epoch: [1/2] | Iterations: [259/1870] | Training loss: 0.400\n",
            "Epoch: [1/2] | Iterations: [260/1870] | Training loss: 0.431\n",
            "Epoch: [1/2] | Iterations: [261/1870] | Training loss: 0.395\n",
            "Epoch: [1/2] | Iterations: [262/1870] | Training loss: 0.370\n",
            "Epoch: [1/2] | Iterations: [263/1870] | Training loss: 0.375\n",
            "Epoch: [1/2] | Iterations: [264/1870] | Training loss: 0.374\n",
            "Epoch: [1/2] | Iterations: [265/1870] | Training loss: 0.399\n",
            "Epoch: [1/2] | Iterations: [266/1870] | Training loss: 0.425\n",
            "Epoch: [1/2] | Iterations: [267/1870] | Training loss: 0.364\n",
            "Epoch: [1/2] | Iterations: [268/1870] | Training loss: 0.411\n",
            "Epoch: [1/2] | Iterations: [269/1870] | Training loss: 0.408\n",
            "Epoch: [1/2] | Iterations: [270/1870] | Training loss: 0.369\n",
            "Epoch: [1/2] | Iterations: [271/1870] | Training loss: 0.459\n",
            "Epoch: [1/2] | Iterations: [272/1870] | Training loss: 0.403\n",
            "Epoch: [1/2] | Iterations: [273/1870] | Training loss: 0.370\n",
            "Epoch: [1/2] | Iterations: [274/1870] | Training loss: 0.407\n",
            "Epoch: [1/2] | Iterations: [275/1870] | Training loss: 0.376\n",
            "Epoch: [1/2] | Iterations: [276/1870] | Training loss: 0.374\n",
            "Epoch: [1/2] | Iterations: [277/1870] | Training loss: 0.373\n",
            "Epoch: [1/2] | Iterations: [278/1870] | Training loss: 0.384\n",
            "Epoch: [1/2] | Iterations: [279/1870] | Training loss: 0.398\n",
            "Epoch: [1/2] | Iterations: [280/1870] | Training loss: 0.369\n",
            "Epoch: [1/2] | Iterations: [281/1870] | Training loss: 0.380\n",
            "Epoch: [1/2] | Iterations: [282/1870] | Training loss: 0.343\n",
            "Epoch: [1/2] | Iterations: [283/1870] | Training loss: 0.333\n",
            "Epoch: [1/2] | Iterations: [284/1870] | Training loss: 0.355\n",
            "Epoch: [1/2] | Iterations: [285/1870] | Training loss: 0.358\n",
            "Epoch: [1/2] | Iterations: [286/1870] | Training loss: 0.357\n",
            "Epoch: [1/2] | Iterations: [287/1870] | Training loss: 0.417\n",
            "Epoch: [1/2] | Iterations: [288/1870] | Training loss: 0.367\n",
            "Epoch: [1/2] | Iterations: [289/1870] | Training loss: 0.394\n",
            "Epoch: [1/2] | Iterations: [290/1870] | Training loss: 0.357\n",
            "Epoch: [1/2] | Iterations: [291/1870] | Training loss: 0.360\n",
            "Epoch: [1/2] | Iterations: [292/1870] | Training loss: 0.400\n",
            "Epoch: [1/2] | Iterations: [293/1870] | Training loss: 0.350\n",
            "Epoch: [1/2] | Iterations: [294/1870] | Training loss: 0.399\n",
            "Epoch: [1/2] | Iterations: [295/1870] | Training loss: 0.328\n",
            "Epoch: [1/2] | Iterations: [296/1870] | Training loss: 0.368\n",
            "Epoch: [1/2] | Iterations: [297/1870] | Training loss: 0.362\n",
            "Epoch: [1/2] | Iterations: [298/1870] | Training loss: 0.341\n",
            "Epoch: [1/2] | Iterations: [299/1870] | Training loss: 0.420\n",
            "Epoch: [1/2] | Iterations: [300/1870] | Training loss: 0.369\n",
            "Epoch: [1/2] | Iterations: [301/1870] | Training loss: 0.357\n",
            "Epoch: [1/2] | Iterations: [302/1870] | Training loss: 0.371\n",
            "Epoch: [1/2] | Iterations: [303/1870] | Training loss: 0.325\n",
            "Epoch: [1/2] | Iterations: [304/1870] | Training loss: 0.320\n",
            "Epoch: [1/2] | Iterations: [305/1870] | Training loss: 0.409\n",
            "Epoch: [1/2] | Iterations: [306/1870] | Training loss: 0.388\n",
            "Epoch: [1/2] | Iterations: [307/1870] | Training loss: 0.371\n",
            "Epoch: [1/2] | Iterations: [308/1870] | Training loss: 0.348\n",
            "Epoch: [1/2] | Iterations: [309/1870] | Training loss: 0.332\n",
            "Epoch: [1/2] | Iterations: [310/1870] | Training loss: 0.340\n",
            "Epoch: [1/2] | Iterations: [311/1870] | Training loss: 0.372\n",
            "Epoch: [1/2] | Iterations: [312/1870] | Training loss: 0.351\n",
            "Epoch: [1/2] | Iterations: [313/1870] | Training loss: 0.376\n",
            "Epoch: [1/2] | Iterations: [314/1870] | Training loss: 0.349\n",
            "Epoch: [1/2] | Iterations: [315/1870] | Training loss: 0.349\n",
            "Epoch: [1/2] | Iterations: [316/1870] | Training loss: 0.335\n",
            "Epoch: [1/2] | Iterations: [317/1870] | Training loss: 0.355\n",
            "Epoch: [1/2] | Iterations: [318/1870] | Training loss: 0.352\n",
            "Epoch: [1/2] | Iterations: [319/1870] | Training loss: 0.296\n",
            "Epoch: [1/2] | Iterations: [320/1870] | Training loss: 0.334\n",
            "Epoch: [1/2] | Iterations: [321/1870] | Training loss: 0.332\n",
            "Epoch: [1/2] | Iterations: [322/1870] | Training loss: 0.294\n",
            "Epoch: [1/2] | Iterations: [323/1870] | Training loss: 0.343\n",
            "Epoch: [1/2] | Iterations: [324/1870] | Training loss: 0.298\n",
            "Epoch: [1/2] | Iterations: [325/1870] | Training loss: 0.345\n",
            "Epoch: [1/2] | Iterations: [326/1870] | Training loss: 0.318\n",
            "Epoch: [1/2] | Iterations: [327/1870] | Training loss: 0.329\n",
            "Epoch: [1/2] | Iterations: [328/1870] | Training loss: 0.319\n",
            "Epoch: [1/2] | Iterations: [329/1870] | Training loss: 0.318\n",
            "Epoch: [1/2] | Iterations: [330/1870] | Training loss: 0.343\n",
            "Epoch: [1/2] | Iterations: [331/1870] | Training loss: 0.345\n",
            "Epoch: [1/2] | Iterations: [332/1870] | Training loss: 0.337\n",
            "Epoch: [1/2] | Iterations: [333/1870] | Training loss: 0.402\n",
            "Epoch: [1/2] | Iterations: [334/1870] | Training loss: 0.340\n",
            "Epoch: [1/2] | Iterations: [335/1870] | Training loss: 0.352\n",
            "Epoch: [1/2] | Iterations: [336/1870] | Training loss: 0.312\n",
            "Epoch: [1/2] | Iterations: [337/1870] | Training loss: 0.361\n",
            "Epoch: [1/2] | Iterations: [338/1870] | Training loss: 0.340\n",
            "Epoch: [1/2] | Iterations: [339/1870] | Training loss: 0.337\n",
            "Epoch: [1/2] | Iterations: [340/1870] | Training loss: 0.351\n",
            "Epoch: [1/2] | Iterations: [341/1870] | Training loss: 0.313\n",
            "Epoch: [1/2] | Iterations: [342/1870] | Training loss: 0.320\n",
            "Epoch: [1/2] | Iterations: [343/1870] | Training loss: 0.311\n",
            "Epoch: [1/2] | Iterations: [344/1870] | Training loss: 0.321\n",
            "Epoch: [1/2] | Iterations: [345/1870] | Training loss: 0.317\n",
            "Epoch: [1/2] | Iterations: [346/1870] | Training loss: 0.313\n",
            "Epoch: [1/2] | Iterations: [347/1870] | Training loss: 0.350\n",
            "Epoch: [1/2] | Iterations: [348/1870] | Training loss: 0.308\n",
            "Epoch: [1/2] | Iterations: [349/1870] | Training loss: 0.332\n",
            "Epoch: [1/2] | Iterations: [350/1870] | Training loss: 0.355\n",
            "Epoch: [1/2] | Iterations: [351/1870] | Training loss: 0.311\n",
            "Epoch: [1/2] | Iterations: [352/1870] | Training loss: 0.305\n",
            "Epoch: [1/2] | Iterations: [353/1870] | Training loss: 0.340\n",
            "Epoch: [1/2] | Iterations: [354/1870] | Training loss: 0.297\n",
            "Epoch: [1/2] | Iterations: [355/1870] | Training loss: 0.318\n",
            "Epoch: [1/2] | Iterations: [356/1870] | Training loss: 0.338\n",
            "Epoch: [1/2] | Iterations: [357/1870] | Training loss: 0.364\n",
            "Epoch: [1/2] | Iterations: [358/1870] | Training loss: 0.325\n",
            "Epoch: [1/2] | Iterations: [359/1870] | Training loss: 0.279\n",
            "Epoch: [1/2] | Iterations: [360/1870] | Training loss: 0.378\n",
            "Epoch: [1/2] | Iterations: [361/1870] | Training loss: 0.296\n",
            "Epoch: [1/2] | Iterations: [362/1870] | Training loss: 0.323\n",
            "Epoch: [1/2] | Iterations: [363/1870] | Training loss: 0.309\n",
            "Epoch: [1/2] | Iterations: [364/1870] | Training loss: 0.346\n",
            "Epoch: [1/2] | Iterations: [365/1870] | Training loss: 0.345\n",
            "Epoch: [1/2] | Iterations: [366/1870] | Training loss: 0.311\n",
            "Epoch: [1/2] | Iterations: [367/1870] | Training loss: 0.296\n",
            "Epoch: [1/2] | Iterations: [368/1870] | Training loss: 0.294\n",
            "Epoch: [1/2] | Iterations: [369/1870] | Training loss: 0.294\n",
            "Epoch: [1/2] | Iterations: [370/1870] | Training loss: 0.277\n",
            "Epoch: [1/2] | Iterations: [371/1870] | Training loss: 0.360\n",
            "Epoch: [1/2] | Iterations: [372/1870] | Training loss: 0.319\n",
            "Epoch: [1/2] | Iterations: [373/1870] | Training loss: 0.277\n",
            "Epoch: [1/2] | Iterations: [374/1870] | Training loss: 0.342\n",
            "Epoch: [1/2] | Iterations: [375/1870] | Training loss: 0.367\n",
            "Epoch: [1/2] | Iterations: [376/1870] | Training loss: 0.276\n",
            "Epoch: [1/2] | Iterations: [377/1870] | Training loss: 0.331\n",
            "Epoch: [1/2] | Iterations: [378/1870] | Training loss: 0.310\n",
            "Epoch: [1/2] | Iterations: [379/1870] | Training loss: 0.254\n",
            "Epoch: [1/2] | Iterations: [380/1870] | Training loss: 0.297\n",
            "Epoch: [1/2] | Iterations: [381/1870] | Training loss: 0.366\n",
            "Epoch: [1/2] | Iterations: [382/1870] | Training loss: 0.269\n",
            "Epoch: [1/2] | Iterations: [383/1870] | Training loss: 0.318\n",
            "Epoch: [1/2] | Iterations: [384/1870] | Training loss: 0.268\n",
            "Epoch: [1/2] | Iterations: [385/1870] | Training loss: 0.262\n",
            "Epoch: [1/2] | Iterations: [386/1870] | Training loss: 0.305\n",
            "Epoch: [1/2] | Iterations: [387/1870] | Training loss: 0.330\n",
            "Epoch: [1/2] | Iterations: [388/1870] | Training loss: 0.264\n",
            "Epoch: [1/2] | Iterations: [389/1870] | Training loss: 0.295\n",
            "Epoch: [1/2] | Iterations: [390/1870] | Training loss: 0.292\n",
            "Epoch: [1/2] | Iterations: [391/1870] | Training loss: 0.278\n",
            "Epoch: [1/2] | Iterations: [392/1870] | Training loss: 0.315\n",
            "Epoch: [1/2] | Iterations: [393/1870] | Training loss: 0.301\n",
            "Epoch: [1/2] | Iterations: [394/1870] | Training loss: 0.261\n",
            "Epoch: [1/2] | Iterations: [395/1870] | Training loss: 0.270\n",
            "Epoch: [1/2] | Iterations: [396/1870] | Training loss: 0.280\n",
            "Epoch: [1/2] | Iterations: [397/1870] | Training loss: 0.309\n",
            "Epoch: [1/2] | Iterations: [398/1870] | Training loss: 0.311\n",
            "Epoch: [1/2] | Iterations: [399/1870] | Training loss: 0.292\n",
            "Epoch: [1/2] | Iterations: [400/1870] | Training loss: 0.311\n",
            "Epoch: [1/2] | Iterations: [401/1870] | Training loss: 0.226\n",
            "Epoch: [1/2] | Iterations: [402/1870] | Training loss: 0.267\n",
            "Epoch: [1/2] | Iterations: [403/1870] | Training loss: 0.297\n",
            "Epoch: [1/2] | Iterations: [404/1870] | Training loss: 0.270\n",
            "Epoch: [1/2] | Iterations: [405/1870] | Training loss: 0.315\n",
            "Epoch: [1/2] | Iterations: [406/1870] | Training loss: 0.259\n",
            "Epoch: [1/2] | Iterations: [407/1870] | Training loss: 0.322\n",
            "Epoch: [1/2] | Iterations: [408/1870] | Training loss: 0.315\n",
            "Epoch: [1/2] | Iterations: [409/1870] | Training loss: 0.333\n",
            "Epoch: [1/2] | Iterations: [410/1870] | Training loss: 0.273\n",
            "Epoch: [1/2] | Iterations: [411/1870] | Training loss: 0.258\n",
            "Epoch: [1/2] | Iterations: [412/1870] | Training loss: 0.276\n",
            "Epoch: [1/2] | Iterations: [413/1870] | Training loss: 0.300\n",
            "Epoch: [1/2] | Iterations: [414/1870] | Training loss: 0.289\n",
            "Epoch: [1/2] | Iterations: [415/1870] | Training loss: 0.276\n",
            "Epoch: [1/2] | Iterations: [416/1870] | Training loss: 0.293\n",
            "Epoch: [1/2] | Iterations: [417/1870] | Training loss: 0.271\n",
            "Epoch: [1/2] | Iterations: [418/1870] | Training loss: 0.233\n",
            "Epoch: [1/2] | Iterations: [419/1870] | Training loss: 0.329\n",
            "Epoch: [1/2] | Iterations: [420/1870] | Training loss: 0.256\n",
            "Epoch: [1/2] | Iterations: [421/1870] | Training loss: 0.246\n",
            "Epoch: [1/2] | Iterations: [422/1870] | Training loss: 0.267\n",
            "Epoch: [1/2] | Iterations: [423/1870] | Training loss: 0.242\n",
            "Epoch: [1/2] | Iterations: [424/1870] | Training loss: 0.278\n",
            "Epoch: [1/2] | Iterations: [425/1870] | Training loss: 0.295\n",
            "Epoch: [1/2] | Iterations: [426/1870] | Training loss: 0.301\n",
            "Epoch: [1/2] | Iterations: [427/1870] | Training loss: 0.278\n",
            "Epoch: [1/2] | Iterations: [428/1870] | Training loss: 0.310\n",
            "Epoch: [1/2] | Iterations: [429/1870] | Training loss: 0.290\n",
            "Epoch: [1/2] | Iterations: [430/1870] | Training loss: 0.307\n",
            "Epoch: [1/2] | Iterations: [431/1870] | Training loss: 0.298\n",
            "Epoch: [1/2] | Iterations: [432/1870] | Training loss: 0.285\n",
            "Epoch: [1/2] | Iterations: [433/1870] | Training loss: 0.258\n",
            "Epoch: [1/2] | Iterations: [434/1870] | Training loss: 0.287\n",
            "Epoch: [1/2] | Iterations: [435/1870] | Training loss: 0.299\n",
            "Epoch: [1/2] | Iterations: [436/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [437/1870] | Training loss: 0.291\n",
            "Epoch: [1/2] | Iterations: [438/1870] | Training loss: 0.275\n",
            "Epoch: [1/2] | Iterations: [439/1870] | Training loss: 0.321\n",
            "Epoch: [1/2] | Iterations: [440/1870] | Training loss: 0.281\n",
            "Epoch: [1/2] | Iterations: [441/1870] | Training loss: 0.267\n",
            "Epoch: [1/2] | Iterations: [442/1870] | Training loss: 0.321\n",
            "Epoch: [1/2] | Iterations: [443/1870] | Training loss: 0.242\n",
            "Epoch: [1/2] | Iterations: [444/1870] | Training loss: 0.204\n",
            "Epoch: [1/2] | Iterations: [445/1870] | Training loss: 0.285\n",
            "Epoch: [1/2] | Iterations: [446/1870] | Training loss: 0.267\n",
            "Epoch: [1/2] | Iterations: [447/1870] | Training loss: 0.248\n",
            "Epoch: [1/2] | Iterations: [448/1870] | Training loss: 0.293\n",
            "Epoch: [1/2] | Iterations: [449/1870] | Training loss: 0.310\n",
            "Epoch: [1/2] | Iterations: [450/1870] | Training loss: 0.270\n",
            "Epoch: [1/2] | Iterations: [451/1870] | Training loss: 0.307\n",
            "Epoch: [1/2] | Iterations: [452/1870] | Training loss: 0.301\n",
            "Epoch: [1/2] | Iterations: [453/1870] | Training loss: 0.241\n",
            "Epoch: [1/2] | Iterations: [454/1870] | Training loss: 0.320\n",
            "Epoch: [1/2] | Iterations: [455/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [456/1870] | Training loss: 0.250\n",
            "Epoch: [1/2] | Iterations: [457/1870] | Training loss: 0.266\n",
            "Epoch: [1/2] | Iterations: [458/1870] | Training loss: 0.230\n",
            "Epoch: [1/2] | Iterations: [459/1870] | Training loss: 0.278\n",
            "Epoch: [1/2] | Iterations: [460/1870] | Training loss: 0.252\n",
            "Epoch: [1/2] | Iterations: [461/1870] | Training loss: 0.259\n",
            "Epoch: [1/2] | Iterations: [462/1870] | Training loss: 0.271\n",
            "Epoch: [1/2] | Iterations: [463/1870] | Training loss: 0.255\n",
            "Epoch: [1/2] | Iterations: [464/1870] | Training loss: 0.269\n",
            "Epoch: [1/2] | Iterations: [465/1870] | Training loss: 0.276\n",
            "Epoch: [1/2] | Iterations: [466/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [467/1870] | Training loss: 0.258\n",
            "Epoch: [1/2] | Iterations: [468/1870] | Training loss: 0.238\n",
            "Epoch: [1/2] | Iterations: [469/1870] | Training loss: 0.276\n",
            "Epoch: [1/2] | Iterations: [470/1870] | Training loss: 0.275\n",
            "Epoch: [1/2] | Iterations: [471/1870] | Training loss: 0.306\n",
            "Epoch: [1/2] | Iterations: [472/1870] | Training loss: 0.241\n",
            "Epoch: [1/2] | Iterations: [473/1870] | Training loss: 0.285\n",
            "Epoch: [1/2] | Iterations: [474/1870] | Training loss: 0.261\n",
            "Epoch: [1/2] | Iterations: [475/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [476/1870] | Training loss: 0.281\n",
            "Epoch: [1/2] | Iterations: [477/1870] | Training loss: 0.285\n",
            "Epoch: [1/2] | Iterations: [478/1870] | Training loss: 0.250\n",
            "Epoch: [1/2] | Iterations: [479/1870] | Training loss: 0.339\n",
            "Epoch: [1/2] | Iterations: [480/1870] | Training loss: 0.240\n",
            "Epoch: [1/2] | Iterations: [481/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [482/1870] | Training loss: 0.269\n",
            "Epoch: [1/2] | Iterations: [483/1870] | Training loss: 0.262\n",
            "Epoch: [1/2] | Iterations: [484/1870] | Training loss: 0.243\n",
            "Epoch: [1/2] | Iterations: [485/1870] | Training loss: 0.312\n",
            "Epoch: [1/2] | Iterations: [486/1870] | Training loss: 0.290\n",
            "Epoch: [1/2] | Iterations: [487/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [488/1870] | Training loss: 0.273\n",
            "Epoch: [1/2] | Iterations: [489/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [490/1870] | Training loss: 0.356\n",
            "Epoch: [1/2] | Iterations: [491/1870] | Training loss: 0.269\n",
            "Epoch: [1/2] | Iterations: [492/1870] | Training loss: 0.249\n",
            "Epoch: [1/2] | Iterations: [493/1870] | Training loss: 0.203\n",
            "Epoch: [1/2] | Iterations: [494/1870] | Training loss: 0.236\n",
            "Epoch: [1/2] | Iterations: [495/1870] | Training loss: 0.241\n",
            "Epoch: [1/2] | Iterations: [496/1870] | Training loss: 0.295\n",
            "Epoch: [1/2] | Iterations: [497/1870] | Training loss: 0.317\n",
            "Epoch: [1/2] | Iterations: [498/1870] | Training loss: 0.183\n",
            "Epoch: [1/2] | Iterations: [499/1870] | Training loss: 0.293\n",
            "Epoch: [1/2] | Iterations: [500/1870] | Training loss: 0.251\n",
            "Epoch: [1/2] | Iterations: [501/1870] | Training loss: 0.218\n",
            "Epoch: [1/2] | Iterations: [502/1870] | Training loss: 0.269\n",
            "Epoch: [1/2] | Iterations: [503/1870] | Training loss: 0.257\n",
            "Epoch: [1/2] | Iterations: [504/1870] | Training loss: 0.218\n",
            "Epoch: [1/2] | Iterations: [505/1870] | Training loss: 0.203\n",
            "Epoch: [1/2] | Iterations: [506/1870] | Training loss: 0.245\n",
            "Epoch: [1/2] | Iterations: [507/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [508/1870] | Training loss: 0.309\n",
            "Epoch: [1/2] | Iterations: [509/1870] | Training loss: 0.255\n",
            "Epoch: [1/2] | Iterations: [510/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [511/1870] | Training loss: 0.247\n",
            "Epoch: [1/2] | Iterations: [512/1870] | Training loss: 0.234\n",
            "Epoch: [1/2] | Iterations: [513/1870] | Training loss: 0.226\n",
            "Epoch: [1/2] | Iterations: [514/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [515/1870] | Training loss: 0.181\n",
            "Epoch: [1/2] | Iterations: [516/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [517/1870] | Training loss: 0.285\n",
            "Epoch: [1/2] | Iterations: [518/1870] | Training loss: 0.243\n",
            "Epoch: [1/2] | Iterations: [519/1870] | Training loss: 0.204\n",
            "Epoch: [1/2] | Iterations: [520/1870] | Training loss: 0.253\n",
            "Epoch: [1/2] | Iterations: [521/1870] | Training loss: 0.235\n",
            "Epoch: [1/2] | Iterations: [522/1870] | Training loss: 0.270\n",
            "Epoch: [1/2] | Iterations: [523/1870] | Training loss: 0.253\n",
            "Epoch: [1/2] | Iterations: [524/1870] | Training loss: 0.273\n",
            "Epoch: [1/2] | Iterations: [525/1870] | Training loss: 0.200\n",
            "Epoch: [1/2] | Iterations: [526/1870] | Training loss: 0.309\n",
            "Epoch: [1/2] | Iterations: [527/1870] | Training loss: 0.235\n",
            "Epoch: [1/2] | Iterations: [528/1870] | Training loss: 0.258\n",
            "Epoch: [1/2] | Iterations: [529/1870] | Training loss: 0.266\n",
            "Epoch: [1/2] | Iterations: [530/1870] | Training loss: 0.372\n",
            "Epoch: [1/2] | Iterations: [531/1870] | Training loss: 0.226\n",
            "Epoch: [1/2] | Iterations: [532/1870] | Training loss: 0.264\n",
            "Epoch: [1/2] | Iterations: [533/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [534/1870] | Training loss: 0.240\n",
            "Epoch: [1/2] | Iterations: [535/1870] | Training loss: 0.193\n",
            "Epoch: [1/2] | Iterations: [536/1870] | Training loss: 0.309\n",
            "Epoch: [1/2] | Iterations: [537/1870] | Training loss: 0.193\n",
            "Epoch: [1/2] | Iterations: [538/1870] | Training loss: 0.199\n",
            "Epoch: [1/2] | Iterations: [539/1870] | Training loss: 0.232\n",
            "Epoch: [1/2] | Iterations: [540/1870] | Training loss: 0.270\n",
            "Epoch: [1/2] | Iterations: [541/1870] | Training loss: 0.207\n",
            "Epoch: [1/2] | Iterations: [542/1870] | Training loss: 0.280\n",
            "Epoch: [1/2] | Iterations: [543/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [544/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [545/1870] | Training loss: 0.278\n",
            "Epoch: [1/2] | Iterations: [546/1870] | Training loss: 0.294\n",
            "Epoch: [1/2] | Iterations: [547/1870] | Training loss: 0.300\n",
            "Epoch: [1/2] | Iterations: [548/1870] | Training loss: 0.248\n",
            "Epoch: [1/2] | Iterations: [549/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [550/1870] | Training loss: 0.196\n",
            "Epoch: [1/2] | Iterations: [551/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [552/1870] | Training loss: 0.275\n",
            "Epoch: [1/2] | Iterations: [553/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [554/1870] | Training loss: 0.275\n",
            "Epoch: [1/2] | Iterations: [555/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [556/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [557/1870] | Training loss: 0.279\n",
            "Epoch: [1/2] | Iterations: [558/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [559/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [560/1870] | Training loss: 0.249\n",
            "Epoch: [1/2] | Iterations: [561/1870] | Training loss: 0.196\n",
            "Epoch: [1/2] | Iterations: [562/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [563/1870] | Training loss: 0.207\n",
            "Epoch: [1/2] | Iterations: [564/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [565/1870] | Training loss: 0.204\n",
            "Epoch: [1/2] | Iterations: [566/1870] | Training loss: 0.285\n",
            "Epoch: [1/2] | Iterations: [567/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [568/1870] | Training loss: 0.201\n",
            "Epoch: [1/2] | Iterations: [569/1870] | Training loss: 0.265\n",
            "Epoch: [1/2] | Iterations: [570/1870] | Training loss: 0.186\n",
            "Epoch: [1/2] | Iterations: [571/1870] | Training loss: 0.238\n",
            "Epoch: [1/2] | Iterations: [572/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [573/1870] | Training loss: 0.254\n",
            "Epoch: [1/2] | Iterations: [574/1870] | Training loss: 0.296\n",
            "Epoch: [1/2] | Iterations: [575/1870] | Training loss: 0.198\n",
            "Epoch: [1/2] | Iterations: [576/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [577/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [578/1870] | Training loss: 0.272\n",
            "Epoch: [1/2] | Iterations: [579/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [580/1870] | Training loss: 0.261\n",
            "Epoch: [1/2] | Iterations: [581/1870] | Training loss: 0.300\n",
            "Epoch: [1/2] | Iterations: [582/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [583/1870] | Training loss: 0.236\n",
            "Epoch: [1/2] | Iterations: [584/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [585/1870] | Training loss: 0.194\n",
            "Epoch: [1/2] | Iterations: [586/1870] | Training loss: 0.264\n",
            "Epoch: [1/2] | Iterations: [587/1870] | Training loss: 0.248\n",
            "Epoch: [1/2] | Iterations: [588/1870] | Training loss: 0.265\n",
            "Epoch: [1/2] | Iterations: [589/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [590/1870] | Training loss: 0.243\n",
            "Epoch: [1/2] | Iterations: [591/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [592/1870] | Training loss: 0.277\n",
            "Epoch: [1/2] | Iterations: [593/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [594/1870] | Training loss: 0.297\n",
            "Epoch: [1/2] | Iterations: [595/1870] | Training loss: 0.226\n",
            "Epoch: [1/2] | Iterations: [596/1870] | Training loss: 0.231\n",
            "Epoch: [1/2] | Iterations: [597/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [598/1870] | Training loss: 0.265\n",
            "Epoch: [1/2] | Iterations: [599/1870] | Training loss: 0.283\n",
            "Epoch: [1/2] | Iterations: [600/1870] | Training loss: 0.216\n",
            "Epoch: [1/2] | Iterations: [601/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [602/1870] | Training loss: 0.242\n",
            "Epoch: [1/2] | Iterations: [603/1870] | Training loss: 0.188\n",
            "Epoch: [1/2] | Iterations: [604/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [605/1870] | Training loss: 0.242\n",
            "Epoch: [1/2] | Iterations: [606/1870] | Training loss: 0.246\n",
            "Epoch: [1/2] | Iterations: [607/1870] | Training loss: 0.257\n",
            "Epoch: [1/2] | Iterations: [608/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [609/1870] | Training loss: 0.207\n",
            "Epoch: [1/2] | Iterations: [610/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [611/1870] | Training loss: 0.263\n",
            "Epoch: [1/2] | Iterations: [612/1870] | Training loss: 0.255\n",
            "Epoch: [1/2] | Iterations: [613/1870] | Training loss: 0.159\n",
            "Epoch: [1/2] | Iterations: [614/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [615/1870] | Training loss: 0.311\n",
            "Epoch: [1/2] | Iterations: [616/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [617/1870] | Training loss: 0.206\n",
            "Epoch: [1/2] | Iterations: [618/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [619/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [620/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [621/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [622/1870] | Training loss: 0.232\n",
            "Epoch: [1/2] | Iterations: [623/1870] | Training loss: 0.223\n",
            "Epoch: [1/2] | Iterations: [624/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [625/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [626/1870] | Training loss: 0.181\n",
            "Epoch: [1/2] | Iterations: [627/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [628/1870] | Training loss: 0.234\n",
            "Epoch: [1/2] | Iterations: [629/1870] | Training loss: 0.169\n",
            "Epoch: [1/2] | Iterations: [630/1870] | Training loss: 0.260\n",
            "Epoch: [1/2] | Iterations: [631/1870] | Training loss: 0.272\n",
            "Epoch: [1/2] | Iterations: [632/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [633/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [634/1870] | Training loss: 0.264\n",
            "Epoch: [1/2] | Iterations: [635/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [636/1870] | Training loss: 0.221\n",
            "Epoch: [1/2] | Iterations: [637/1870] | Training loss: 0.164\n",
            "Epoch: [1/2] | Iterations: [638/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [639/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [640/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [641/1870] | Training loss: 0.194\n",
            "Epoch: [1/2] | Iterations: [642/1870] | Training loss: 0.252\n",
            "Epoch: [1/2] | Iterations: [643/1870] | Training loss: 0.254\n",
            "Epoch: [1/2] | Iterations: [644/1870] | Training loss: 0.259\n",
            "Epoch: [1/2] | Iterations: [645/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [646/1870] | Training loss: 0.164\n",
            "Epoch: [1/2] | Iterations: [647/1870] | Training loss: 0.288\n",
            "Epoch: [1/2] | Iterations: [648/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [649/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [650/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [651/1870] | Training loss: 0.302\n",
            "Epoch: [1/2] | Iterations: [652/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [653/1870] | Training loss: 0.196\n",
            "Epoch: [1/2] | Iterations: [654/1870] | Training loss: 0.181\n",
            "Epoch: [1/2] | Iterations: [655/1870] | Training loss: 0.159\n",
            "Epoch: [1/2] | Iterations: [656/1870] | Training loss: 0.235\n",
            "Epoch: [1/2] | Iterations: [657/1870] | Training loss: 0.264\n",
            "Epoch: [1/2] | Iterations: [658/1870] | Training loss: 0.203\n",
            "Epoch: [1/2] | Iterations: [659/1870] | Training loss: 0.293\n",
            "Epoch: [1/2] | Iterations: [660/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [661/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [662/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [663/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [664/1870] | Training loss: 0.277\n",
            "Epoch: [1/2] | Iterations: [665/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [666/1870] | Training loss: 0.262\n",
            "Epoch: [1/2] | Iterations: [667/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [668/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [669/1870] | Training loss: 0.148\n",
            "Epoch: [1/2] | Iterations: [670/1870] | Training loss: 0.237\n",
            "Epoch: [1/2] | Iterations: [671/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [672/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [673/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [674/1870] | Training loss: 0.141\n",
            "Epoch: [1/2] | Iterations: [675/1870] | Training loss: 0.159\n",
            "Epoch: [1/2] | Iterations: [676/1870] | Training loss: 0.183\n",
            "Epoch: [1/2] | Iterations: [677/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [678/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [679/1870] | Training loss: 0.222\n",
            "Epoch: [1/2] | Iterations: [680/1870] | Training loss: 0.231\n",
            "Epoch: [1/2] | Iterations: [681/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [682/1870] | Training loss: 0.262\n",
            "Epoch: [1/2] | Iterations: [683/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [684/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [685/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [686/1870] | Training loss: 0.182\n",
            "Epoch: [1/2] | Iterations: [687/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [688/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [689/1870] | Training loss: 0.178\n",
            "Epoch: [1/2] | Iterations: [690/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [691/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [692/1870] | Training loss: 0.197\n",
            "Epoch: [1/2] | Iterations: [693/1870] | Training loss: 0.238\n",
            "Epoch: [1/2] | Iterations: [694/1870] | Training loss: 0.224\n",
            "Epoch: [1/2] | Iterations: [695/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [696/1870] | Training loss: 0.233\n",
            "Epoch: [1/2] | Iterations: [697/1870] | Training loss: 0.206\n",
            "Epoch: [1/2] | Iterations: [698/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [699/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [700/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [701/1870] | Training loss: 0.261\n",
            "Epoch: [1/2] | Iterations: [702/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [703/1870] | Training loss: 0.151\n",
            "Epoch: [1/2] | Iterations: [704/1870] | Training loss: 0.244\n",
            "Epoch: [1/2] | Iterations: [705/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [706/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [707/1870] | Training loss: 0.236\n",
            "Epoch: [1/2] | Iterations: [708/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [709/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [710/1870] | Training loss: 0.272\n",
            "Epoch: [1/2] | Iterations: [711/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [712/1870] | Training loss: 0.238\n",
            "Epoch: [1/2] | Iterations: [713/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [714/1870] | Training loss: 0.183\n",
            "Epoch: [1/2] | Iterations: [715/1870] | Training loss: 0.188\n",
            "Epoch: [1/2] | Iterations: [716/1870] | Training loss: 0.145\n",
            "Epoch: [1/2] | Iterations: [717/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [718/1870] | Training loss: 0.227\n",
            "Epoch: [1/2] | Iterations: [719/1870] | Training loss: 0.159\n",
            "Epoch: [1/2] | Iterations: [720/1870] | Training loss: 0.236\n",
            "Epoch: [1/2] | Iterations: [721/1870] | Training loss: 0.233\n",
            "Epoch: [1/2] | Iterations: [722/1870] | Training loss: 0.246\n",
            "Epoch: [1/2] | Iterations: [723/1870] | Training loss: 0.255\n",
            "Epoch: [1/2] | Iterations: [724/1870] | Training loss: 0.273\n",
            "Epoch: [1/2] | Iterations: [725/1870] | Training loss: 0.249\n",
            "Epoch: [1/2] | Iterations: [726/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [727/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [728/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [729/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [730/1870] | Training loss: 0.301\n",
            "Epoch: [1/2] | Iterations: [731/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [732/1870] | Training loss: 0.304\n",
            "Epoch: [1/2] | Iterations: [733/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [734/1870] | Training loss: 0.274\n",
            "Epoch: [1/2] | Iterations: [735/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [736/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [737/1870] | Training loss: 0.167\n",
            "Epoch: [1/2] | Iterations: [738/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [739/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [740/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [741/1870] | Training loss: 0.145\n",
            "Epoch: [1/2] | Iterations: [742/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [743/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [744/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [745/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [746/1870] | Training loss: 0.248\n",
            "Epoch: [1/2] | Iterations: [747/1870] | Training loss: 0.186\n",
            "Epoch: [1/2] | Iterations: [748/1870] | Training loss: 0.245\n",
            "Epoch: [1/2] | Iterations: [749/1870] | Training loss: 0.303\n",
            "Epoch: [1/2] | Iterations: [750/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [751/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [752/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [753/1870] | Training loss: 0.248\n",
            "Epoch: [1/2] | Iterations: [754/1870] | Training loss: 0.269\n",
            "Epoch: [1/2] | Iterations: [755/1870] | Training loss: 0.147\n",
            "Epoch: [1/2] | Iterations: [756/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [757/1870] | Training loss: 0.126\n",
            "Epoch: [1/2] | Iterations: [758/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [759/1870] | Training loss: 0.182\n",
            "Epoch: [1/2] | Iterations: [760/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [761/1870] | Training loss: 0.186\n",
            "Epoch: [1/2] | Iterations: [762/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [763/1870] | Training loss: 0.134\n",
            "Epoch: [1/2] | Iterations: [764/1870] | Training loss: 0.177\n",
            "Epoch: [1/2] | Iterations: [765/1870] | Training loss: 0.257\n",
            "Epoch: [1/2] | Iterations: [766/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [767/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [768/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [769/1870] | Training loss: 0.159\n",
            "Epoch: [1/2] | Iterations: [770/1870] | Training loss: 0.227\n",
            "Epoch: [1/2] | Iterations: [771/1870] | Training loss: 0.290\n",
            "Epoch: [1/2] | Iterations: [772/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [773/1870] | Training loss: 0.154\n",
            "Epoch: [1/2] | Iterations: [774/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [775/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [776/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [777/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [778/1870] | Training loss: 0.186\n",
            "Epoch: [1/2] | Iterations: [779/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [780/1870] | Training loss: 0.256\n",
            "Epoch: [1/2] | Iterations: [781/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [782/1870] | Training loss: 0.141\n",
            "Epoch: [1/2] | Iterations: [783/1870] | Training loss: 0.293\n",
            "Epoch: [1/2] | Iterations: [784/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [785/1870] | Training loss: 0.221\n",
            "Epoch: [1/2] | Iterations: [786/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [787/1870] | Training loss: 0.199\n",
            "Epoch: [1/2] | Iterations: [788/1870] | Training loss: 0.205\n",
            "Epoch: [1/2] | Iterations: [789/1870] | Training loss: 0.114\n",
            "Epoch: [1/2] | Iterations: [790/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [791/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [792/1870] | Training loss: 0.235\n",
            "Epoch: [1/2] | Iterations: [793/1870] | Training loss: 0.231\n",
            "Epoch: [1/2] | Iterations: [794/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [795/1870] | Training loss: 0.241\n",
            "Epoch: [1/2] | Iterations: [796/1870] | Training loss: 0.241\n",
            "Epoch: [1/2] | Iterations: [797/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [798/1870] | Training loss: 0.222\n",
            "Epoch: [1/2] | Iterations: [799/1870] | Training loss: 0.250\n",
            "Epoch: [1/2] | Iterations: [800/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [801/1870] | Training loss: 0.148\n",
            "Epoch: [1/2] | Iterations: [802/1870] | Training loss: 0.140\n",
            "Epoch: [1/2] | Iterations: [803/1870] | Training loss: 0.292\n",
            "Epoch: [1/2] | Iterations: [804/1870] | Training loss: 0.240\n",
            "Epoch: [1/2] | Iterations: [805/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [806/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [807/1870] | Training loss: 0.183\n",
            "Epoch: [1/2] | Iterations: [808/1870] | Training loss: 0.216\n",
            "Epoch: [1/2] | Iterations: [809/1870] | Training loss: 0.145\n",
            "Epoch: [1/2] | Iterations: [810/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [811/1870] | Training loss: 0.240\n",
            "Epoch: [1/2] | Iterations: [812/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [813/1870] | Training loss: 0.188\n",
            "Epoch: [1/2] | Iterations: [814/1870] | Training loss: 0.182\n",
            "Epoch: [1/2] | Iterations: [815/1870] | Training loss: 0.182\n",
            "Epoch: [1/2] | Iterations: [816/1870] | Training loss: 0.108\n",
            "Epoch: [1/2] | Iterations: [817/1870] | Training loss: 0.140\n",
            "Epoch: [1/2] | Iterations: [818/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [819/1870] | Training loss: 0.200\n",
            "Epoch: [1/2] | Iterations: [820/1870] | Training loss: 0.236\n",
            "Epoch: [1/2] | Iterations: [821/1870] | Training loss: 0.271\n",
            "Epoch: [1/2] | Iterations: [822/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [823/1870] | Training loss: 0.301\n",
            "Epoch: [1/2] | Iterations: [824/1870] | Training loss: 0.164\n",
            "Epoch: [1/2] | Iterations: [825/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [826/1870] | Training loss: 0.218\n",
            "Epoch: [1/2] | Iterations: [827/1870] | Training loss: 0.204\n",
            "Epoch: [1/2] | Iterations: [828/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [829/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [830/1870] | Training loss: 0.205\n",
            "Epoch: [1/2] | Iterations: [831/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [832/1870] | Training loss: 0.140\n",
            "Epoch: [1/2] | Iterations: [833/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [834/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [835/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [836/1870] | Training loss: 0.236\n",
            "Epoch: [1/2] | Iterations: [837/1870] | Training loss: 0.237\n",
            "Epoch: [1/2] | Iterations: [838/1870] | Training loss: 0.172\n",
            "Epoch: [1/2] | Iterations: [839/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [840/1870] | Training loss: 0.147\n",
            "Epoch: [1/2] | Iterations: [841/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [842/1870] | Training loss: 0.148\n",
            "Epoch: [1/2] | Iterations: [843/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [844/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [845/1870] | Training loss: 0.138\n",
            "Epoch: [1/2] | Iterations: [846/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [847/1870] | Training loss: 0.167\n",
            "Epoch: [1/2] | Iterations: [848/1870] | Training loss: 0.161\n",
            "Epoch: [1/2] | Iterations: [849/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [850/1870] | Training loss: 0.119\n",
            "Epoch: [1/2] | Iterations: [851/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [852/1870] | Training loss: 0.236\n",
            "Epoch: [1/2] | Iterations: [853/1870] | Training loss: 0.318\n",
            "Epoch: [1/2] | Iterations: [854/1870] | Training loss: 0.135\n",
            "Epoch: [1/2] | Iterations: [855/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [856/1870] | Training loss: 0.132\n",
            "Epoch: [1/2] | Iterations: [857/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [858/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [859/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [860/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [861/1870] | Training loss: 0.237\n",
            "Epoch: [1/2] | Iterations: [862/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [863/1870] | Training loss: 0.242\n",
            "Epoch: [1/2] | Iterations: [864/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [865/1870] | Training loss: 0.231\n",
            "Epoch: [1/2] | Iterations: [866/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [867/1870] | Training loss: 0.261\n",
            "Epoch: [1/2] | Iterations: [868/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [869/1870] | Training loss: 0.234\n",
            "Epoch: [1/2] | Iterations: [870/1870] | Training loss: 0.188\n",
            "Epoch: [1/2] | Iterations: [871/1870] | Training loss: 0.223\n",
            "Epoch: [1/2] | Iterations: [872/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [873/1870] | Training loss: 0.199\n",
            "Epoch: [1/2] | Iterations: [874/1870] | Training loss: 0.245\n",
            "Epoch: [1/2] | Iterations: [875/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [876/1870] | Training loss: 0.141\n",
            "Epoch: [1/2] | Iterations: [877/1870] | Training loss: 0.154\n",
            "Epoch: [1/2] | Iterations: [878/1870] | Training loss: 0.137\n",
            "Epoch: [1/2] | Iterations: [879/1870] | Training loss: 0.172\n",
            "Epoch: [1/2] | Iterations: [880/1870] | Training loss: 0.196\n",
            "Epoch: [1/2] | Iterations: [881/1870] | Training loss: 0.183\n",
            "Epoch: [1/2] | Iterations: [882/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [883/1870] | Training loss: 0.161\n",
            "Epoch: [1/2] | Iterations: [884/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [885/1870] | Training loss: 0.126\n",
            "Epoch: [1/2] | Iterations: [886/1870] | Training loss: 0.280\n",
            "Epoch: [1/2] | Iterations: [887/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [888/1870] | Training loss: 0.168\n",
            "Epoch: [1/2] | Iterations: [889/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [890/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [891/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [892/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [893/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [894/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [895/1870] | Training loss: 0.113\n",
            "Epoch: [1/2] | Iterations: [896/1870] | Training loss: 0.173\n",
            "Epoch: [1/2] | Iterations: [897/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [898/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [899/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [900/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [901/1870] | Training loss: 0.117\n",
            "Epoch: [1/2] | Iterations: [902/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [903/1870] | Training loss: 0.147\n",
            "Epoch: [1/2] | Iterations: [904/1870] | Training loss: 0.096\n",
            "Epoch: [1/2] | Iterations: [905/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [906/1870] | Training loss: 0.200\n",
            "Epoch: [1/2] | Iterations: [907/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [908/1870] | Training loss: 0.233\n",
            "Epoch: [1/2] | Iterations: [909/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [910/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [911/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [912/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [913/1870] | Training loss: 0.167\n",
            "Epoch: [1/2] | Iterations: [914/1870] | Training loss: 0.238\n",
            "Epoch: [1/2] | Iterations: [915/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [916/1870] | Training loss: 0.221\n",
            "Epoch: [1/2] | Iterations: [917/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [918/1870] | Training loss: 0.126\n",
            "Epoch: [1/2] | Iterations: [919/1870] | Training loss: 0.251\n",
            "Epoch: [1/2] | Iterations: [920/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [921/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [922/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [923/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [924/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [925/1870] | Training loss: 0.147\n",
            "Epoch: [1/2] | Iterations: [926/1870] | Training loss: 0.199\n",
            "Epoch: [1/2] | Iterations: [927/1870] | Training loss: 0.113\n",
            "Epoch: [1/2] | Iterations: [928/1870] | Training loss: 0.300\n",
            "Epoch: [1/2] | Iterations: [929/1870] | Training loss: 0.177\n",
            "Epoch: [1/2] | Iterations: [930/1870] | Training loss: 0.125\n",
            "Epoch: [1/2] | Iterations: [931/1870] | Training loss: 0.188\n",
            "Epoch: [1/2] | Iterations: [932/1870] | Training loss: 0.201\n",
            "Epoch: [1/2] | Iterations: [933/1870] | Training loss: 0.106\n",
            "Epoch: [1/2] | Iterations: [934/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [935/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [936/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [937/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [938/1870] | Training loss: 0.279\n",
            "Epoch: [1/2] | Iterations: [939/1870] | Training loss: 0.242\n",
            "Epoch: [1/2] | Iterations: [940/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [941/1870] | Training loss: 0.213\n",
            "Epoch: [1/2] | Iterations: [942/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [943/1870] | Training loss: 0.126\n",
            "Epoch: [1/2] | Iterations: [944/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [945/1870] | Training loss: 0.111\n",
            "Epoch: [1/2] | Iterations: [946/1870] | Training loss: 0.199\n",
            "Epoch: [1/2] | Iterations: [947/1870] | Training loss: 0.254\n",
            "Epoch: [1/2] | Iterations: [948/1870] | Training loss: 0.115\n",
            "Epoch: [1/2] | Iterations: [949/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [950/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [951/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [952/1870] | Training loss: 0.134\n",
            "Epoch: [1/2] | Iterations: [953/1870] | Training loss: 0.113\n",
            "Epoch: [1/2] | Iterations: [954/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [955/1870] | Training loss: 0.154\n",
            "Epoch: [1/2] | Iterations: [956/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [957/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [958/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [959/1870] | Training loss: 0.172\n",
            "Epoch: [1/2] | Iterations: [960/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [961/1870] | Training loss: 0.197\n",
            "Epoch: [1/2] | Iterations: [962/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [963/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [964/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [965/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [966/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [967/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [968/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [969/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [970/1870] | Training loss: 0.231\n",
            "Epoch: [1/2] | Iterations: [971/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [972/1870] | Training loss: 0.197\n",
            "Epoch: [1/2] | Iterations: [973/1870] | Training loss: 0.147\n",
            "Epoch: [1/2] | Iterations: [974/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [975/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [976/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [977/1870] | Training loss: 0.242\n",
            "Epoch: [1/2] | Iterations: [978/1870] | Training loss: 0.247\n",
            "Epoch: [1/2] | Iterations: [979/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [980/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [981/1870] | Training loss: 0.301\n",
            "Epoch: [1/2] | Iterations: [982/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [983/1870] | Training loss: 0.285\n",
            "Epoch: [1/2] | Iterations: [984/1870] | Training loss: 0.176\n",
            "Epoch: [1/2] | Iterations: [985/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [986/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [987/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [988/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [989/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [990/1870] | Training loss: 0.230\n",
            "Epoch: [1/2] | Iterations: [991/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [992/1870] | Training loss: 0.223\n",
            "Epoch: [1/2] | Iterations: [993/1870] | Training loss: 0.145\n",
            "Epoch: [1/2] | Iterations: [994/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [995/1870] | Training loss: 0.117\n",
            "Epoch: [1/2] | Iterations: [996/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [997/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [998/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [999/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [1000/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [1001/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [1002/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [1003/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [1004/1870] | Training loss: 0.204\n",
            "Epoch: [1/2] | Iterations: [1005/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [1006/1870] | Training loss: 0.247\n",
            "Epoch: [1/2] | Iterations: [1007/1870] | Training loss: 0.248\n",
            "Epoch: [1/2] | Iterations: [1008/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [1009/1870] | Training loss: 0.134\n",
            "Epoch: [1/2] | Iterations: [1010/1870] | Training loss: 0.181\n",
            "Epoch: [1/2] | Iterations: [1011/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [1012/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1013/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [1014/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1015/1870] | Training loss: 0.099\n",
            "Epoch: [1/2] | Iterations: [1016/1870] | Training loss: 0.134\n",
            "Epoch: [1/2] | Iterations: [1017/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1018/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1019/1870] | Training loss: 0.145\n",
            "Epoch: [1/2] | Iterations: [1020/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [1021/1870] | Training loss: 0.206\n",
            "Epoch: [1/2] | Iterations: [1022/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [1023/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [1024/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1025/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1026/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [1027/1870] | Training loss: 0.193\n",
            "Epoch: [1/2] | Iterations: [1028/1870] | Training loss: 0.161\n",
            "Epoch: [1/2] | Iterations: [1029/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [1030/1870] | Training loss: 0.200\n",
            "Epoch: [1/2] | Iterations: [1031/1870] | Training loss: 0.098\n",
            "Epoch: [1/2] | Iterations: [1032/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [1033/1870] | Training loss: 0.269\n",
            "Epoch: [1/2] | Iterations: [1034/1870] | Training loss: 0.114\n",
            "Epoch: [1/2] | Iterations: [1035/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [1036/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [1037/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1038/1870] | Training loss: 0.218\n",
            "Epoch: [1/2] | Iterations: [1039/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1040/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1041/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [1042/1870] | Training loss: 0.081\n",
            "Epoch: [1/2] | Iterations: [1043/1870] | Training loss: 0.213\n",
            "Epoch: [1/2] | Iterations: [1044/1870] | Training loss: 0.199\n",
            "Epoch: [1/2] | Iterations: [1045/1870] | Training loss: 0.172\n",
            "Epoch: [1/2] | Iterations: [1046/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1047/1870] | Training loss: 0.173\n",
            "Epoch: [1/2] | Iterations: [1048/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [1049/1870] | Training loss: 0.132\n",
            "Epoch: [1/2] | Iterations: [1050/1870] | Training loss: 0.138\n",
            "Epoch: [1/2] | Iterations: [1051/1870] | Training loss: 0.148\n",
            "Epoch: [1/2] | Iterations: [1052/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [1053/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [1054/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1055/1870] | Training loss: 0.097\n",
            "Epoch: [1/2] | Iterations: [1056/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [1057/1870] | Training loss: 0.095\n",
            "Epoch: [1/2] | Iterations: [1058/1870] | Training loss: 0.197\n",
            "Epoch: [1/2] | Iterations: [1059/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [1060/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [1061/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [1062/1870] | Training loss: 0.108\n",
            "Epoch: [1/2] | Iterations: [1063/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [1064/1870] | Training loss: 0.105\n",
            "Epoch: [1/2] | Iterations: [1065/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [1066/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1067/1870] | Training loss: 0.076\n",
            "Epoch: [1/2] | Iterations: [1068/1870] | Training loss: 0.176\n",
            "Epoch: [1/2] | Iterations: [1069/1870] | Training loss: 0.277\n",
            "Epoch: [1/2] | Iterations: [1070/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1071/1870] | Training loss: 0.111\n",
            "Epoch: [1/2] | Iterations: [1072/1870] | Training loss: 0.074\n",
            "Epoch: [1/2] | Iterations: [1073/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [1074/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [1075/1870] | Training loss: 0.169\n",
            "Epoch: [1/2] | Iterations: [1076/1870] | Training loss: 0.106\n",
            "Epoch: [1/2] | Iterations: [1077/1870] | Training loss: 0.148\n",
            "Epoch: [1/2] | Iterations: [1078/1870] | Training loss: 0.278\n",
            "Epoch: [1/2] | Iterations: [1079/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [1080/1870] | Training loss: 0.120\n",
            "Epoch: [1/2] | Iterations: [1081/1870] | Training loss: 0.083\n",
            "Epoch: [1/2] | Iterations: [1082/1870] | Training loss: 0.281\n",
            "Epoch: [1/2] | Iterations: [1083/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [1084/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [1085/1870] | Training loss: 0.110\n",
            "Epoch: [1/2] | Iterations: [1086/1870] | Training loss: 0.150\n",
            "Epoch: [1/2] | Iterations: [1087/1870] | Training loss: 0.201\n",
            "Epoch: [1/2] | Iterations: [1088/1870] | Training loss: 0.081\n",
            "Epoch: [1/2] | Iterations: [1089/1870] | Training loss: 0.207\n",
            "Epoch: [1/2] | Iterations: [1090/1870] | Training loss: 0.201\n",
            "Epoch: [1/2] | Iterations: [1091/1870] | Training loss: 0.130\n",
            "Epoch: [1/2] | Iterations: [1092/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1093/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [1094/1870] | Training loss: 0.133\n",
            "Epoch: [1/2] | Iterations: [1095/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [1096/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [1097/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [1098/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [1099/1870] | Training loss: 0.107\n",
            "Epoch: [1/2] | Iterations: [1100/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [1101/1870] | Training loss: 0.227\n",
            "Epoch: [1/2] | Iterations: [1102/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [1103/1870] | Training loss: 0.263\n",
            "Epoch: [1/2] | Iterations: [1104/1870] | Training loss: 0.271\n",
            "Epoch: [1/2] | Iterations: [1105/1870] | Training loss: 0.213\n",
            "Epoch: [1/2] | Iterations: [1106/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [1107/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1108/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [1109/1870] | Training loss: 0.222\n",
            "Epoch: [1/2] | Iterations: [1110/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [1111/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1112/1870] | Training loss: 0.309\n",
            "Epoch: [1/2] | Iterations: [1113/1870] | Training loss: 0.104\n",
            "Epoch: [1/2] | Iterations: [1114/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [1115/1870] | Training loss: 0.248\n",
            "Epoch: [1/2] | Iterations: [1116/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [1117/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [1118/1870] | Training loss: 0.238\n",
            "Epoch: [1/2] | Iterations: [1119/1870] | Training loss: 0.100\n",
            "Epoch: [1/2] | Iterations: [1120/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [1121/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [1122/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1123/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [1124/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [1125/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [1126/1870] | Training loss: 0.260\n",
            "Epoch: [1/2] | Iterations: [1127/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1128/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [1129/1870] | Training loss: 0.198\n",
            "Epoch: [1/2] | Iterations: [1130/1870] | Training loss: 0.237\n",
            "Epoch: [1/2] | Iterations: [1131/1870] | Training loss: 0.224\n",
            "Epoch: [1/2] | Iterations: [1132/1870] | Training loss: 0.250\n",
            "Epoch: [1/2] | Iterations: [1133/1870] | Training loss: 0.138\n",
            "Epoch: [1/2] | Iterations: [1134/1870] | Training loss: 0.123\n",
            "Epoch: [1/2] | Iterations: [1135/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [1136/1870] | Training loss: 0.205\n",
            "Epoch: [1/2] | Iterations: [1137/1870] | Training loss: 0.108\n",
            "Epoch: [1/2] | Iterations: [1138/1870] | Training loss: 0.251\n",
            "Epoch: [1/2] | Iterations: [1139/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [1140/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1141/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [1142/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [1143/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [1144/1870] | Training loss: 0.125\n",
            "Epoch: [1/2] | Iterations: [1145/1870] | Training loss: 0.097\n",
            "Epoch: [1/2] | Iterations: [1146/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [1147/1870] | Training loss: 0.122\n",
            "Epoch: [1/2] | Iterations: [1148/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1149/1870] | Training loss: 0.099\n",
            "Epoch: [1/2] | Iterations: [1150/1870] | Training loss: 0.322\n",
            "Epoch: [1/2] | Iterations: [1151/1870] | Training loss: 0.186\n",
            "Epoch: [1/2] | Iterations: [1152/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1153/1870] | Training loss: 0.125\n",
            "Epoch: [1/2] | Iterations: [1154/1870] | Training loss: 0.206\n",
            "Epoch: [1/2] | Iterations: [1155/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [1156/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [1157/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1158/1870] | Training loss: 0.250\n",
            "Epoch: [1/2] | Iterations: [1159/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [1160/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [1161/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1162/1870] | Training loss: 0.164\n",
            "Epoch: [1/2] | Iterations: [1163/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [1164/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1165/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1166/1870] | Training loss: 0.274\n",
            "Epoch: [1/2] | Iterations: [1167/1870] | Training loss: 0.097\n",
            "Epoch: [1/2] | Iterations: [1168/1870] | Training loss: 0.207\n",
            "Epoch: [1/2] | Iterations: [1169/1870] | Training loss: 0.255\n",
            "Epoch: [1/2] | Iterations: [1170/1870] | Training loss: 0.135\n",
            "Epoch: [1/2] | Iterations: [1171/1870] | Training loss: 0.110\n",
            "Epoch: [1/2] | Iterations: [1172/1870] | Training loss: 0.095\n",
            "Epoch: [1/2] | Iterations: [1173/1870] | Training loss: 0.148\n",
            "Epoch: [1/2] | Iterations: [1174/1870] | Training loss: 0.169\n",
            "Epoch: [1/2] | Iterations: [1175/1870] | Training loss: 0.161\n",
            "Epoch: [1/2] | Iterations: [1176/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [1177/1870] | Training loss: 0.075\n",
            "Epoch: [1/2] | Iterations: [1178/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [1179/1870] | Training loss: 0.116\n",
            "Epoch: [1/2] | Iterations: [1180/1870] | Training loss: 0.265\n",
            "Epoch: [1/2] | Iterations: [1181/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [1182/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [1183/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1184/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [1185/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1186/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [1187/1870] | Training loss: 0.226\n",
            "Epoch: [1/2] | Iterations: [1188/1870] | Training loss: 0.093\n",
            "Epoch: [1/2] | Iterations: [1189/1870] | Training loss: 0.137\n",
            "Epoch: [1/2] | Iterations: [1190/1870] | Training loss: 0.213\n",
            "Epoch: [1/2] | Iterations: [1191/1870] | Training loss: 0.205\n",
            "Epoch: [1/2] | Iterations: [1192/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1193/1870] | Training loss: 0.232\n",
            "Epoch: [1/2] | Iterations: [1194/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [1195/1870] | Training loss: 0.182\n",
            "Epoch: [1/2] | Iterations: [1196/1870] | Training loss: 0.182\n",
            "Epoch: [1/2] | Iterations: [1197/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [1198/1870] | Training loss: 0.173\n",
            "Epoch: [1/2] | Iterations: [1199/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [1200/1870] | Training loss: 0.151\n",
            "Epoch: [1/2] | Iterations: [1201/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [1202/1870] | Training loss: 0.216\n",
            "Epoch: [1/2] | Iterations: [1203/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1204/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [1205/1870] | Training loss: 0.169\n",
            "Epoch: [1/2] | Iterations: [1206/1870] | Training loss: 0.204\n",
            "Epoch: [1/2] | Iterations: [1207/1870] | Training loss: 0.084\n",
            "Epoch: [1/2] | Iterations: [1208/1870] | Training loss: 0.079\n",
            "Epoch: [1/2] | Iterations: [1209/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1210/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [1211/1870] | Training loss: 0.258\n",
            "Epoch: [1/2] | Iterations: [1212/1870] | Training loss: 0.230\n",
            "Epoch: [1/2] | Iterations: [1213/1870] | Training loss: 0.120\n",
            "Epoch: [1/2] | Iterations: [1214/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [1215/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1216/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [1217/1870] | Training loss: 0.104\n",
            "Epoch: [1/2] | Iterations: [1218/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [1219/1870] | Training loss: 0.130\n",
            "Epoch: [1/2] | Iterations: [1220/1870] | Training loss: 0.132\n",
            "Epoch: [1/2] | Iterations: [1221/1870] | Training loss: 0.176\n",
            "Epoch: [1/2] | Iterations: [1222/1870] | Training loss: 0.206\n",
            "Epoch: [1/2] | Iterations: [1223/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [1224/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [1225/1870] | Training loss: 0.145\n",
            "Epoch: [1/2] | Iterations: [1226/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1227/1870] | Training loss: 0.137\n",
            "Epoch: [1/2] | Iterations: [1228/1870] | Training loss: 0.177\n",
            "Epoch: [1/2] | Iterations: [1229/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1230/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1231/1870] | Training loss: 0.178\n",
            "Epoch: [1/2] | Iterations: [1232/1870] | Training loss: 0.104\n",
            "Epoch: [1/2] | Iterations: [1233/1870] | Training loss: 0.176\n",
            "Epoch: [1/2] | Iterations: [1234/1870] | Training loss: 0.249\n",
            "Epoch: [1/2] | Iterations: [1235/1870] | Training loss: 0.238\n",
            "Epoch: [1/2] | Iterations: [1236/1870] | Training loss: 0.257\n",
            "Epoch: [1/2] | Iterations: [1237/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [1238/1870] | Training loss: 0.292\n",
            "Epoch: [1/2] | Iterations: [1239/1870] | Training loss: 0.100\n",
            "Epoch: [1/2] | Iterations: [1240/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [1241/1870] | Training loss: 0.193\n",
            "Epoch: [1/2] | Iterations: [1242/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [1243/1870] | Training loss: 0.119\n",
            "Epoch: [1/2] | Iterations: [1244/1870] | Training loss: 0.099\n",
            "Epoch: [1/2] | Iterations: [1245/1870] | Training loss: 0.140\n",
            "Epoch: [1/2] | Iterations: [1246/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1247/1870] | Training loss: 0.125\n",
            "Epoch: [1/2] | Iterations: [1248/1870] | Training loss: 0.135\n",
            "Epoch: [1/2] | Iterations: [1249/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [1250/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [1251/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [1252/1870] | Training loss: 0.207\n",
            "Epoch: [1/2] | Iterations: [1253/1870] | Training loss: 0.116\n",
            "Epoch: [1/2] | Iterations: [1254/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [1255/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [1256/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [1257/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [1258/1870] | Training loss: 0.121\n",
            "Epoch: [1/2] | Iterations: [1259/1870] | Training loss: 0.254\n",
            "Epoch: [1/2] | Iterations: [1260/1870] | Training loss: 0.224\n",
            "Epoch: [1/2] | Iterations: [1261/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [1262/1870] | Training loss: 0.142\n",
            "Epoch: [1/2] | Iterations: [1263/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [1264/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1265/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1266/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1267/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [1268/1870] | Training loss: 0.115\n",
            "Epoch: [1/2] | Iterations: [1269/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [1270/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1271/1870] | Training loss: 0.148\n",
            "Epoch: [1/2] | Iterations: [1272/1870] | Training loss: 0.263\n",
            "Epoch: [1/2] | Iterations: [1273/1870] | Training loss: 0.219\n",
            "Epoch: [1/2] | Iterations: [1274/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1275/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [1276/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [1277/1870] | Training loss: 0.223\n",
            "Epoch: [1/2] | Iterations: [1278/1870] | Training loss: 0.105\n",
            "Epoch: [1/2] | Iterations: [1279/1870] | Training loss: 0.161\n",
            "Epoch: [1/2] | Iterations: [1280/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [1281/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [1282/1870] | Training loss: 0.106\n",
            "Epoch: [1/2] | Iterations: [1283/1870] | Training loss: 0.161\n",
            "Epoch: [1/2] | Iterations: [1284/1870] | Training loss: 0.107\n",
            "Epoch: [1/2] | Iterations: [1285/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [1286/1870] | Training loss: 0.273\n",
            "Epoch: [1/2] | Iterations: [1287/1870] | Training loss: 0.119\n",
            "Epoch: [1/2] | Iterations: [1288/1870] | Training loss: 0.199\n",
            "Epoch: [1/2] | Iterations: [1289/1870] | Training loss: 0.236\n",
            "Epoch: [1/2] | Iterations: [1290/1870] | Training loss: 0.114\n",
            "Epoch: [1/2] | Iterations: [1291/1870] | Training loss: 0.268\n",
            "Epoch: [1/2] | Iterations: [1292/1870] | Training loss: 0.313\n",
            "Epoch: [1/2] | Iterations: [1293/1870] | Training loss: 0.087\n",
            "Epoch: [1/2] | Iterations: [1294/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [1295/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1296/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [1297/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [1298/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [1299/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [1300/1870] | Training loss: 0.134\n",
            "Epoch: [1/2] | Iterations: [1301/1870] | Training loss: 0.232\n",
            "Epoch: [1/2] | Iterations: [1302/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [1303/1870] | Training loss: 0.200\n",
            "Epoch: [1/2] | Iterations: [1304/1870] | Training loss: 0.072\n",
            "Epoch: [1/2] | Iterations: [1305/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [1306/1870] | Training loss: 0.252\n",
            "Epoch: [1/2] | Iterations: [1307/1870] | Training loss: 0.083\n",
            "Epoch: [1/2] | Iterations: [1308/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [1309/1870] | Training loss: 0.114\n",
            "Epoch: [1/2] | Iterations: [1310/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [1311/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [1312/1870] | Training loss: 0.193\n",
            "Epoch: [1/2] | Iterations: [1313/1870] | Training loss: 0.193\n",
            "Epoch: [1/2] | Iterations: [1314/1870] | Training loss: 0.115\n",
            "Epoch: [1/2] | Iterations: [1315/1870] | Training loss: 0.240\n",
            "Epoch: [1/2] | Iterations: [1316/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [1317/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [1318/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [1319/1870] | Training loss: 0.249\n",
            "Epoch: [1/2] | Iterations: [1320/1870] | Training loss: 0.206\n",
            "Epoch: [1/2] | Iterations: [1321/1870] | Training loss: 0.183\n",
            "Epoch: [1/2] | Iterations: [1322/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [1323/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [1324/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [1325/1870] | Training loss: 0.089\n",
            "Epoch: [1/2] | Iterations: [1326/1870] | Training loss: 0.098\n",
            "Epoch: [1/2] | Iterations: [1327/1870] | Training loss: 0.137\n",
            "Epoch: [1/2] | Iterations: [1328/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1329/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [1330/1870] | Training loss: 0.193\n",
            "Epoch: [1/2] | Iterations: [1331/1870] | Training loss: 0.257\n",
            "Epoch: [1/2] | Iterations: [1332/1870] | Training loss: 0.167\n",
            "Epoch: [1/2] | Iterations: [1333/1870] | Training loss: 0.301\n",
            "Epoch: [1/2] | Iterations: [1334/1870] | Training loss: 0.126\n",
            "Epoch: [1/2] | Iterations: [1335/1870] | Training loss: 0.224\n",
            "Epoch: [1/2] | Iterations: [1336/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [1337/1870] | Training loss: 0.151\n",
            "Epoch: [1/2] | Iterations: [1338/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1339/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [1340/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [1341/1870] | Training loss: 0.207\n",
            "Epoch: [1/2] | Iterations: [1342/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [1343/1870] | Training loss: 0.205\n",
            "Epoch: [1/2] | Iterations: [1344/1870] | Training loss: 0.125\n",
            "Epoch: [1/2] | Iterations: [1345/1870] | Training loss: 0.182\n",
            "Epoch: [1/2] | Iterations: [1346/1870] | Training loss: 0.296\n",
            "Epoch: [1/2] | Iterations: [1347/1870] | Training loss: 0.138\n",
            "Epoch: [1/2] | Iterations: [1348/1870] | Training loss: 0.141\n",
            "Epoch: [1/2] | Iterations: [1349/1870] | Training loss: 0.133\n",
            "Epoch: [1/2] | Iterations: [1350/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [1351/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1352/1870] | Training loss: 0.213\n",
            "Epoch: [1/2] | Iterations: [1353/1870] | Training loss: 0.119\n",
            "Epoch: [1/2] | Iterations: [1354/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1355/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [1356/1870] | Training loss: 0.241\n",
            "Epoch: [1/2] | Iterations: [1357/1870] | Training loss: 0.205\n",
            "Epoch: [1/2] | Iterations: [1358/1870] | Training loss: 0.167\n",
            "Epoch: [1/2] | Iterations: [1359/1870] | Training loss: 0.100\n",
            "Epoch: [1/2] | Iterations: [1360/1870] | Training loss: 0.099\n",
            "Epoch: [1/2] | Iterations: [1361/1870] | Training loss: 0.116\n",
            "Epoch: [1/2] | Iterations: [1362/1870] | Training loss: 0.114\n",
            "Epoch: [1/2] | Iterations: [1363/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [1364/1870] | Training loss: 0.177\n",
            "Epoch: [1/2] | Iterations: [1365/1870] | Training loss: 0.126\n",
            "Epoch: [1/2] | Iterations: [1366/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [1367/1870] | Training loss: 0.256\n",
            "Epoch: [1/2] | Iterations: [1368/1870] | Training loss: 0.161\n",
            "Epoch: [1/2] | Iterations: [1369/1870] | Training loss: 0.200\n",
            "Epoch: [1/2] | Iterations: [1370/1870] | Training loss: 0.121\n",
            "Epoch: [1/2] | Iterations: [1371/1870] | Training loss: 0.125\n",
            "Epoch: [1/2] | Iterations: [1372/1870] | Training loss: 0.203\n",
            "Epoch: [1/2] | Iterations: [1373/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [1374/1870] | Training loss: 0.265\n",
            "Epoch: [1/2] | Iterations: [1375/1870] | Training loss: 0.087\n",
            "Epoch: [1/2] | Iterations: [1376/1870] | Training loss: 0.214\n",
            "Epoch: [1/2] | Iterations: [1377/1870] | Training loss: 0.228\n",
            "Epoch: [1/2] | Iterations: [1378/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [1379/1870] | Training loss: 0.236\n",
            "Epoch: [1/2] | Iterations: [1380/1870] | Training loss: 0.103\n",
            "Epoch: [1/2] | Iterations: [1381/1870] | Training loss: 0.233\n",
            "Epoch: [1/2] | Iterations: [1382/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [1383/1870] | Training loss: 0.183\n",
            "Epoch: [1/2] | Iterations: [1384/1870] | Training loss: 0.135\n",
            "Epoch: [1/2] | Iterations: [1385/1870] | Training loss: 0.104\n",
            "Epoch: [1/2] | Iterations: [1386/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [1387/1870] | Training loss: 0.172\n",
            "Epoch: [1/2] | Iterations: [1388/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [1389/1870] | Training loss: 0.186\n",
            "Epoch: [1/2] | Iterations: [1390/1870] | Training loss: 0.167\n",
            "Epoch: [1/2] | Iterations: [1391/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [1392/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [1393/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1394/1870] | Training loss: 0.112\n",
            "Epoch: [1/2] | Iterations: [1395/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [1396/1870] | Training loss: 0.271\n",
            "Epoch: [1/2] | Iterations: [1397/1870] | Training loss: 0.275\n",
            "Epoch: [1/2] | Iterations: [1398/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1399/1870] | Training loss: 0.134\n",
            "Epoch: [1/2] | Iterations: [1400/1870] | Training loss: 0.223\n",
            "Epoch: [1/2] | Iterations: [1401/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [1402/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [1403/1870] | Training loss: 0.263\n",
            "Epoch: [1/2] | Iterations: [1404/1870] | Training loss: 0.205\n",
            "Epoch: [1/2] | Iterations: [1405/1870] | Training loss: 0.111\n",
            "Epoch: [1/2] | Iterations: [1406/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [1407/1870] | Training loss: 0.259\n",
            "Epoch: [1/2] | Iterations: [1408/1870] | Training loss: 0.177\n",
            "Epoch: [1/2] | Iterations: [1409/1870] | Training loss: 0.216\n",
            "Epoch: [1/2] | Iterations: [1410/1870] | Training loss: 0.260\n",
            "Epoch: [1/2] | Iterations: [1411/1870] | Training loss: 0.194\n",
            "Epoch: [1/2] | Iterations: [1412/1870] | Training loss: 0.142\n",
            "Epoch: [1/2] | Iterations: [1413/1870] | Training loss: 0.186\n",
            "Epoch: [1/2] | Iterations: [1414/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [1415/1870] | Training loss: 0.261\n",
            "Epoch: [1/2] | Iterations: [1416/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [1417/1870] | Training loss: 0.320\n",
            "Epoch: [1/2] | Iterations: [1418/1870] | Training loss: 0.251\n",
            "Epoch: [1/2] | Iterations: [1419/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1420/1870] | Training loss: 0.115\n",
            "Epoch: [1/2] | Iterations: [1421/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1422/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1423/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [1424/1870] | Training loss: 0.247\n",
            "Epoch: [1/2] | Iterations: [1425/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [1426/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [1427/1870] | Training loss: 0.222\n",
            "Epoch: [1/2] | Iterations: [1428/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [1429/1870] | Training loss: 0.233\n",
            "Epoch: [1/2] | Iterations: [1430/1870] | Training loss: 0.238\n",
            "Epoch: [1/2] | Iterations: [1431/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1432/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [1433/1870] | Training loss: 0.115\n",
            "Epoch: [1/2] | Iterations: [1434/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [1435/1870] | Training loss: 0.230\n",
            "Epoch: [1/2] | Iterations: [1436/1870] | Training loss: 0.111\n",
            "Epoch: [1/2] | Iterations: [1437/1870] | Training loss: 0.109\n",
            "Epoch: [1/2] | Iterations: [1438/1870] | Training loss: 0.207\n",
            "Epoch: [1/2] | Iterations: [1439/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1440/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [1441/1870] | Training loss: 0.250\n",
            "Epoch: [1/2] | Iterations: [1442/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [1443/1870] | Training loss: 0.096\n",
            "Epoch: [1/2] | Iterations: [1444/1870] | Training loss: 0.292\n",
            "Epoch: [1/2] | Iterations: [1445/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [1446/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [1447/1870] | Training loss: 0.202\n",
            "Epoch: [1/2] | Iterations: [1448/1870] | Training loss: 0.276\n",
            "Epoch: [1/2] | Iterations: [1449/1870] | Training loss: 0.203\n",
            "Epoch: [1/2] | Iterations: [1450/1870] | Training loss: 0.305\n",
            "Epoch: [1/2] | Iterations: [1451/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [1452/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [1453/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [1454/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [1455/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [1456/1870] | Training loss: 0.120\n",
            "Epoch: [1/2] | Iterations: [1457/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1458/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [1459/1870] | Training loss: 0.119\n",
            "Epoch: [1/2] | Iterations: [1460/1870] | Training loss: 0.173\n",
            "Epoch: [1/2] | Iterations: [1461/1870] | Training loss: 0.243\n",
            "Epoch: [1/2] | Iterations: [1462/1870] | Training loss: 0.154\n",
            "Epoch: [1/2] | Iterations: [1463/1870] | Training loss: 0.132\n",
            "Epoch: [1/2] | Iterations: [1464/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [1465/1870] | Training loss: 0.224\n",
            "Epoch: [1/2] | Iterations: [1466/1870] | Training loss: 0.123\n",
            "Epoch: [1/2] | Iterations: [1467/1870] | Training loss: 0.088\n",
            "Epoch: [1/2] | Iterations: [1468/1870] | Training loss: 0.168\n",
            "Epoch: [1/2] | Iterations: [1469/1870] | Training loss: 0.178\n",
            "Epoch: [1/2] | Iterations: [1470/1870] | Training loss: 0.120\n",
            "Epoch: [1/2] | Iterations: [1471/1870] | Training loss: 0.218\n",
            "Epoch: [1/2] | Iterations: [1472/1870] | Training loss: 0.087\n",
            "Epoch: [1/2] | Iterations: [1473/1870] | Training loss: 0.133\n",
            "Epoch: [1/2] | Iterations: [1474/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1475/1870] | Training loss: 0.252\n",
            "Epoch: [1/2] | Iterations: [1476/1870] | Training loss: 0.310\n",
            "Epoch: [1/2] | Iterations: [1477/1870] | Training loss: 0.173\n",
            "Epoch: [1/2] | Iterations: [1478/1870] | Training loss: 0.164\n",
            "Epoch: [1/2] | Iterations: [1479/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [1480/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [1481/1870] | Training loss: 0.112\n",
            "Epoch: [1/2] | Iterations: [1482/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [1483/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1484/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1485/1870] | Training loss: 0.173\n",
            "Epoch: [1/2] | Iterations: [1486/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [1487/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [1488/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [1489/1870] | Training loss: 0.123\n",
            "Epoch: [1/2] | Iterations: [1490/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1491/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [1492/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [1493/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [1494/1870] | Training loss: 0.109\n",
            "Epoch: [1/2] | Iterations: [1495/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [1496/1870] | Training loss: 0.083\n",
            "Epoch: [1/2] | Iterations: [1497/1870] | Training loss: 0.083\n",
            "Epoch: [1/2] | Iterations: [1498/1870] | Training loss: 0.191\n",
            "Epoch: [1/2] | Iterations: [1499/1870] | Training loss: 0.260\n",
            "Epoch: [1/2] | Iterations: [1500/1870] | Training loss: 0.235\n",
            "Epoch: [1/2] | Iterations: [1501/1870] | Training loss: 0.196\n",
            "Epoch: [1/2] | Iterations: [1502/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [1503/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [1504/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1505/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [1506/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [1507/1870] | Training loss: 0.222\n",
            "Epoch: [1/2] | Iterations: [1508/1870] | Training loss: 0.272\n",
            "Epoch: [1/2] | Iterations: [1509/1870] | Training loss: 0.176\n",
            "Epoch: [1/2] | Iterations: [1510/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1511/1870] | Training loss: 0.150\n",
            "Epoch: [1/2] | Iterations: [1512/1870] | Training loss: 0.123\n",
            "Epoch: [1/2] | Iterations: [1513/1870] | Training loss: 0.111\n",
            "Epoch: [1/2] | Iterations: [1514/1870] | Training loss: 0.183\n",
            "Epoch: [1/2] | Iterations: [1515/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [1516/1870] | Training loss: 0.204\n",
            "Epoch: [1/2] | Iterations: [1517/1870] | Training loss: 0.388\n",
            "Epoch: [1/2] | Iterations: [1518/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1519/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [1520/1870] | Training loss: 0.086\n",
            "Epoch: [1/2] | Iterations: [1521/1870] | Training loss: 0.172\n",
            "Epoch: [1/2] | Iterations: [1522/1870] | Training loss: 0.194\n",
            "Epoch: [1/2] | Iterations: [1523/1870] | Training loss: 0.240\n",
            "Epoch: [1/2] | Iterations: [1524/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [1525/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [1526/1870] | Training loss: 0.142\n",
            "Epoch: [1/2] | Iterations: [1527/1870] | Training loss: 0.193\n",
            "Epoch: [1/2] | Iterations: [1528/1870] | Training loss: 0.291\n",
            "Epoch: [1/2] | Iterations: [1529/1870] | Training loss: 0.142\n",
            "Epoch: [1/2] | Iterations: [1530/1870] | Training loss: 0.115\n",
            "Epoch: [1/2] | Iterations: [1531/1870] | Training loss: 0.173\n",
            "Epoch: [1/2] | Iterations: [1532/1870] | Training loss: 0.167\n",
            "Epoch: [1/2] | Iterations: [1533/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [1534/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [1535/1870] | Training loss: 0.221\n",
            "Epoch: [1/2] | Iterations: [1536/1870] | Training loss: 0.244\n",
            "Epoch: [1/2] | Iterations: [1537/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [1538/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [1539/1870] | Training loss: 0.137\n",
            "Epoch: [1/2] | Iterations: [1540/1870] | Training loss: 0.218\n",
            "Epoch: [1/2] | Iterations: [1541/1870] | Training loss: 0.110\n",
            "Epoch: [1/2] | Iterations: [1542/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1543/1870] | Training loss: 0.206\n",
            "Epoch: [1/2] | Iterations: [1544/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [1545/1870] | Training loss: 0.117\n",
            "Epoch: [1/2] | Iterations: [1546/1870] | Training loss: 0.120\n",
            "Epoch: [1/2] | Iterations: [1547/1870] | Training loss: 0.158\n",
            "Epoch: [1/2] | Iterations: [1548/1870] | Training loss: 0.109\n",
            "Epoch: [1/2] | Iterations: [1549/1870] | Training loss: 0.070\n",
            "Epoch: [1/2] | Iterations: [1550/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [1551/1870] | Training loss: 0.168\n",
            "Epoch: [1/2] | Iterations: [1552/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [1553/1870] | Training loss: 0.174\n",
            "Epoch: [1/2] | Iterations: [1554/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [1555/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1556/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [1557/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [1558/1870] | Training loss: 0.178\n",
            "Epoch: [1/2] | Iterations: [1559/1870] | Training loss: 0.095\n",
            "Epoch: [1/2] | Iterations: [1560/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [1561/1870] | Training loss: 0.141\n",
            "Epoch: [1/2] | Iterations: [1562/1870] | Training loss: 0.199\n",
            "Epoch: [1/2] | Iterations: [1563/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1564/1870] | Training loss: 0.106\n",
            "Epoch: [1/2] | Iterations: [1565/1870] | Training loss: 0.074\n",
            "Epoch: [1/2] | Iterations: [1566/1870] | Training loss: 0.076\n",
            "Epoch: [1/2] | Iterations: [1567/1870] | Training loss: 0.249\n",
            "Epoch: [1/2] | Iterations: [1568/1870] | Training loss: 0.233\n",
            "Epoch: [1/2] | Iterations: [1569/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [1570/1870] | Training loss: 0.134\n",
            "Epoch: [1/2] | Iterations: [1571/1870] | Training loss: 0.098\n",
            "Epoch: [1/2] | Iterations: [1572/1870] | Training loss: 0.196\n",
            "Epoch: [1/2] | Iterations: [1573/1870] | Training loss: 0.056\n",
            "Epoch: [1/2] | Iterations: [1574/1870] | Training loss: 0.171\n",
            "Epoch: [1/2] | Iterations: [1575/1870] | Training loss: 0.298\n",
            "Epoch: [1/2] | Iterations: [1576/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [1577/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [1578/1870] | Training loss: 0.108\n",
            "Epoch: [1/2] | Iterations: [1579/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [1580/1870] | Training loss: 0.113\n",
            "Epoch: [1/2] | Iterations: [1581/1870] | Training loss: 0.099\n",
            "Epoch: [1/2] | Iterations: [1582/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1583/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [1584/1870] | Training loss: 0.074\n",
            "Epoch: [1/2] | Iterations: [1585/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1586/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [1587/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [1588/1870] | Training loss: 0.109\n",
            "Epoch: [1/2] | Iterations: [1589/1870] | Training loss: 0.145\n",
            "Epoch: [1/2] | Iterations: [1590/1870] | Training loss: 0.231\n",
            "Epoch: [1/2] | Iterations: [1591/1870] | Training loss: 0.178\n",
            "Epoch: [1/2] | Iterations: [1592/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1593/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [1594/1870] | Training loss: 0.120\n",
            "Epoch: [1/2] | Iterations: [1595/1870] | Training loss: 0.096\n",
            "Epoch: [1/2] | Iterations: [1596/1870] | Training loss: 0.218\n",
            "Epoch: [1/2] | Iterations: [1597/1870] | Training loss: 0.267\n",
            "Epoch: [1/2] | Iterations: [1598/1870] | Training loss: 0.223\n",
            "Epoch: [1/2] | Iterations: [1599/1870] | Training loss: 0.152\n",
            "Epoch: [1/2] | Iterations: [1600/1870] | Training loss: 0.162\n",
            "Epoch: [1/2] | Iterations: [1601/1870] | Training loss: 0.167\n",
            "Epoch: [1/2] | Iterations: [1602/1870] | Training loss: 0.233\n",
            "Epoch: [1/2] | Iterations: [1603/1870] | Training loss: 0.164\n",
            "Epoch: [1/2] | Iterations: [1604/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1605/1870] | Training loss: 0.170\n",
            "Epoch: [1/2] | Iterations: [1606/1870] | Training loss: 0.284\n",
            "Epoch: [1/2] | Iterations: [1607/1870] | Training loss: 0.151\n",
            "Epoch: [1/2] | Iterations: [1608/1870] | Training loss: 0.132\n",
            "Epoch: [1/2] | Iterations: [1609/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [1610/1870] | Training loss: 0.122\n",
            "Epoch: [1/2] | Iterations: [1611/1870] | Training loss: 0.119\n",
            "Epoch: [1/2] | Iterations: [1612/1870] | Training loss: 0.216\n",
            "Epoch: [1/2] | Iterations: [1613/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1614/1870] | Training loss: 0.137\n",
            "Epoch: [1/2] | Iterations: [1615/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [1616/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1617/1870] | Training loss: 0.281\n",
            "Epoch: [1/2] | Iterations: [1618/1870] | Training loss: 0.138\n",
            "Epoch: [1/2] | Iterations: [1619/1870] | Training loss: 0.098\n",
            "Epoch: [1/2] | Iterations: [1620/1870] | Training loss: 0.232\n",
            "Epoch: [1/2] | Iterations: [1621/1870] | Training loss: 0.112\n",
            "Epoch: [1/2] | Iterations: [1622/1870] | Training loss: 0.138\n",
            "Epoch: [1/2] | Iterations: [1623/1870] | Training loss: 0.203\n",
            "Epoch: [1/2] | Iterations: [1624/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [1625/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1626/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [1627/1870] | Training loss: 0.217\n",
            "Epoch: [1/2] | Iterations: [1628/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1629/1870] | Training loss: 0.245\n",
            "Epoch: [1/2] | Iterations: [1630/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [1631/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [1632/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1633/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [1634/1870] | Training loss: 0.246\n",
            "Epoch: [1/2] | Iterations: [1635/1870] | Training loss: 0.087\n",
            "Epoch: [1/2] | Iterations: [1636/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1637/1870] | Training loss: 0.115\n",
            "Epoch: [1/2] | Iterations: [1638/1870] | Training loss: 0.204\n",
            "Epoch: [1/2] | Iterations: [1639/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1640/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [1641/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [1642/1870] | Training loss: 0.120\n",
            "Epoch: [1/2] | Iterations: [1643/1870] | Training loss: 0.248\n",
            "Epoch: [1/2] | Iterations: [1644/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [1645/1870] | Training loss: 0.113\n",
            "Epoch: [1/2] | Iterations: [1646/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [1647/1870] | Training loss: 0.133\n",
            "Epoch: [1/2] | Iterations: [1648/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [1649/1870] | Training loss: 0.172\n",
            "Epoch: [1/2] | Iterations: [1650/1870] | Training loss: 0.271\n",
            "Epoch: [1/2] | Iterations: [1651/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1652/1870] | Training loss: 0.126\n",
            "Epoch: [1/2] | Iterations: [1653/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1654/1870] | Training loss: 0.154\n",
            "Epoch: [1/2] | Iterations: [1655/1870] | Training loss: 0.169\n",
            "Epoch: [1/2] | Iterations: [1656/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1657/1870] | Training loss: 0.089\n",
            "Epoch: [1/2] | Iterations: [1658/1870] | Training loss: 0.181\n",
            "Epoch: [1/2] | Iterations: [1659/1870] | Training loss: 0.161\n",
            "Epoch: [1/2] | Iterations: [1660/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [1661/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [1662/1870] | Training loss: 0.121\n",
            "Epoch: [1/2] | Iterations: [1663/1870] | Training loss: 0.132\n",
            "Epoch: [1/2] | Iterations: [1664/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1665/1870] | Training loss: 0.135\n",
            "Epoch: [1/2] | Iterations: [1666/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [1667/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1668/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [1669/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1670/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1671/1870] | Training loss: 0.156\n",
            "Epoch: [1/2] | Iterations: [1672/1870] | Training loss: 0.188\n",
            "Epoch: [1/2] | Iterations: [1673/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [1674/1870] | Training loss: 0.108\n",
            "Epoch: [1/2] | Iterations: [1675/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1676/1870] | Training loss: 0.086\n",
            "Epoch: [1/2] | Iterations: [1677/1870] | Training loss: 0.091\n",
            "Epoch: [1/2] | Iterations: [1678/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1679/1870] | Training loss: 0.059\n",
            "Epoch: [1/2] | Iterations: [1680/1870] | Training loss: 0.141\n",
            "Epoch: [1/2] | Iterations: [1681/1870] | Training loss: 0.096\n",
            "Epoch: [1/2] | Iterations: [1682/1870] | Training loss: 0.185\n",
            "Epoch: [1/2] | Iterations: [1683/1870] | Training loss: 0.107\n",
            "Epoch: [1/2] | Iterations: [1684/1870] | Training loss: 0.184\n",
            "Epoch: [1/2] | Iterations: [1685/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [1686/1870] | Training loss: 0.108\n",
            "Epoch: [1/2] | Iterations: [1687/1870] | Training loss: 0.145\n",
            "Epoch: [1/2] | Iterations: [1688/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1689/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [1690/1870] | Training loss: 0.151\n",
            "Epoch: [1/2] | Iterations: [1691/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1692/1870] | Training loss: 0.085\n",
            "Epoch: [1/2] | Iterations: [1693/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1694/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [1695/1870] | Training loss: 0.112\n",
            "Epoch: [1/2] | Iterations: [1696/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [1697/1870] | Training loss: 0.250\n",
            "Epoch: [1/2] | Iterations: [1698/1870] | Training loss: 0.161\n",
            "Epoch: [1/2] | Iterations: [1699/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [1700/1870] | Training loss: 0.094\n",
            "Epoch: [1/2] | Iterations: [1701/1870] | Training loss: 0.101\n",
            "Epoch: [1/2] | Iterations: [1702/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [1703/1870] | Training loss: 0.148\n",
            "Epoch: [1/2] | Iterations: [1704/1870] | Training loss: 0.142\n",
            "Epoch: [1/2] | Iterations: [1705/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [1706/1870] | Training loss: 0.168\n",
            "Epoch: [1/2] | Iterations: [1707/1870] | Training loss: 0.085\n",
            "Epoch: [1/2] | Iterations: [1708/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1709/1870] | Training loss: 0.126\n",
            "Epoch: [1/2] | Iterations: [1710/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [1711/1870] | Training loss: 0.121\n",
            "Epoch: [1/2] | Iterations: [1712/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [1713/1870] | Training loss: 0.238\n",
            "Epoch: [1/2] | Iterations: [1714/1870] | Training loss: 0.233\n",
            "Epoch: [1/2] | Iterations: [1715/1870] | Training loss: 0.165\n",
            "Epoch: [1/2] | Iterations: [1716/1870] | Training loss: 0.226\n",
            "Epoch: [1/2] | Iterations: [1717/1870] | Training loss: 0.172\n",
            "Epoch: [1/2] | Iterations: [1718/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [1719/1870] | Training loss: 0.177\n",
            "Epoch: [1/2] | Iterations: [1720/1870] | Training loss: 0.146\n",
            "Epoch: [1/2] | Iterations: [1721/1870] | Training loss: 0.121\n",
            "Epoch: [1/2] | Iterations: [1722/1870] | Training loss: 0.078\n",
            "Epoch: [1/2] | Iterations: [1723/1870] | Training loss: 0.104\n",
            "Epoch: [1/2] | Iterations: [1724/1870] | Training loss: 0.138\n",
            "Epoch: [1/2] | Iterations: [1725/1870] | Training loss: 0.159\n",
            "Epoch: [1/2] | Iterations: [1726/1870] | Training loss: 0.234\n",
            "Epoch: [1/2] | Iterations: [1727/1870] | Training loss: 0.190\n",
            "Epoch: [1/2] | Iterations: [1728/1870] | Training loss: 0.155\n",
            "Epoch: [1/2] | Iterations: [1729/1870] | Training loss: 0.259\n",
            "Epoch: [1/2] | Iterations: [1730/1870] | Training loss: 0.248\n",
            "Epoch: [1/2] | Iterations: [1731/1870] | Training loss: 0.065\n",
            "Epoch: [1/2] | Iterations: [1732/1870] | Training loss: 0.182\n",
            "Epoch: [1/2] | Iterations: [1733/1870] | Training loss: 0.235\n",
            "Epoch: [1/2] | Iterations: [1734/1870] | Training loss: 0.129\n",
            "Epoch: [1/2] | Iterations: [1735/1870] | Training loss: 0.169\n",
            "Epoch: [1/2] | Iterations: [1736/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [1737/1870] | Training loss: 0.169\n",
            "Epoch: [1/2] | Iterations: [1738/1870] | Training loss: 0.068\n",
            "Epoch: [1/2] | Iterations: [1739/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1740/1870] | Training loss: 0.101\n",
            "Epoch: [1/2] | Iterations: [1741/1870] | Training loss: 0.107\n",
            "Epoch: [1/2] | Iterations: [1742/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [1743/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [1744/1870] | Training loss: 0.114\n",
            "Epoch: [1/2] | Iterations: [1745/1870] | Training loss: 0.112\n",
            "Epoch: [1/2] | Iterations: [1746/1870] | Training loss: 0.074\n",
            "Epoch: [1/2] | Iterations: [1747/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1748/1870] | Training loss: 0.265\n",
            "Epoch: [1/2] | Iterations: [1749/1870] | Training loss: 0.243\n",
            "Epoch: [1/2] | Iterations: [1750/1870] | Training loss: 0.104\n",
            "Epoch: [1/2] | Iterations: [1751/1870] | Training loss: 0.115\n",
            "Epoch: [1/2] | Iterations: [1752/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [1753/1870] | Training loss: 0.225\n",
            "Epoch: [1/2] | Iterations: [1754/1870] | Training loss: 0.211\n",
            "Epoch: [1/2] | Iterations: [1755/1870] | Training loss: 0.157\n",
            "Epoch: [1/2] | Iterations: [1756/1870] | Training loss: 0.168\n",
            "Epoch: [1/2] | Iterations: [1757/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1758/1870] | Training loss: 0.150\n",
            "Epoch: [1/2] | Iterations: [1759/1870] | Training loss: 0.121\n",
            "Epoch: [1/2] | Iterations: [1760/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1761/1870] | Training loss: 0.137\n",
            "Epoch: [1/2] | Iterations: [1762/1870] | Training loss: 0.123\n",
            "Epoch: [1/2] | Iterations: [1763/1870] | Training loss: 0.103\n",
            "Epoch: [1/2] | Iterations: [1764/1870] | Training loss: 0.151\n",
            "Epoch: [1/2] | Iterations: [1765/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [1766/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [1767/1870] | Training loss: 0.103\n",
            "Epoch: [1/2] | Iterations: [1768/1870] | Training loss: 0.088\n",
            "Epoch: [1/2] | Iterations: [1769/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1770/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [1771/1870] | Training loss: 0.115\n",
            "Epoch: [1/2] | Iterations: [1772/1870] | Training loss: 0.066\n",
            "Epoch: [1/2] | Iterations: [1773/1870] | Training loss: 0.147\n",
            "Epoch: [1/2] | Iterations: [1774/1870] | Training loss: 0.229\n",
            "Epoch: [1/2] | Iterations: [1775/1870] | Training loss: 0.102\n",
            "Epoch: [1/2] | Iterations: [1776/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1777/1870] | Training loss: 0.153\n",
            "Epoch: [1/2] | Iterations: [1778/1870] | Training loss: 0.085\n",
            "Epoch: [1/2] | Iterations: [1779/1870] | Training loss: 0.097\n",
            "Epoch: [1/2] | Iterations: [1780/1870] | Training loss: 0.169\n",
            "Epoch: [1/2] | Iterations: [1781/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [1782/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1783/1870] | Training loss: 0.234\n",
            "Epoch: [1/2] | Iterations: [1784/1870] | Training loss: 0.124\n",
            "Epoch: [1/2] | Iterations: [1785/1870] | Training loss: 0.114\n",
            "Epoch: [1/2] | Iterations: [1786/1870] | Training loss: 0.095\n",
            "Epoch: [1/2] | Iterations: [1787/1870] | Training loss: 0.247\n",
            "Epoch: [1/2] | Iterations: [1788/1870] | Training loss: 0.166\n",
            "Epoch: [1/2] | Iterations: [1789/1870] | Training loss: 0.195\n",
            "Epoch: [1/2] | Iterations: [1790/1870] | Training loss: 0.128\n",
            "Epoch: [1/2] | Iterations: [1791/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [1792/1870] | Training loss: 0.140\n",
            "Epoch: [1/2] | Iterations: [1793/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1794/1870] | Training loss: 0.143\n",
            "Epoch: [1/2] | Iterations: [1795/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [1796/1870] | Training loss: 0.074\n",
            "Epoch: [1/2] | Iterations: [1797/1870] | Training loss: 0.082\n",
            "Epoch: [1/2] | Iterations: [1798/1870] | Training loss: 0.139\n",
            "Epoch: [1/2] | Iterations: [1799/1870] | Training loss: 0.175\n",
            "Epoch: [1/2] | Iterations: [1800/1870] | Training loss: 0.100\n",
            "Epoch: [1/2] | Iterations: [1801/1870] | Training loss: 0.098\n",
            "Epoch: [1/2] | Iterations: [1802/1870] | Training loss: 0.118\n",
            "Epoch: [1/2] | Iterations: [1803/1870] | Training loss: 0.164\n",
            "Epoch: [1/2] | Iterations: [1804/1870] | Training loss: 0.133\n",
            "Epoch: [1/2] | Iterations: [1805/1870] | Training loss: 0.117\n",
            "Epoch: [1/2] | Iterations: [1806/1870] | Training loss: 0.135\n",
            "Epoch: [1/2] | Iterations: [1807/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [1808/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [1809/1870] | Training loss: 0.198\n",
            "Epoch: [1/2] | Iterations: [1810/1870] | Training loss: 0.148\n",
            "Epoch: [1/2] | Iterations: [1811/1870] | Training loss: 0.122\n",
            "Epoch: [1/2] | Iterations: [1812/1870] | Training loss: 0.168\n",
            "Epoch: [1/2] | Iterations: [1813/1870] | Training loss: 0.151\n",
            "Epoch: [1/2] | Iterations: [1814/1870] | Training loss: 0.127\n",
            "Epoch: [1/2] | Iterations: [1815/1870] | Training loss: 0.123\n",
            "Epoch: [1/2] | Iterations: [1816/1870] | Training loss: 0.180\n",
            "Epoch: [1/2] | Iterations: [1817/1870] | Training loss: 0.111\n",
            "Epoch: [1/2] | Iterations: [1818/1870] | Training loss: 0.136\n",
            "Epoch: [1/2] | Iterations: [1819/1870] | Training loss: 0.212\n",
            "Epoch: [1/2] | Iterations: [1820/1870] | Training loss: 0.107\n",
            "Epoch: [1/2] | Iterations: [1821/1870] | Training loss: 0.149\n",
            "Epoch: [1/2] | Iterations: [1822/1870] | Training loss: 0.210\n",
            "Epoch: [1/2] | Iterations: [1823/1870] | Training loss: 0.198\n",
            "Epoch: [1/2] | Iterations: [1824/1870] | Training loss: 0.133\n",
            "Epoch: [1/2] | Iterations: [1825/1870] | Training loss: 0.179\n",
            "Epoch: [1/2] | Iterations: [1826/1870] | Training loss: 0.080\n",
            "Epoch: [1/2] | Iterations: [1827/1870] | Training loss: 0.144\n",
            "Epoch: [1/2] | Iterations: [1828/1870] | Training loss: 0.181\n",
            "Epoch: [1/2] | Iterations: [1829/1870] | Training loss: 0.110\n",
            "Epoch: [1/2] | Iterations: [1830/1870] | Training loss: 0.147\n",
            "Epoch: [1/2] | Iterations: [1831/1870] | Training loss: 0.151\n",
            "Epoch: [1/2] | Iterations: [1832/1870] | Training loss: 0.122\n",
            "Epoch: [1/2] | Iterations: [1833/1870] | Training loss: 0.208\n",
            "Epoch: [1/2] | Iterations: [1834/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1835/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [1836/1870] | Training loss: 0.159\n",
            "Epoch: [1/2] | Iterations: [1837/1870] | Training loss: 0.140\n",
            "Epoch: [1/2] | Iterations: [1838/1870] | Training loss: 0.089\n",
            "Epoch: [1/2] | Iterations: [1839/1870] | Training loss: 0.125\n",
            "Epoch: [1/2] | Iterations: [1840/1870] | Training loss: 0.135\n",
            "Epoch: [1/2] | Iterations: [1841/1870] | Training loss: 0.163\n",
            "Epoch: [1/2] | Iterations: [1842/1870] | Training loss: 0.150\n",
            "Epoch: [1/2] | Iterations: [1843/1870] | Training loss: 0.164\n",
            "Epoch: [1/2] | Iterations: [1844/1870] | Training loss: 0.086\n",
            "Epoch: [1/2] | Iterations: [1845/1870] | Training loss: 0.226\n",
            "Epoch: [1/2] | Iterations: [1846/1870] | Training loss: 0.218\n",
            "Epoch: [1/2] | Iterations: [1847/1870] | Training loss: 0.131\n",
            "Epoch: [1/2] | Iterations: [1848/1870] | Training loss: 0.239\n",
            "Epoch: [1/2] | Iterations: [1849/1870] | Training loss: 0.059\n",
            "Epoch: [1/2] | Iterations: [1850/1870] | Training loss: 0.125\n",
            "Epoch: [1/2] | Iterations: [1851/1870] | Training loss: 0.086\n",
            "Epoch: [1/2] | Iterations: [1852/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [1853/1870] | Training loss: 0.125\n",
            "Epoch: [1/2] | Iterations: [1854/1870] | Training loss: 0.187\n",
            "Epoch: [1/2] | Iterations: [1855/1870] | Training loss: 0.160\n",
            "Epoch: [1/2] | Iterations: [1856/1870] | Training loss: 0.189\n",
            "Epoch: [1/2] | Iterations: [1857/1870] | Training loss: 0.108\n",
            "Epoch: [1/2] | Iterations: [1858/1870] | Training loss: 0.287\n",
            "Epoch: [1/2] | Iterations: [1859/1870] | Training loss: 0.272\n",
            "Epoch: [1/2] | Iterations: [1860/1870] | Training loss: 0.227\n",
            "Epoch: [1/2] | Iterations: [1861/1870] | Training loss: 0.188\n",
            "Epoch: [1/2] | Iterations: [1862/1870] | Training loss: 0.215\n",
            "Epoch: [1/2] | Iterations: [1863/1870] | Training loss: 0.192\n",
            "Epoch: [1/2] | Iterations: [1864/1870] | Training loss: 0.095\n",
            "Epoch: [1/2] | Iterations: [1865/1870] | Training loss: 0.209\n",
            "Epoch: [1/2] | Iterations: [1866/1870] | Training loss: 0.098\n",
            "Epoch: [1/2] | Iterations: [1867/1870] | Training loss: 0.220\n",
            "Epoch: [1/2] | Iterations: [1868/1870] | Training loss: 0.181\n",
            "Epoch: [1/2] | Iterations: [1869/1870] | Training loss: 0.167\n",
            "Epoch: [1/2] | Iterations: [1870/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [1/1870] | Training loss: 0.190\n",
            "Epoch: [2/2] | Iterations: [2/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [3/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [4/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [5/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [6/1870] | Training loss: 0.242\n",
            "Epoch: [2/2] | Iterations: [7/1870] | Training loss: 0.281\n",
            "Epoch: [2/2] | Iterations: [8/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [9/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [10/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [11/1870] | Training loss: 0.213\n",
            "Epoch: [2/2] | Iterations: [12/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [13/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [14/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [15/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [16/1870] | Training loss: 0.093\n",
            "Epoch: [2/2] | Iterations: [17/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [18/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [19/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [20/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [21/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [22/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [23/1870] | Training loss: 0.095\n",
            "Epoch: [2/2] | Iterations: [24/1870] | Training loss: 0.220\n",
            "Epoch: [2/2] | Iterations: [25/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [26/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [27/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [28/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [29/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [30/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [31/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [32/1870] | Training loss: 0.250\n",
            "Epoch: [2/2] | Iterations: [33/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [34/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [35/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [36/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [37/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [38/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [39/1870] | Training loss: 0.226\n",
            "Epoch: [2/2] | Iterations: [40/1870] | Training loss: 0.208\n",
            "Epoch: [2/2] | Iterations: [41/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [42/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [43/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [44/1870] | Training loss: 0.230\n",
            "Epoch: [2/2] | Iterations: [45/1870] | Training loss: 0.201\n",
            "Epoch: [2/2] | Iterations: [46/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [47/1870] | Training loss: 0.227\n",
            "Epoch: [2/2] | Iterations: [48/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [49/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [50/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [51/1870] | Training loss: 0.083\n",
            "Epoch: [2/2] | Iterations: [52/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [53/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [54/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [55/1870] | Training loss: 0.079\n",
            "Epoch: [2/2] | Iterations: [56/1870] | Training loss: 0.084\n",
            "Epoch: [2/2] | Iterations: [57/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [58/1870] | Training loss: 0.105\n",
            "Epoch: [2/2] | Iterations: [59/1870] | Training loss: 0.059\n",
            "Epoch: [2/2] | Iterations: [60/1870] | Training loss: 0.093\n",
            "Epoch: [2/2] | Iterations: [61/1870] | Training loss: 0.227\n",
            "Epoch: [2/2] | Iterations: [62/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [63/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [64/1870] | Training loss: 0.275\n",
            "Epoch: [2/2] | Iterations: [65/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [66/1870] | Training loss: 0.244\n",
            "Epoch: [2/2] | Iterations: [67/1870] | Training loss: 0.097\n",
            "Epoch: [2/2] | Iterations: [68/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [69/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [70/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [71/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [72/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [73/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [74/1870] | Training loss: 0.179\n",
            "Epoch: [2/2] | Iterations: [75/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [76/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [77/1870] | Training loss: 0.205\n",
            "Epoch: [2/2] | Iterations: [78/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [79/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [80/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [81/1870] | Training loss: 0.185\n",
            "Epoch: [2/2] | Iterations: [82/1870] | Training loss: 0.257\n",
            "Epoch: [2/2] | Iterations: [83/1870] | Training loss: 0.225\n",
            "Epoch: [2/2] | Iterations: [84/1870] | Training loss: 0.157\n",
            "Epoch: [2/2] | Iterations: [85/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [86/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [87/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [88/1870] | Training loss: 0.227\n",
            "Epoch: [2/2] | Iterations: [89/1870] | Training loss: 0.324\n",
            "Epoch: [2/2] | Iterations: [90/1870] | Training loss: 0.216\n",
            "Epoch: [2/2] | Iterations: [91/1870] | Training loss: 0.157\n",
            "Epoch: [2/2] | Iterations: [92/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [93/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [94/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [95/1870] | Training loss: 0.183\n",
            "Epoch: [2/2] | Iterations: [96/1870] | Training loss: 0.232\n",
            "Epoch: [2/2] | Iterations: [97/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [98/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [99/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [100/1870] | Training loss: 0.261\n",
            "Epoch: [2/2] | Iterations: [101/1870] | Training loss: 0.057\n",
            "Epoch: [2/2] | Iterations: [102/1870] | Training loss: 0.084\n",
            "Epoch: [2/2] | Iterations: [103/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [104/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [105/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [106/1870] | Training loss: 0.073\n",
            "Epoch: [2/2] | Iterations: [107/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [108/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [109/1870] | Training loss: 0.233\n",
            "Epoch: [2/2] | Iterations: [110/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [111/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [112/1870] | Training loss: 0.208\n",
            "Epoch: [2/2] | Iterations: [113/1870] | Training loss: 0.220\n",
            "Epoch: [2/2] | Iterations: [114/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [115/1870] | Training loss: 0.238\n",
            "Epoch: [2/2] | Iterations: [116/1870] | Training loss: 0.071\n",
            "Epoch: [2/2] | Iterations: [117/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [118/1870] | Training loss: 0.223\n",
            "Epoch: [2/2] | Iterations: [119/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [120/1870] | Training loss: 0.249\n",
            "Epoch: [2/2] | Iterations: [121/1870] | Training loss: 0.228\n",
            "Epoch: [2/2] | Iterations: [122/1870] | Training loss: 0.305\n",
            "Epoch: [2/2] | Iterations: [123/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [124/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [125/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [126/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [127/1870] | Training loss: 0.159\n",
            "Epoch: [2/2] | Iterations: [128/1870] | Training loss: 0.272\n",
            "Epoch: [2/2] | Iterations: [129/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [130/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [131/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [132/1870] | Training loss: 0.075\n",
            "Epoch: [2/2] | Iterations: [133/1870] | Training loss: 0.102\n",
            "Epoch: [2/2] | Iterations: [134/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [135/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [136/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [137/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [138/1870] | Training loss: 0.215\n",
            "Epoch: [2/2] | Iterations: [139/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [140/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [141/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [142/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [143/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [144/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [145/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [146/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [147/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [148/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [149/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [150/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [151/1870] | Training loss: 0.067\n",
            "Epoch: [2/2] | Iterations: [152/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [153/1870] | Training loss: 0.157\n",
            "Epoch: [2/2] | Iterations: [154/1870] | Training loss: 0.246\n",
            "Epoch: [2/2] | Iterations: [155/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [156/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [157/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [158/1870] | Training loss: 0.060\n",
            "Epoch: [2/2] | Iterations: [159/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [160/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [161/1870] | Training loss: 0.187\n",
            "Epoch: [2/2] | Iterations: [162/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [163/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [164/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [165/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [166/1870] | Training loss: 0.190\n",
            "Epoch: [2/2] | Iterations: [167/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [168/1870] | Training loss: 0.354\n",
            "Epoch: [2/2] | Iterations: [169/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [170/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [171/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [172/1870] | Training loss: 0.304\n",
            "Epoch: [2/2] | Iterations: [173/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [174/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [175/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [176/1870] | Training loss: 0.185\n",
            "Epoch: [2/2] | Iterations: [177/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [178/1870] | Training loss: 0.184\n",
            "Epoch: [2/2] | Iterations: [179/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [180/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [181/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [182/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [183/1870] | Training loss: 0.055\n",
            "Epoch: [2/2] | Iterations: [184/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [185/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [186/1870] | Training loss: 0.268\n",
            "Epoch: [2/2] | Iterations: [187/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [188/1870] | Training loss: 0.249\n",
            "Epoch: [2/2] | Iterations: [189/1870] | Training loss: 0.234\n",
            "Epoch: [2/2] | Iterations: [190/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [191/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [192/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [193/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [194/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [195/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [196/1870] | Training loss: 0.217\n",
            "Epoch: [2/2] | Iterations: [197/1870] | Training loss: 0.213\n",
            "Epoch: [2/2] | Iterations: [198/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [199/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [200/1870] | Training loss: 0.080\n",
            "Epoch: [2/2] | Iterations: [201/1870] | Training loss: 0.203\n",
            "Epoch: [2/2] | Iterations: [202/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [203/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [204/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [205/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [206/1870] | Training loss: 0.223\n",
            "Epoch: [2/2] | Iterations: [207/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [208/1870] | Training loss: 0.203\n",
            "Epoch: [2/2] | Iterations: [209/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [210/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [211/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [212/1870] | Training loss: 0.258\n",
            "Epoch: [2/2] | Iterations: [213/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [214/1870] | Training loss: 0.205\n",
            "Epoch: [2/2] | Iterations: [215/1870] | Training loss: 0.159\n",
            "Epoch: [2/2] | Iterations: [216/1870] | Training loss: 0.183\n",
            "Epoch: [2/2] | Iterations: [217/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [218/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [219/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [220/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [221/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [222/1870] | Training loss: 0.288\n",
            "Epoch: [2/2] | Iterations: [223/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [224/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [225/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [226/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [227/1870] | Training loss: 0.224\n",
            "Epoch: [2/2] | Iterations: [228/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [229/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [230/1870] | Training loss: 0.088\n",
            "Epoch: [2/2] | Iterations: [231/1870] | Training loss: 0.232\n",
            "Epoch: [2/2] | Iterations: [232/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [233/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [234/1870] | Training loss: 0.093\n",
            "Epoch: [2/2] | Iterations: [235/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [236/1870] | Training loss: 0.109\n",
            "Epoch: [2/2] | Iterations: [237/1870] | Training loss: 0.150\n",
            "Epoch: [2/2] | Iterations: [238/1870] | Training loss: 0.183\n",
            "Epoch: [2/2] | Iterations: [239/1870] | Training loss: 0.187\n",
            "Epoch: [2/2] | Iterations: [240/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [241/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [242/1870] | Training loss: 0.150\n",
            "Epoch: [2/2] | Iterations: [243/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [244/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [245/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [246/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [247/1870] | Training loss: 0.075\n",
            "Epoch: [2/2] | Iterations: [248/1870] | Training loss: 0.216\n",
            "Epoch: [2/2] | Iterations: [249/1870] | Training loss: 0.271\n",
            "Epoch: [2/2] | Iterations: [250/1870] | Training loss: 0.283\n",
            "Epoch: [2/2] | Iterations: [251/1870] | Training loss: 0.239\n",
            "Epoch: [2/2] | Iterations: [252/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [253/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [254/1870] | Training loss: 0.193\n",
            "Epoch: [2/2] | Iterations: [255/1870] | Training loss: 0.084\n",
            "Epoch: [2/2] | Iterations: [256/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [257/1870] | Training loss: 0.223\n",
            "Epoch: [2/2] | Iterations: [258/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [259/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [260/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [261/1870] | Training loss: 0.075\n",
            "Epoch: [2/2] | Iterations: [262/1870] | Training loss: 0.189\n",
            "Epoch: [2/2] | Iterations: [263/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [264/1870] | Training loss: 0.199\n",
            "Epoch: [2/2] | Iterations: [265/1870] | Training loss: 0.234\n",
            "Epoch: [2/2] | Iterations: [266/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [267/1870] | Training loss: 0.190\n",
            "Epoch: [2/2] | Iterations: [268/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [269/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [270/1870] | Training loss: 0.190\n",
            "Epoch: [2/2] | Iterations: [271/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [272/1870] | Training loss: 0.201\n",
            "Epoch: [2/2] | Iterations: [273/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [274/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [275/1870] | Training loss: 0.236\n",
            "Epoch: [2/2] | Iterations: [276/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [277/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [278/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [279/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [280/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [281/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [282/1870] | Training loss: 0.243\n",
            "Epoch: [2/2] | Iterations: [283/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [284/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [285/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [286/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [287/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [288/1870] | Training loss: 0.095\n",
            "Epoch: [2/2] | Iterations: [289/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [290/1870] | Training loss: 0.238\n",
            "Epoch: [2/2] | Iterations: [291/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [292/1870] | Training loss: 0.245\n",
            "Epoch: [2/2] | Iterations: [293/1870] | Training loss: 0.218\n",
            "Epoch: [2/2] | Iterations: [294/1870] | Training loss: 0.217\n",
            "Epoch: [2/2] | Iterations: [295/1870] | Training loss: 0.197\n",
            "Epoch: [2/2] | Iterations: [296/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [297/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [298/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [299/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [300/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [301/1870] | Training loss: 0.070\n",
            "Epoch: [2/2] | Iterations: [302/1870] | Training loss: 0.063\n",
            "Epoch: [2/2] | Iterations: [303/1870] | Training loss: 0.238\n",
            "Epoch: [2/2] | Iterations: [304/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [305/1870] | Training loss: 0.274\n",
            "Epoch: [2/2] | Iterations: [306/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [307/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [308/1870] | Training loss: 0.261\n",
            "Epoch: [2/2] | Iterations: [309/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [310/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [311/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [312/1870] | Training loss: 0.301\n",
            "Epoch: [2/2] | Iterations: [313/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [314/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [315/1870] | Training loss: 0.230\n",
            "Epoch: [2/2] | Iterations: [316/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [317/1870] | Training loss: 0.257\n",
            "Epoch: [2/2] | Iterations: [318/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [319/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [320/1870] | Training loss: 0.205\n",
            "Epoch: [2/2] | Iterations: [321/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [322/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [323/1870] | Training loss: 0.077\n",
            "Epoch: [2/2] | Iterations: [324/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [325/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [326/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [327/1870] | Training loss: 0.071\n",
            "Epoch: [2/2] | Iterations: [328/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [329/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [330/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [331/1870] | Training loss: 0.253\n",
            "Epoch: [2/2] | Iterations: [332/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [333/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [334/1870] | Training loss: 0.086\n",
            "Epoch: [2/2] | Iterations: [335/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [336/1870] | Training loss: 0.171\n",
            "Epoch: [2/2] | Iterations: [337/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [338/1870] | Training loss: 0.055\n",
            "Epoch: [2/2] | Iterations: [339/1870] | Training loss: 0.238\n",
            "Epoch: [2/2] | Iterations: [340/1870] | Training loss: 0.244\n",
            "Epoch: [2/2] | Iterations: [341/1870] | Training loss: 0.203\n",
            "Epoch: [2/2] | Iterations: [342/1870] | Training loss: 0.205\n",
            "Epoch: [2/2] | Iterations: [343/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [344/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [345/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [346/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [347/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [348/1870] | Training loss: 0.085\n",
            "Epoch: [2/2] | Iterations: [349/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [350/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [351/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [352/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [353/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [354/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [355/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [356/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [357/1870] | Training loss: 0.256\n",
            "Epoch: [2/2] | Iterations: [358/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [359/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [360/1870] | Training loss: 0.216\n",
            "Epoch: [2/2] | Iterations: [361/1870] | Training loss: 0.236\n",
            "Epoch: [2/2] | Iterations: [362/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [363/1870] | Training loss: 0.332\n",
            "Epoch: [2/2] | Iterations: [364/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [365/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [366/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [367/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [368/1870] | Training loss: 0.189\n",
            "Epoch: [2/2] | Iterations: [369/1870] | Training loss: 0.251\n",
            "Epoch: [2/2] | Iterations: [370/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [371/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [372/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [373/1870] | Training loss: 0.210\n",
            "Epoch: [2/2] | Iterations: [374/1870] | Training loss: 0.184\n",
            "Epoch: [2/2] | Iterations: [375/1870] | Training loss: 0.232\n",
            "Epoch: [2/2] | Iterations: [376/1870] | Training loss: 0.231\n",
            "Epoch: [2/2] | Iterations: [377/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [378/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [379/1870] | Training loss: 0.080\n",
            "Epoch: [2/2] | Iterations: [380/1870] | Training loss: 0.204\n",
            "Epoch: [2/2] | Iterations: [381/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [382/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [383/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [384/1870] | Training loss: 0.078\n",
            "Epoch: [2/2] | Iterations: [385/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [386/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [387/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [388/1870] | Training loss: 0.080\n",
            "Epoch: [2/2] | Iterations: [389/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [390/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [391/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [392/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [393/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [394/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [395/1870] | Training loss: 0.157\n",
            "Epoch: [2/2] | Iterations: [396/1870] | Training loss: 0.271\n",
            "Epoch: [2/2] | Iterations: [397/1870] | Training loss: 0.195\n",
            "Epoch: [2/2] | Iterations: [398/1870] | Training loss: 0.084\n",
            "Epoch: [2/2] | Iterations: [399/1870] | Training loss: 0.087\n",
            "Epoch: [2/2] | Iterations: [400/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [401/1870] | Training loss: 0.216\n",
            "Epoch: [2/2] | Iterations: [402/1870] | Training loss: 0.189\n",
            "Epoch: [2/2] | Iterations: [403/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [404/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [405/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [406/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [407/1870] | Training loss: 0.095\n",
            "Epoch: [2/2] | Iterations: [408/1870] | Training loss: 0.263\n",
            "Epoch: [2/2] | Iterations: [409/1870] | Training loss: 0.095\n",
            "Epoch: [2/2] | Iterations: [410/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [411/1870] | Training loss: 0.105\n",
            "Epoch: [2/2] | Iterations: [412/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [413/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [414/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [415/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [416/1870] | Training loss: 0.150\n",
            "Epoch: [2/2] | Iterations: [417/1870] | Training loss: 0.231\n",
            "Epoch: [2/2] | Iterations: [418/1870] | Training loss: 0.097\n",
            "Epoch: [2/2] | Iterations: [419/1870] | Training loss: 0.209\n",
            "Epoch: [2/2] | Iterations: [420/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [421/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [422/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [423/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [424/1870] | Training loss: 0.251\n",
            "Epoch: [2/2] | Iterations: [425/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [426/1870] | Training loss: 0.056\n",
            "Epoch: [2/2] | Iterations: [427/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [428/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [429/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [430/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [431/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [432/1870] | Training loss: 0.109\n",
            "Epoch: [2/2] | Iterations: [433/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [434/1870] | Training loss: 0.300\n",
            "Epoch: [2/2] | Iterations: [435/1870] | Training loss: 0.218\n",
            "Epoch: [2/2] | Iterations: [436/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [437/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [438/1870] | Training loss: 0.087\n",
            "Epoch: [2/2] | Iterations: [439/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [440/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [441/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [442/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [443/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [444/1870] | Training loss: 0.261\n",
            "Epoch: [2/2] | Iterations: [445/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [446/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [447/1870] | Training loss: 0.082\n",
            "Epoch: [2/2] | Iterations: [448/1870] | Training loss: 0.243\n",
            "Epoch: [2/2] | Iterations: [449/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [450/1870] | Training loss: 0.239\n",
            "Epoch: [2/2] | Iterations: [451/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [452/1870] | Training loss: 0.240\n",
            "Epoch: [2/2] | Iterations: [453/1870] | Training loss: 0.301\n",
            "Epoch: [2/2] | Iterations: [454/1870] | Training loss: 0.233\n",
            "Epoch: [2/2] | Iterations: [455/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [456/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [457/1870] | Training loss: 0.195\n",
            "Epoch: [2/2] | Iterations: [458/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [459/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [460/1870] | Training loss: 0.256\n",
            "Epoch: [2/2] | Iterations: [461/1870] | Training loss: 0.072\n",
            "Epoch: [2/2] | Iterations: [462/1870] | Training loss: 0.194\n",
            "Epoch: [2/2] | Iterations: [463/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [464/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [465/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [466/1870] | Training loss: 0.228\n",
            "Epoch: [2/2] | Iterations: [467/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [468/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [469/1870] | Training loss: 0.237\n",
            "Epoch: [2/2] | Iterations: [470/1870] | Training loss: 0.083\n",
            "Epoch: [2/2] | Iterations: [471/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [472/1870] | Training loss: 0.183\n",
            "Epoch: [2/2] | Iterations: [473/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [474/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [475/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [476/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [477/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [478/1870] | Training loss: 0.150\n",
            "Epoch: [2/2] | Iterations: [479/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [480/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [481/1870] | Training loss: 0.150\n",
            "Epoch: [2/2] | Iterations: [482/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [483/1870] | Training loss: 0.193\n",
            "Epoch: [2/2] | Iterations: [484/1870] | Training loss: 0.102\n",
            "Epoch: [2/2] | Iterations: [485/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [486/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [487/1870] | Training loss: 0.197\n",
            "Epoch: [2/2] | Iterations: [488/1870] | Training loss: 0.249\n",
            "Epoch: [2/2] | Iterations: [489/1870] | Training loss: 0.194\n",
            "Epoch: [2/2] | Iterations: [490/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [491/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [492/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [493/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [494/1870] | Training loss: 0.060\n",
            "Epoch: [2/2] | Iterations: [495/1870] | Training loss: 0.243\n",
            "Epoch: [2/2] | Iterations: [496/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [497/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [498/1870] | Training loss: 0.085\n",
            "Epoch: [2/2] | Iterations: [499/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [500/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [501/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [502/1870] | Training loss: 0.187\n",
            "Epoch: [2/2] | Iterations: [503/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [504/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [505/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [506/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [507/1870] | Training loss: 0.061\n",
            "Epoch: [2/2] | Iterations: [508/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [509/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [510/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [511/1870] | Training loss: 0.234\n",
            "Epoch: [2/2] | Iterations: [512/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [513/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [514/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [515/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [516/1870] | Training loss: 0.289\n",
            "Epoch: [2/2] | Iterations: [517/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [518/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [519/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [520/1870] | Training loss: 0.184\n",
            "Epoch: [2/2] | Iterations: [521/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [522/1870] | Training loss: 0.207\n",
            "Epoch: [2/2] | Iterations: [523/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [524/1870] | Training loss: 0.265\n",
            "Epoch: [2/2] | Iterations: [525/1870] | Training loss: 0.082\n",
            "Epoch: [2/2] | Iterations: [526/1870] | Training loss: 0.068\n",
            "Epoch: [2/2] | Iterations: [527/1870] | Training loss: 0.196\n",
            "Epoch: [2/2] | Iterations: [528/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [529/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [530/1870] | Training loss: 0.106\n",
            "Epoch: [2/2] | Iterations: [531/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [532/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [533/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [534/1870] | Training loss: 0.260\n",
            "Epoch: [2/2] | Iterations: [535/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [536/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [537/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [538/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [539/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [540/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [541/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [542/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [543/1870] | Training loss: 0.205\n",
            "Epoch: [2/2] | Iterations: [544/1870] | Training loss: 0.093\n",
            "Epoch: [2/2] | Iterations: [545/1870] | Training loss: 0.291\n",
            "Epoch: [2/2] | Iterations: [546/1870] | Training loss: 0.259\n",
            "Epoch: [2/2] | Iterations: [547/1870] | Training loss: 0.192\n",
            "Epoch: [2/2] | Iterations: [548/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [549/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [550/1870] | Training loss: 0.192\n",
            "Epoch: [2/2] | Iterations: [551/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [552/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [553/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [554/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [555/1870] | Training loss: 0.157\n",
            "Epoch: [2/2] | Iterations: [556/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [557/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [558/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [559/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [560/1870] | Training loss: 0.171\n",
            "Epoch: [2/2] | Iterations: [561/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [562/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [563/1870] | Training loss: 0.253\n",
            "Epoch: [2/2] | Iterations: [564/1870] | Training loss: 0.184\n",
            "Epoch: [2/2] | Iterations: [565/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [566/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [567/1870] | Training loss: 0.105\n",
            "Epoch: [2/2] | Iterations: [568/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [569/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [570/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [571/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [572/1870] | Training loss: 0.088\n",
            "Epoch: [2/2] | Iterations: [573/1870] | Training loss: 0.249\n",
            "Epoch: [2/2] | Iterations: [574/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [575/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [576/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [577/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [578/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [579/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [580/1870] | Training loss: 0.199\n",
            "Epoch: [2/2] | Iterations: [581/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [582/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [583/1870] | Training loss: 0.106\n",
            "Epoch: [2/2] | Iterations: [584/1870] | Training loss: 0.196\n",
            "Epoch: [2/2] | Iterations: [585/1870] | Training loss: 0.082\n",
            "Epoch: [2/2] | Iterations: [586/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [587/1870] | Training loss: 0.221\n",
            "Epoch: [2/2] | Iterations: [588/1870] | Training loss: 0.097\n",
            "Epoch: [2/2] | Iterations: [589/1870] | Training loss: 0.216\n",
            "Epoch: [2/2] | Iterations: [590/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [591/1870] | Training loss: 0.241\n",
            "Epoch: [2/2] | Iterations: [592/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [593/1870] | Training loss: 0.184\n",
            "Epoch: [2/2] | Iterations: [594/1870] | Training loss: 0.195\n",
            "Epoch: [2/2] | Iterations: [595/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [596/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [597/1870] | Training loss: 0.270\n",
            "Epoch: [2/2] | Iterations: [598/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [599/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [600/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [601/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [602/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [603/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [604/1870] | Training loss: 0.217\n",
            "Epoch: [2/2] | Iterations: [605/1870] | Training loss: 0.213\n",
            "Epoch: [2/2] | Iterations: [606/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [607/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [608/1870] | Training loss: 0.276\n",
            "Epoch: [2/2] | Iterations: [609/1870] | Training loss: 0.070\n",
            "Epoch: [2/2] | Iterations: [610/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [611/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [612/1870] | Training loss: 0.201\n",
            "Epoch: [2/2] | Iterations: [613/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [614/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [615/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [616/1870] | Training loss: 0.216\n",
            "Epoch: [2/2] | Iterations: [617/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [618/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [619/1870] | Training loss: 0.073\n",
            "Epoch: [2/2] | Iterations: [620/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [621/1870] | Training loss: 0.056\n",
            "Epoch: [2/2] | Iterations: [622/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [623/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [624/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [625/1870] | Training loss: 0.080\n",
            "Epoch: [2/2] | Iterations: [626/1870] | Training loss: 0.199\n",
            "Epoch: [2/2] | Iterations: [627/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [628/1870] | Training loss: 0.263\n",
            "Epoch: [2/2] | Iterations: [629/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [630/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [631/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [632/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [633/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [634/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [635/1870] | Training loss: 0.203\n",
            "Epoch: [2/2] | Iterations: [636/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [637/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [638/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [639/1870] | Training loss: 0.215\n",
            "Epoch: [2/2] | Iterations: [640/1870] | Training loss: 0.199\n",
            "Epoch: [2/2] | Iterations: [641/1870] | Training loss: 0.089\n",
            "Epoch: [2/2] | Iterations: [642/1870] | Training loss: 0.073\n",
            "Epoch: [2/2] | Iterations: [643/1870] | Training loss: 0.070\n",
            "Epoch: [2/2] | Iterations: [644/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [645/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [646/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [647/1870] | Training loss: 0.214\n",
            "Epoch: [2/2] | Iterations: [648/1870] | Training loss: 0.208\n",
            "Epoch: [2/2] | Iterations: [649/1870] | Training loss: 0.252\n",
            "Epoch: [2/2] | Iterations: [650/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [651/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [652/1870] | Training loss: 0.203\n",
            "Epoch: [2/2] | Iterations: [653/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [654/1870] | Training loss: 0.087\n",
            "Epoch: [2/2] | Iterations: [655/1870] | Training loss: 0.294\n",
            "Epoch: [2/2] | Iterations: [656/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [657/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [658/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [659/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [660/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [661/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [662/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [663/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [664/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [665/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [666/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [667/1870] | Training loss: 0.251\n",
            "Epoch: [2/2] | Iterations: [668/1870] | Training loss: 0.192\n",
            "Epoch: [2/2] | Iterations: [669/1870] | Training loss: 0.232\n",
            "Epoch: [2/2] | Iterations: [670/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [671/1870] | Training loss: 0.085\n",
            "Epoch: [2/2] | Iterations: [672/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [673/1870] | Training loss: 0.077\n",
            "Epoch: [2/2] | Iterations: [674/1870] | Training loss: 0.248\n",
            "Epoch: [2/2] | Iterations: [675/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [676/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [677/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [678/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [679/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [680/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [681/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [682/1870] | Training loss: 0.192\n",
            "Epoch: [2/2] | Iterations: [683/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [684/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [685/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [686/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [687/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [688/1870] | Training loss: 0.051\n",
            "Epoch: [2/2] | Iterations: [689/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [690/1870] | Training loss: 0.232\n",
            "Epoch: [2/2] | Iterations: [691/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [692/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [693/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [694/1870] | Training loss: 0.214\n",
            "Epoch: [2/2] | Iterations: [695/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [696/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [697/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [698/1870] | Training loss: 0.209\n",
            "Epoch: [2/2] | Iterations: [699/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [700/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [701/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [702/1870] | Training loss: 0.226\n",
            "Epoch: [2/2] | Iterations: [703/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [704/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [705/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [706/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [707/1870] | Training loss: 0.072\n",
            "Epoch: [2/2] | Iterations: [708/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [709/1870] | Training loss: 0.083\n",
            "Epoch: [2/2] | Iterations: [710/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [711/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [712/1870] | Training loss: 0.193\n",
            "Epoch: [2/2] | Iterations: [713/1870] | Training loss: 0.217\n",
            "Epoch: [2/2] | Iterations: [714/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [715/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [716/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [717/1870] | Training loss: 0.179\n",
            "Epoch: [2/2] | Iterations: [718/1870] | Training loss: 0.215\n",
            "Epoch: [2/2] | Iterations: [719/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [720/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [721/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [722/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [723/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [724/1870] | Training loss: 0.277\n",
            "Epoch: [2/2] | Iterations: [725/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [726/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [727/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [728/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [729/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [730/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [731/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [732/1870] | Training loss: 0.252\n",
            "Epoch: [2/2] | Iterations: [733/1870] | Training loss: 0.081\n",
            "Epoch: [2/2] | Iterations: [734/1870] | Training loss: 0.087\n",
            "Epoch: [2/2] | Iterations: [735/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [736/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [737/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [738/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [739/1870] | Training loss: 0.068\n",
            "Epoch: [2/2] | Iterations: [740/1870] | Training loss: 0.067\n",
            "Epoch: [2/2] | Iterations: [741/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [742/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [743/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [744/1870] | Training loss: 0.281\n",
            "Epoch: [2/2] | Iterations: [745/1870] | Training loss: 0.093\n",
            "Epoch: [2/2] | Iterations: [746/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [747/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [748/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [749/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [750/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [751/1870] | Training loss: 0.055\n",
            "Epoch: [2/2] | Iterations: [752/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [753/1870] | Training loss: 0.220\n",
            "Epoch: [2/2] | Iterations: [754/1870] | Training loss: 0.051\n",
            "Epoch: [2/2] | Iterations: [755/1870] | Training loss: 0.063\n",
            "Epoch: [2/2] | Iterations: [756/1870] | Training loss: 0.157\n",
            "Epoch: [2/2] | Iterations: [757/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [758/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [759/1870] | Training loss: 0.081\n",
            "Epoch: [2/2] | Iterations: [760/1870] | Training loss: 0.284\n",
            "Epoch: [2/2] | Iterations: [761/1870] | Training loss: 0.268\n",
            "Epoch: [2/2] | Iterations: [762/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [763/1870] | Training loss: 0.183\n",
            "Epoch: [2/2] | Iterations: [764/1870] | Training loss: 0.106\n",
            "Epoch: [2/2] | Iterations: [765/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [766/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [767/1870] | Training loss: 0.314\n",
            "Epoch: [2/2] | Iterations: [768/1870] | Training loss: 0.226\n",
            "Epoch: [2/2] | Iterations: [769/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [770/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [771/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [772/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [773/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [774/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [775/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [776/1870] | Training loss: 0.262\n",
            "Epoch: [2/2] | Iterations: [777/1870] | Training loss: 0.179\n",
            "Epoch: [2/2] | Iterations: [778/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [779/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [780/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [781/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [782/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [783/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [784/1870] | Training loss: 0.073\n",
            "Epoch: [2/2] | Iterations: [785/1870] | Training loss: 0.208\n",
            "Epoch: [2/2] | Iterations: [786/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [787/1870] | Training loss: 0.234\n",
            "Epoch: [2/2] | Iterations: [788/1870] | Training loss: 0.089\n",
            "Epoch: [2/2] | Iterations: [789/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [790/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [791/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [792/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [793/1870] | Training loss: 0.192\n",
            "Epoch: [2/2] | Iterations: [794/1870] | Training loss: 0.105\n",
            "Epoch: [2/2] | Iterations: [795/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [796/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [797/1870] | Training loss: 0.227\n",
            "Epoch: [2/2] | Iterations: [798/1870] | Training loss: 0.263\n",
            "Epoch: [2/2] | Iterations: [799/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [800/1870] | Training loss: 0.187\n",
            "Epoch: [2/2] | Iterations: [801/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [802/1870] | Training loss: 0.244\n",
            "Epoch: [2/2] | Iterations: [803/1870] | Training loss: 0.171\n",
            "Epoch: [2/2] | Iterations: [804/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [805/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [806/1870] | Training loss: 0.179\n",
            "Epoch: [2/2] | Iterations: [807/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [808/1870] | Training loss: 0.082\n",
            "Epoch: [2/2] | Iterations: [809/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [810/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [811/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [812/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [813/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [814/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [815/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [816/1870] | Training loss: 0.213\n",
            "Epoch: [2/2] | Iterations: [817/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [818/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [819/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [820/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [821/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [822/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [823/1870] | Training loss: 0.083\n",
            "Epoch: [2/2] | Iterations: [824/1870] | Training loss: 0.219\n",
            "Epoch: [2/2] | Iterations: [825/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [826/1870] | Training loss: 0.245\n",
            "Epoch: [2/2] | Iterations: [827/1870] | Training loss: 0.239\n",
            "Epoch: [2/2] | Iterations: [828/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [829/1870] | Training loss: 0.194\n",
            "Epoch: [2/2] | Iterations: [830/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [831/1870] | Training loss: 0.244\n",
            "Epoch: [2/2] | Iterations: [832/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [833/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [834/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [835/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [836/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [837/1870] | Training loss: 0.272\n",
            "Epoch: [2/2] | Iterations: [838/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [839/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [840/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [841/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [842/1870] | Training loss: 0.253\n",
            "Epoch: [2/2] | Iterations: [843/1870] | Training loss: 0.078\n",
            "Epoch: [2/2] | Iterations: [844/1870] | Training loss: 0.225\n",
            "Epoch: [2/2] | Iterations: [845/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [846/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [847/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [848/1870] | Training loss: 0.183\n",
            "Epoch: [2/2] | Iterations: [849/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [850/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [851/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [852/1870] | Training loss: 0.224\n",
            "Epoch: [2/2] | Iterations: [853/1870] | Training loss: 0.205\n",
            "Epoch: [2/2] | Iterations: [854/1870] | Training loss: 0.235\n",
            "Epoch: [2/2] | Iterations: [855/1870] | Training loss: 0.105\n",
            "Epoch: [2/2] | Iterations: [856/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [857/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [858/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [859/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [860/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [861/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [862/1870] | Training loss: 0.195\n",
            "Epoch: [2/2] | Iterations: [863/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [864/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [865/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [866/1870] | Training loss: 0.330\n",
            "Epoch: [2/2] | Iterations: [867/1870] | Training loss: 0.059\n",
            "Epoch: [2/2] | Iterations: [868/1870] | Training loss: 0.240\n",
            "Epoch: [2/2] | Iterations: [869/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [870/1870] | Training loss: 0.210\n",
            "Epoch: [2/2] | Iterations: [871/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [872/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [873/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [874/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [875/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [876/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [877/1870] | Training loss: 0.211\n",
            "Epoch: [2/2] | Iterations: [878/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [879/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [880/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [881/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [882/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [883/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [884/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [885/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [886/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [887/1870] | Training loss: 0.203\n",
            "Epoch: [2/2] | Iterations: [888/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [889/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [890/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [891/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [892/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [893/1870] | Training loss: 0.105\n",
            "Epoch: [2/2] | Iterations: [894/1870] | Training loss: 0.242\n",
            "Epoch: [2/2] | Iterations: [895/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [896/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [897/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [898/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [899/1870] | Training loss: 0.185\n",
            "Epoch: [2/2] | Iterations: [900/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [901/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [902/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [903/1870] | Training loss: 0.189\n",
            "Epoch: [2/2] | Iterations: [904/1870] | Training loss: 0.159\n",
            "Epoch: [2/2] | Iterations: [905/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [906/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [907/1870] | Training loss: 0.197\n",
            "Epoch: [2/2] | Iterations: [908/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [909/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [910/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [911/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [912/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [913/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [914/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [915/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [916/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [917/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [918/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [919/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [920/1870] | Training loss: 0.179\n",
            "Epoch: [2/2] | Iterations: [921/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [922/1870] | Training loss: 0.205\n",
            "Epoch: [2/2] | Iterations: [923/1870] | Training loss: 0.075\n",
            "Epoch: [2/2] | Iterations: [924/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [925/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [926/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [927/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [928/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [929/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [930/1870] | Training loss: 0.234\n",
            "Epoch: [2/2] | Iterations: [931/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [932/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [933/1870] | Training loss: 0.171\n",
            "Epoch: [2/2] | Iterations: [934/1870] | Training loss: 0.106\n",
            "Epoch: [2/2] | Iterations: [935/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [936/1870] | Training loss: 0.083\n",
            "Epoch: [2/2] | Iterations: [937/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [938/1870] | Training loss: 0.070\n",
            "Epoch: [2/2] | Iterations: [939/1870] | Training loss: 0.210\n",
            "Epoch: [2/2] | Iterations: [940/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [941/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [942/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [943/1870] | Training loss: 0.070\n",
            "Epoch: [2/2] | Iterations: [944/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [945/1870] | Training loss: 0.207\n",
            "Epoch: [2/2] | Iterations: [946/1870] | Training loss: 0.079\n",
            "Epoch: [2/2] | Iterations: [947/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [948/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [949/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [950/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [951/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [952/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [953/1870] | Training loss: 0.380\n",
            "Epoch: [2/2] | Iterations: [954/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [955/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [956/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [957/1870] | Training loss: 0.078\n",
            "Epoch: [2/2] | Iterations: [958/1870] | Training loss: 0.109\n",
            "Epoch: [2/2] | Iterations: [959/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [960/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [961/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [962/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [963/1870] | Training loss: 0.083\n",
            "Epoch: [2/2] | Iterations: [964/1870] | Training loss: 0.223\n",
            "Epoch: [2/2] | Iterations: [965/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [966/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [967/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [968/1870] | Training loss: 0.193\n",
            "Epoch: [2/2] | Iterations: [969/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [970/1870] | Training loss: 0.288\n",
            "Epoch: [2/2] | Iterations: [971/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [972/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [973/1870] | Training loss: 0.203\n",
            "Epoch: [2/2] | Iterations: [974/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [975/1870] | Training loss: 0.205\n",
            "Epoch: [2/2] | Iterations: [976/1870] | Training loss: 0.109\n",
            "Epoch: [2/2] | Iterations: [977/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [978/1870] | Training loss: 0.087\n",
            "Epoch: [2/2] | Iterations: [979/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [980/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [981/1870] | Training loss: 0.193\n",
            "Epoch: [2/2] | Iterations: [982/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [983/1870] | Training loss: 0.223\n",
            "Epoch: [2/2] | Iterations: [984/1870] | Training loss: 0.249\n",
            "Epoch: [2/2] | Iterations: [985/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [986/1870] | Training loss: 0.245\n",
            "Epoch: [2/2] | Iterations: [987/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [988/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [989/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [990/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [991/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [992/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [993/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [994/1870] | Training loss: 0.210\n",
            "Epoch: [2/2] | Iterations: [995/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [996/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [997/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [998/1870] | Training loss: 0.225\n",
            "Epoch: [2/2] | Iterations: [999/1870] | Training loss: 0.197\n",
            "Epoch: [2/2] | Iterations: [1000/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [1001/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [1002/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [1003/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [1004/1870] | Training loss: 0.067\n",
            "Epoch: [2/2] | Iterations: [1005/1870] | Training loss: 0.217\n",
            "Epoch: [2/2] | Iterations: [1006/1870] | Training loss: 0.185\n",
            "Epoch: [2/2] | Iterations: [1007/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [1008/1870] | Training loss: 0.253\n",
            "Epoch: [2/2] | Iterations: [1009/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1010/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1011/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [1012/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [1013/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1014/1870] | Training loss: 0.109\n",
            "Epoch: [2/2] | Iterations: [1015/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1016/1870] | Training loss: 0.223\n",
            "Epoch: [2/2] | Iterations: [1017/1870] | Training loss: 0.083\n",
            "Epoch: [2/2] | Iterations: [1018/1870] | Training loss: 0.084\n",
            "Epoch: [2/2] | Iterations: [1019/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [1020/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [1021/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [1022/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [1023/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1024/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [1025/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1026/1870] | Training loss: 0.209\n",
            "Epoch: [2/2] | Iterations: [1027/1870] | Training loss: 0.140\n",
            "Epoch: [2/2] | Iterations: [1028/1870] | Training loss: 0.194\n",
            "Epoch: [2/2] | Iterations: [1029/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1030/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [1031/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [1032/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [1033/1870] | Training loss: 0.228\n",
            "Epoch: [2/2] | Iterations: [1034/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [1035/1870] | Training loss: 0.086\n",
            "Epoch: [2/2] | Iterations: [1036/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [1037/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [1038/1870] | Training loss: 0.081\n",
            "Epoch: [2/2] | Iterations: [1039/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1040/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [1041/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1042/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1043/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [1044/1870] | Training loss: 0.093\n",
            "Epoch: [2/2] | Iterations: [1045/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [1046/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [1047/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [1048/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [1049/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [1050/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [1051/1870] | Training loss: 0.082\n",
            "Epoch: [2/2] | Iterations: [1052/1870] | Training loss: 0.171\n",
            "Epoch: [2/2] | Iterations: [1053/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1054/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [1055/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1056/1870] | Training loss: 0.082\n",
            "Epoch: [2/2] | Iterations: [1057/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [1058/1870] | Training loss: 0.238\n",
            "Epoch: [2/2] | Iterations: [1059/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [1060/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [1061/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1062/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [1063/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [1064/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [1065/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [1066/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1067/1870] | Training loss: 0.074\n",
            "Epoch: [2/2] | Iterations: [1068/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [1069/1870] | Training loss: 0.201\n",
            "Epoch: [2/2] | Iterations: [1070/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1071/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [1072/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [1073/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [1074/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [1075/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [1076/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [1077/1870] | Training loss: 0.341\n",
            "Epoch: [2/2] | Iterations: [1078/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [1079/1870] | Training loss: 0.248\n",
            "Epoch: [2/2] | Iterations: [1080/1870] | Training loss: 0.272\n",
            "Epoch: [2/2] | Iterations: [1081/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [1082/1870] | Training loss: 0.303\n",
            "Epoch: [2/2] | Iterations: [1083/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [1084/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1085/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [1086/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [1087/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [1088/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [1089/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [1090/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1091/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1092/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [1093/1870] | Training loss: 0.227\n",
            "Epoch: [2/2] | Iterations: [1094/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [1095/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [1096/1870] | Training loss: 0.219\n",
            "Epoch: [2/2] | Iterations: [1097/1870] | Training loss: 0.065\n",
            "Epoch: [2/2] | Iterations: [1098/1870] | Training loss: 0.074\n",
            "Epoch: [2/2] | Iterations: [1099/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [1100/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1101/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [1102/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [1103/1870] | Training loss: 0.192\n",
            "Epoch: [2/2] | Iterations: [1104/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [1105/1870] | Training loss: 0.073\n",
            "Epoch: [2/2] | Iterations: [1106/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [1107/1870] | Training loss: 0.185\n",
            "Epoch: [2/2] | Iterations: [1108/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [1109/1870] | Training loss: 0.253\n",
            "Epoch: [2/2] | Iterations: [1110/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1111/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [1112/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [1113/1870] | Training loss: 0.071\n",
            "Epoch: [2/2] | Iterations: [1114/1870] | Training loss: 0.217\n",
            "Epoch: [2/2] | Iterations: [1115/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [1116/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [1117/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1118/1870] | Training loss: 0.257\n",
            "Epoch: [2/2] | Iterations: [1119/1870] | Training loss: 0.089\n",
            "Epoch: [2/2] | Iterations: [1120/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [1121/1870] | Training loss: 0.228\n",
            "Epoch: [2/2] | Iterations: [1122/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1123/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1124/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [1125/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [1126/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [1127/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [1128/1870] | Training loss: 0.214\n",
            "Epoch: [2/2] | Iterations: [1129/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [1130/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [1131/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1132/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [1133/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [1134/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1135/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [1136/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [1137/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [1138/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [1139/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [1140/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [1141/1870] | Training loss: 0.065\n",
            "Epoch: [2/2] | Iterations: [1142/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [1143/1870] | Training loss: 0.068\n",
            "Epoch: [2/2] | Iterations: [1144/1870] | Training loss: 0.105\n",
            "Epoch: [2/2] | Iterations: [1145/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1146/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1147/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [1148/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [1149/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1150/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [1151/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [1152/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [1153/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [1154/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [1155/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [1156/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [1157/1870] | Training loss: 0.078\n",
            "Epoch: [2/2] | Iterations: [1158/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [1159/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1160/1870] | Training loss: 0.240\n",
            "Epoch: [2/2] | Iterations: [1161/1870] | Training loss: 0.262\n",
            "Epoch: [2/2] | Iterations: [1162/1870] | Training loss: 0.193\n",
            "Epoch: [2/2] | Iterations: [1163/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1164/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [1165/1870] | Training loss: 0.199\n",
            "Epoch: [2/2] | Iterations: [1166/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1167/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [1168/1870] | Training loss: 0.238\n",
            "Epoch: [2/2] | Iterations: [1169/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [1170/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [1171/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [1172/1870] | Training loss: 0.184\n",
            "Epoch: [2/2] | Iterations: [1173/1870] | Training loss: 0.097\n",
            "Epoch: [2/2] | Iterations: [1174/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [1175/1870] | Training loss: 0.055\n",
            "Epoch: [2/2] | Iterations: [1176/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [1177/1870] | Training loss: 0.109\n",
            "Epoch: [2/2] | Iterations: [1178/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [1179/1870] | Training loss: 0.183\n",
            "Epoch: [2/2] | Iterations: [1180/1870] | Training loss: 0.203\n",
            "Epoch: [2/2] | Iterations: [1181/1870] | Training loss: 0.183\n",
            "Epoch: [2/2] | Iterations: [1182/1870] | Training loss: 0.102\n",
            "Epoch: [2/2] | Iterations: [1183/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [1184/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [1185/1870] | Training loss: 0.209\n",
            "Epoch: [2/2] | Iterations: [1186/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [1187/1870] | Training loss: 0.179\n",
            "Epoch: [2/2] | Iterations: [1188/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [1189/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [1190/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [1191/1870] | Training loss: 0.197\n",
            "Epoch: [2/2] | Iterations: [1192/1870] | Training loss: 0.201\n",
            "Epoch: [2/2] | Iterations: [1193/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [1194/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [1195/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [1196/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [1197/1870] | Training loss: 0.237\n",
            "Epoch: [2/2] | Iterations: [1198/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [1199/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [1200/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [1201/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [1202/1870] | Training loss: 0.064\n",
            "Epoch: [2/2] | Iterations: [1203/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [1204/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [1205/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [1206/1870] | Training loss: 0.227\n",
            "Epoch: [2/2] | Iterations: [1207/1870] | Training loss: 0.106\n",
            "Epoch: [2/2] | Iterations: [1208/1870] | Training loss: 0.210\n",
            "Epoch: [2/2] | Iterations: [1209/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [1210/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [1211/1870] | Training loss: 0.086\n",
            "Epoch: [2/2] | Iterations: [1212/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1213/1870] | Training loss: 0.107\n",
            "Epoch: [2/2] | Iterations: [1214/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [1215/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [1216/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1217/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [1218/1870] | Training loss: 0.105\n",
            "Epoch: [2/2] | Iterations: [1219/1870] | Training loss: 0.064\n",
            "Epoch: [2/2] | Iterations: [1220/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1221/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [1222/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [1223/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [1224/1870] | Training loss: 0.225\n",
            "Epoch: [2/2] | Iterations: [1225/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [1226/1870] | Training loss: 0.056\n",
            "Epoch: [2/2] | Iterations: [1227/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1228/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [1229/1870] | Training loss: 0.088\n",
            "Epoch: [2/2] | Iterations: [1230/1870] | Training loss: 0.109\n",
            "Epoch: [2/2] | Iterations: [1231/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1232/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [1233/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [1234/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [1235/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [1236/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [1237/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [1238/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1239/1870] | Training loss: 0.171\n",
            "Epoch: [2/2] | Iterations: [1240/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [1241/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1242/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [1243/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1244/1870] | Training loss: 0.266\n",
            "Epoch: [2/2] | Iterations: [1245/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1246/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [1247/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1248/1870] | Training loss: 0.271\n",
            "Epoch: [2/2] | Iterations: [1249/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [1250/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [1251/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [1252/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [1253/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [1254/1870] | Training loss: 0.245\n",
            "Epoch: [2/2] | Iterations: [1255/1870] | Training loss: 0.199\n",
            "Epoch: [2/2] | Iterations: [1256/1870] | Training loss: 0.239\n",
            "Epoch: [2/2] | Iterations: [1257/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [1258/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1259/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [1260/1870] | Training loss: 0.078\n",
            "Epoch: [2/2] | Iterations: [1261/1870] | Training loss: 0.080\n",
            "Epoch: [2/2] | Iterations: [1262/1870] | Training loss: 0.150\n",
            "Epoch: [2/2] | Iterations: [1263/1870] | Training loss: 0.265\n",
            "Epoch: [2/2] | Iterations: [1264/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [1265/1870] | Training loss: 0.086\n",
            "Epoch: [2/2] | Iterations: [1266/1870] | Training loss: 0.270\n",
            "Epoch: [2/2] | Iterations: [1267/1870] | Training loss: 0.084\n",
            "Epoch: [2/2] | Iterations: [1268/1870] | Training loss: 0.105\n",
            "Epoch: [2/2] | Iterations: [1269/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [1270/1870] | Training loss: 0.150\n",
            "Epoch: [2/2] | Iterations: [1271/1870] | Training loss: 0.201\n",
            "Epoch: [2/2] | Iterations: [1272/1870] | Training loss: 0.065\n",
            "Epoch: [2/2] | Iterations: [1273/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [1274/1870] | Training loss: 0.218\n",
            "Epoch: [2/2] | Iterations: [1275/1870] | Training loss: 0.208\n",
            "Epoch: [2/2] | Iterations: [1276/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [1277/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [1278/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [1279/1870] | Training loss: 0.098\n",
            "Epoch: [2/2] | Iterations: [1280/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [1281/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [1282/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [1283/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [1284/1870] | Training loss: 0.159\n",
            "Epoch: [2/2] | Iterations: [1285/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1286/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [1287/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [1288/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [1289/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [1290/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [1291/1870] | Training loss: 0.078\n",
            "Epoch: [2/2] | Iterations: [1292/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [1293/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1294/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [1295/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1296/1870] | Training loss: 0.257\n",
            "Epoch: [2/2] | Iterations: [1297/1870] | Training loss: 0.197\n",
            "Epoch: [2/2] | Iterations: [1298/1870] | Training loss: 0.197\n",
            "Epoch: [2/2] | Iterations: [1299/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1300/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [1301/1870] | Training loss: 0.070\n",
            "Epoch: [2/2] | Iterations: [1302/1870] | Training loss: 0.224\n",
            "Epoch: [2/2] | Iterations: [1303/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [1304/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1305/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [1306/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [1307/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [1308/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [1309/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [1310/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1311/1870] | Training loss: 0.066\n",
            "Epoch: [2/2] | Iterations: [1312/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1313/1870] | Training loss: 0.097\n",
            "Epoch: [2/2] | Iterations: [1314/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1315/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [1316/1870] | Training loss: 0.059\n",
            "Epoch: [2/2] | Iterations: [1317/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [1318/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [1319/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [1320/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [1321/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [1322/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [1323/1870] | Training loss: 0.097\n",
            "Epoch: [2/2] | Iterations: [1324/1870] | Training loss: 0.230\n",
            "Epoch: [2/2] | Iterations: [1325/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [1326/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [1327/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [1328/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [1329/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [1330/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [1331/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [1332/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1333/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [1334/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [1335/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [1336/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1337/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1338/1870] | Training loss: 0.228\n",
            "Epoch: [2/2] | Iterations: [1339/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1340/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [1341/1870] | Training loss: 0.084\n",
            "Epoch: [2/2] | Iterations: [1342/1870] | Training loss: 0.209\n",
            "Epoch: [2/2] | Iterations: [1343/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [1344/1870] | Training loss: 0.097\n",
            "Epoch: [2/2] | Iterations: [1345/1870] | Training loss: 0.211\n",
            "Epoch: [2/2] | Iterations: [1346/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [1347/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [1348/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [1349/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1350/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [1351/1870] | Training loss: 0.189\n",
            "Epoch: [2/2] | Iterations: [1352/1870] | Training loss: 0.247\n",
            "Epoch: [2/2] | Iterations: [1353/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [1354/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [1355/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1356/1870] | Training loss: 0.190\n",
            "Epoch: [2/2] | Iterations: [1357/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [1358/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1359/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [1360/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [1361/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [1362/1870] | Training loss: 0.095\n",
            "Epoch: [2/2] | Iterations: [1363/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [1364/1870] | Training loss: 0.058\n",
            "Epoch: [2/2] | Iterations: [1365/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1366/1870] | Training loss: 0.239\n",
            "Epoch: [2/2] | Iterations: [1367/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [1368/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [1369/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [1370/1870] | Training loss: 0.159\n",
            "Epoch: [2/2] | Iterations: [1371/1870] | Training loss: 0.226\n",
            "Epoch: [2/2] | Iterations: [1372/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [1373/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [1374/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [1375/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1376/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [1377/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [1378/1870] | Training loss: 0.097\n",
            "Epoch: [2/2] | Iterations: [1379/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1380/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [1381/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [1382/1870] | Training loss: 0.233\n",
            "Epoch: [2/2] | Iterations: [1383/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1384/1870] | Training loss: 0.102\n",
            "Epoch: [2/2] | Iterations: [1385/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1386/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [1387/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [1388/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1389/1870] | Training loss: 0.050\n",
            "Epoch: [2/2] | Iterations: [1390/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [1391/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [1392/1870] | Training loss: 0.075\n",
            "Epoch: [2/2] | Iterations: [1393/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1394/1870] | Training loss: 0.247\n",
            "Epoch: [2/2] | Iterations: [1395/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [1396/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [1397/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [1398/1870] | Training loss: 0.160\n",
            "Epoch: [2/2] | Iterations: [1399/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [1400/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1401/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [1402/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [1403/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [1404/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1405/1870] | Training loss: 0.085\n",
            "Epoch: [2/2] | Iterations: [1406/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1407/1870] | Training loss: 0.071\n",
            "Epoch: [2/2] | Iterations: [1408/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [1409/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [1410/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [1411/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [1412/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [1413/1870] | Training loss: 0.199\n",
            "Epoch: [2/2] | Iterations: [1414/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [1415/1870] | Training loss: 0.253\n",
            "Epoch: [2/2] | Iterations: [1416/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [1417/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [1418/1870] | Training loss: 0.119\n",
            "Epoch: [2/2] | Iterations: [1419/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [1420/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [1421/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [1422/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [1423/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [1424/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [1425/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [1426/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1427/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1428/1870] | Training loss: 0.159\n",
            "Epoch: [2/2] | Iterations: [1429/1870] | Training loss: 0.185\n",
            "Epoch: [2/2] | Iterations: [1430/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [1431/1870] | Training loss: 0.179\n",
            "Epoch: [2/2] | Iterations: [1432/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [1433/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1434/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [1435/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [1436/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [1437/1870] | Training loss: 0.208\n",
            "Epoch: [2/2] | Iterations: [1438/1870] | Training loss: 0.075\n",
            "Epoch: [2/2] | Iterations: [1439/1870] | Training loss: 0.091\n",
            "Epoch: [2/2] | Iterations: [1440/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [1441/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [1442/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [1443/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [1444/1870] | Training loss: 0.204\n",
            "Epoch: [2/2] | Iterations: [1445/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1446/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [1447/1870] | Training loss: 0.307\n",
            "Epoch: [2/2] | Iterations: [1448/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [1449/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [1450/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [1451/1870] | Training loss: 0.253\n",
            "Epoch: [2/2] | Iterations: [1452/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [1453/1870] | Training loss: 0.150\n",
            "Epoch: [2/2] | Iterations: [1454/1870] | Training loss: 0.187\n",
            "Epoch: [2/2] | Iterations: [1455/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [1456/1870] | Training loss: 0.228\n",
            "Epoch: [2/2] | Iterations: [1457/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [1458/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1459/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [1460/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [1461/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [1462/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1463/1870] | Training loss: 0.208\n",
            "Epoch: [2/2] | Iterations: [1464/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [1465/1870] | Training loss: 0.083\n",
            "Epoch: [2/2] | Iterations: [1466/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [1467/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [1468/1870] | Training loss: 0.204\n",
            "Epoch: [2/2] | Iterations: [1469/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [1470/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [1471/1870] | Training loss: 0.190\n",
            "Epoch: [2/2] | Iterations: [1472/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [1473/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [1474/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1475/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1476/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1477/1870] | Training loss: 0.230\n",
            "Epoch: [2/2] | Iterations: [1478/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [1479/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [1480/1870] | Training loss: 0.243\n",
            "Epoch: [2/2] | Iterations: [1481/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [1482/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [1483/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [1484/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1485/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [1486/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [1487/1870] | Training loss: 0.245\n",
            "Epoch: [2/2] | Iterations: [1488/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [1489/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [1490/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [1491/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [1492/1870] | Training loss: 0.235\n",
            "Epoch: [2/2] | Iterations: [1493/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [1494/1870] | Training loss: 0.080\n",
            "Epoch: [2/2] | Iterations: [1495/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1496/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [1497/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [1498/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [1499/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [1500/1870] | Training loss: 0.233\n",
            "Epoch: [2/2] | Iterations: [1501/1870] | Training loss: 0.237\n",
            "Epoch: [2/2] | Iterations: [1502/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [1503/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [1504/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [1505/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1506/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [1507/1870] | Training loss: 0.089\n",
            "Epoch: [2/2] | Iterations: [1508/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [1509/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1510/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [1511/1870] | Training loss: 0.087\n",
            "Epoch: [2/2] | Iterations: [1512/1870] | Training loss: 0.093\n",
            "Epoch: [2/2] | Iterations: [1513/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [1514/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [1515/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [1516/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [1517/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1518/1870] | Training loss: 0.157\n",
            "Epoch: [2/2] | Iterations: [1519/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [1520/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [1521/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [1522/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [1523/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [1524/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [1525/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [1526/1870] | Training loss: 0.087\n",
            "Epoch: [2/2] | Iterations: [1527/1870] | Training loss: 0.257\n",
            "Epoch: [2/2] | Iterations: [1528/1870] | Training loss: 0.212\n",
            "Epoch: [2/2] | Iterations: [1529/1870] | Training loss: 0.076\n",
            "Epoch: [2/2] | Iterations: [1530/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1531/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [1532/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [1533/1870] | Training loss: 0.190\n",
            "Epoch: [2/2] | Iterations: [1534/1870] | Training loss: 0.195\n",
            "Epoch: [2/2] | Iterations: [1535/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [1536/1870] | Training loss: 0.122\n",
            "Epoch: [2/2] | Iterations: [1537/1870] | Training loss: 0.130\n",
            "Epoch: [2/2] | Iterations: [1538/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [1539/1870] | Training loss: 0.095\n",
            "Epoch: [2/2] | Iterations: [1540/1870] | Training loss: 0.076\n",
            "Epoch: [2/2] | Iterations: [1541/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [1542/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [1543/1870] | Training loss: 0.089\n",
            "Epoch: [2/2] | Iterations: [1544/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1545/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [1546/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [1547/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [1548/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [1549/1870] | Training loss: 0.226\n",
            "Epoch: [2/2] | Iterations: [1550/1870] | Training loss: 0.106\n",
            "Epoch: [2/2] | Iterations: [1551/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [1552/1870] | Training loss: 0.157\n",
            "Epoch: [2/2] | Iterations: [1553/1870] | Training loss: 0.210\n",
            "Epoch: [2/2] | Iterations: [1554/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [1555/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1556/1870] | Training loss: 0.260\n",
            "Epoch: [2/2] | Iterations: [1557/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [1558/1870] | Training loss: 0.109\n",
            "Epoch: [2/2] | Iterations: [1559/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [1560/1870] | Training loss: 0.137\n",
            "Epoch: [2/2] | Iterations: [1561/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [1562/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [1563/1870] | Training loss: 0.225\n",
            "Epoch: [2/2] | Iterations: [1564/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [1565/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1566/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [1567/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [1568/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [1569/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [1570/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [1571/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [1572/1870] | Training loss: 0.088\n",
            "Epoch: [2/2] | Iterations: [1573/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [1574/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [1575/1870] | Training loss: 0.202\n",
            "Epoch: [2/2] | Iterations: [1576/1870] | Training loss: 0.179\n",
            "Epoch: [2/2] | Iterations: [1577/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [1578/1870] | Training loss: 0.187\n",
            "Epoch: [2/2] | Iterations: [1579/1870] | Training loss: 0.218\n",
            "Epoch: [2/2] | Iterations: [1580/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [1581/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [1582/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [1583/1870] | Training loss: 0.218\n",
            "Epoch: [2/2] | Iterations: [1584/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [1585/1870] | Training loss: 0.113\n",
            "Epoch: [2/2] | Iterations: [1586/1870] | Training loss: 0.188\n",
            "Epoch: [2/2] | Iterations: [1587/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1588/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [1589/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [1590/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [1591/1870] | Training loss: 0.255\n",
            "Epoch: [2/2] | Iterations: [1592/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [1593/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [1594/1870] | Training loss: 0.226\n",
            "Epoch: [2/2] | Iterations: [1595/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [1596/1870] | Training loss: 0.169\n",
            "Epoch: [2/2] | Iterations: [1597/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1598/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1599/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [1600/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [1601/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [1602/1870] | Training loss: 0.081\n",
            "Epoch: [2/2] | Iterations: [1603/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [1604/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [1605/1870] | Training loss: 0.227\n",
            "Epoch: [2/2] | Iterations: [1606/1870] | Training loss: 0.229\n",
            "Epoch: [2/2] | Iterations: [1607/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [1608/1870] | Training loss: 0.218\n",
            "Epoch: [2/2] | Iterations: [1609/1870] | Training loss: 0.172\n",
            "Epoch: [2/2] | Iterations: [1610/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1611/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [1612/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1613/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [1614/1870] | Training loss: 0.068\n",
            "Epoch: [2/2] | Iterations: [1615/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [1616/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [1617/1870] | Training loss: 0.191\n",
            "Epoch: [2/2] | Iterations: [1618/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [1619/1870] | Training loss: 0.218\n",
            "Epoch: [2/2] | Iterations: [1620/1870] | Training loss: 0.213\n",
            "Epoch: [2/2] | Iterations: [1621/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [1622/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [1623/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [1624/1870] | Training loss: 0.209\n",
            "Epoch: [2/2] | Iterations: [1625/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [1626/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [1627/1870] | Training loss: 0.261\n",
            "Epoch: [2/2] | Iterations: [1628/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1629/1870] | Training loss: 0.184\n",
            "Epoch: [2/2] | Iterations: [1630/1870] | Training loss: 0.103\n",
            "Epoch: [2/2] | Iterations: [1631/1870] | Training loss: 0.126\n",
            "Epoch: [2/2] | Iterations: [1632/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [1633/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1634/1870] | Training loss: 0.197\n",
            "Epoch: [2/2] | Iterations: [1635/1870] | Training loss: 0.089\n",
            "Epoch: [2/2] | Iterations: [1636/1870] | Training loss: 0.320\n",
            "Epoch: [2/2] | Iterations: [1637/1870] | Training loss: 0.222\n",
            "Epoch: [2/2] | Iterations: [1638/1870] | Training loss: 0.117\n",
            "Epoch: [2/2] | Iterations: [1639/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1640/1870] | Training loss: 0.156\n",
            "Epoch: [2/2] | Iterations: [1641/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [1642/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1643/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1644/1870] | Training loss: 0.149\n",
            "Epoch: [2/2] | Iterations: [1645/1870] | Training loss: 0.082\n",
            "Epoch: [2/2] | Iterations: [1646/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [1647/1870] | Training loss: 0.146\n",
            "Epoch: [2/2] | Iterations: [1648/1870] | Training loss: 0.071\n",
            "Epoch: [2/2] | Iterations: [1649/1870] | Training loss: 0.115\n",
            "Epoch: [2/2] | Iterations: [1650/1870] | Training loss: 0.213\n",
            "Epoch: [2/2] | Iterations: [1651/1870] | Training loss: 0.085\n",
            "Epoch: [2/2] | Iterations: [1652/1870] | Training loss: 0.192\n",
            "Epoch: [2/2] | Iterations: [1653/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [1654/1870] | Training loss: 0.199\n",
            "Epoch: [2/2] | Iterations: [1655/1870] | Training loss: 0.262\n",
            "Epoch: [2/2] | Iterations: [1656/1870] | Training loss: 0.190\n",
            "Epoch: [2/2] | Iterations: [1657/1870] | Training loss: 0.125\n",
            "Epoch: [2/2] | Iterations: [1658/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [1659/1870] | Training loss: 0.150\n",
            "Epoch: [2/2] | Iterations: [1660/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [1661/1870] | Training loss: 0.131\n",
            "Epoch: [2/2] | Iterations: [1662/1870] | Training loss: 0.185\n",
            "Epoch: [2/2] | Iterations: [1663/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [1664/1870] | Training loss: 0.228\n",
            "Epoch: [2/2] | Iterations: [1665/1870] | Training loss: 0.185\n",
            "Epoch: [2/2] | Iterations: [1666/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [1667/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1668/1870] | Training loss: 0.106\n",
            "Epoch: [2/2] | Iterations: [1669/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1670/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [1671/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [1672/1870] | Training loss: 0.065\n",
            "Epoch: [2/2] | Iterations: [1673/1870] | Training loss: 0.226\n",
            "Epoch: [2/2] | Iterations: [1674/1870] | Training loss: 0.214\n",
            "Epoch: [2/2] | Iterations: [1675/1870] | Training loss: 0.187\n",
            "Epoch: [2/2] | Iterations: [1676/1870] | Training loss: 0.162\n",
            "Epoch: [2/2] | Iterations: [1677/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1678/1870] | Training loss: 0.058\n",
            "Epoch: [2/2] | Iterations: [1679/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [1680/1870] | Training loss: 0.271\n",
            "Epoch: [2/2] | Iterations: [1681/1870] | Training loss: 0.120\n",
            "Epoch: [2/2] | Iterations: [1682/1870] | Training loss: 0.099\n",
            "Epoch: [2/2] | Iterations: [1683/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1684/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [1685/1870] | Training loss: 0.192\n",
            "Epoch: [2/2] | Iterations: [1686/1870] | Training loss: 0.164\n",
            "Epoch: [2/2] | Iterations: [1687/1870] | Training loss: 0.173\n",
            "Epoch: [2/2] | Iterations: [1688/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [1689/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [1690/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1691/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [1692/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [1693/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1694/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1695/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1696/1870] | Training loss: 0.186\n",
            "Epoch: [2/2] | Iterations: [1697/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [1698/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [1699/1870] | Training loss: 0.095\n",
            "Epoch: [2/2] | Iterations: [1700/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1701/1870] | Training loss: 0.095\n",
            "Epoch: [2/2] | Iterations: [1702/1870] | Training loss: 0.182\n",
            "Epoch: [2/2] | Iterations: [1703/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1704/1870] | Training loss: 0.085\n",
            "Epoch: [2/2] | Iterations: [1705/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [1706/1870] | Training loss: 0.102\n",
            "Epoch: [2/2] | Iterations: [1707/1870] | Training loss: 0.167\n",
            "Epoch: [2/2] | Iterations: [1708/1870] | Training loss: 0.183\n",
            "Epoch: [2/2] | Iterations: [1709/1870] | Training loss: 0.092\n",
            "Epoch: [2/2] | Iterations: [1710/1870] | Training loss: 0.246\n",
            "Epoch: [2/2] | Iterations: [1711/1870] | Training loss: 0.228\n",
            "Epoch: [2/2] | Iterations: [1712/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1713/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1714/1870] | Training loss: 0.208\n",
            "Epoch: [2/2] | Iterations: [1715/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1716/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1717/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1718/1870] | Training loss: 0.100\n",
            "Epoch: [2/2] | Iterations: [1719/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [1720/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [1721/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [1722/1870] | Training loss: 0.219\n",
            "Epoch: [2/2] | Iterations: [1723/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1724/1870] | Training loss: 0.187\n",
            "Epoch: [2/2] | Iterations: [1725/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1726/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [1727/1870] | Training loss: 0.143\n",
            "Epoch: [2/2] | Iterations: [1728/1870] | Training loss: 0.085\n",
            "Epoch: [2/2] | Iterations: [1729/1870] | Training loss: 0.142\n",
            "Epoch: [2/2] | Iterations: [1730/1870] | Training loss: 0.135\n",
            "Epoch: [2/2] | Iterations: [1731/1870] | Training loss: 0.241\n",
            "Epoch: [2/2] | Iterations: [1732/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1733/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1734/1870] | Training loss: 0.198\n",
            "Epoch: [2/2] | Iterations: [1735/1870] | Training loss: 0.052\n",
            "Epoch: [2/2] | Iterations: [1736/1870] | Training loss: 0.078\n",
            "Epoch: [2/2] | Iterations: [1737/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [1738/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1739/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1740/1870] | Training loss: 0.197\n",
            "Epoch: [2/2] | Iterations: [1741/1870] | Training loss: 0.225\n",
            "Epoch: [2/2] | Iterations: [1742/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1743/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [1744/1870] | Training loss: 0.082\n",
            "Epoch: [2/2] | Iterations: [1745/1870] | Training loss: 0.218\n",
            "Epoch: [2/2] | Iterations: [1746/1870] | Training loss: 0.138\n",
            "Epoch: [2/2] | Iterations: [1747/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1748/1870] | Training loss: 0.136\n",
            "Epoch: [2/2] | Iterations: [1749/1870] | Training loss: 0.089\n",
            "Epoch: [2/2] | Iterations: [1750/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [1751/1870] | Training loss: 0.193\n",
            "Epoch: [2/2] | Iterations: [1752/1870] | Training loss: 0.157\n",
            "Epoch: [2/2] | Iterations: [1753/1870] | Training loss: 0.185\n",
            "Epoch: [2/2] | Iterations: [1754/1870] | Training loss: 0.144\n",
            "Epoch: [2/2] | Iterations: [1755/1870] | Training loss: 0.123\n",
            "Epoch: [2/2] | Iterations: [1756/1870] | Training loss: 0.077\n",
            "Epoch: [2/2] | Iterations: [1757/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [1758/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [1759/1870] | Training loss: 0.166\n",
            "Epoch: [2/2] | Iterations: [1760/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [1761/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1762/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [1763/1870] | Training loss: 0.219\n",
            "Epoch: [2/2] | Iterations: [1764/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [1765/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [1766/1870] | Training loss: 0.129\n",
            "Epoch: [2/2] | Iterations: [1767/1870] | Training loss: 0.194\n",
            "Epoch: [2/2] | Iterations: [1768/1870] | Training loss: 0.145\n",
            "Epoch: [2/2] | Iterations: [1769/1870] | Training loss: 0.082\n",
            "Epoch: [2/2] | Iterations: [1770/1870] | Training loss: 0.261\n",
            "Epoch: [2/2] | Iterations: [1771/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1772/1870] | Training loss: 0.076\n",
            "Epoch: [2/2] | Iterations: [1773/1870] | Training loss: 0.095\n",
            "Epoch: [2/2] | Iterations: [1774/1870] | Training loss: 0.224\n",
            "Epoch: [2/2] | Iterations: [1775/1870] | Training loss: 0.159\n",
            "Epoch: [2/2] | Iterations: [1776/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [1777/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [1778/1870] | Training loss: 0.106\n",
            "Epoch: [2/2] | Iterations: [1779/1870] | Training loss: 0.121\n",
            "Epoch: [2/2] | Iterations: [1780/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1781/1870] | Training loss: 0.204\n",
            "Epoch: [2/2] | Iterations: [1782/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [1783/1870] | Training loss: 0.148\n",
            "Epoch: [2/2] | Iterations: [1784/1870] | Training loss: 0.141\n",
            "Epoch: [2/2] | Iterations: [1785/1870] | Training loss: 0.111\n",
            "Epoch: [2/2] | Iterations: [1786/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1787/1870] | Training loss: 0.147\n",
            "Epoch: [2/2] | Iterations: [1788/1870] | Training loss: 0.127\n",
            "Epoch: [2/2] | Iterations: [1789/1870] | Training loss: 0.159\n",
            "Epoch: [2/2] | Iterations: [1790/1870] | Training loss: 0.225\n",
            "Epoch: [2/2] | Iterations: [1791/1870] | Training loss: 0.213\n",
            "Epoch: [2/2] | Iterations: [1792/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [1793/1870] | Training loss: 0.180\n",
            "Epoch: [2/2] | Iterations: [1794/1870] | Training loss: 0.254\n",
            "Epoch: [2/2] | Iterations: [1795/1870] | Training loss: 0.165\n",
            "Epoch: [2/2] | Iterations: [1796/1870] | Training loss: 0.181\n",
            "Epoch: [2/2] | Iterations: [1797/1870] | Training loss: 0.270\n",
            "Epoch: [2/2] | Iterations: [1798/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [1799/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [1800/1870] | Training loss: 0.112\n",
            "Epoch: [2/2] | Iterations: [1801/1870] | Training loss: 0.064\n",
            "Epoch: [2/2] | Iterations: [1802/1870] | Training loss: 0.196\n",
            "Epoch: [2/2] | Iterations: [1803/1870] | Training loss: 0.083\n",
            "Epoch: [2/2] | Iterations: [1804/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [1805/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [1806/1870] | Training loss: 0.096\n",
            "Epoch: [2/2] | Iterations: [1807/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [1808/1870] | Training loss: 0.246\n",
            "Epoch: [2/2] | Iterations: [1809/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1810/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [1811/1870] | Training loss: 0.163\n",
            "Epoch: [2/2] | Iterations: [1812/1870] | Training loss: 0.087\n",
            "Epoch: [2/2] | Iterations: [1813/1870] | Training loss: 0.155\n",
            "Epoch: [2/2] | Iterations: [1814/1870] | Training loss: 0.133\n",
            "Epoch: [2/2] | Iterations: [1815/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1816/1870] | Training loss: 0.210\n",
            "Epoch: [2/2] | Iterations: [1817/1870] | Training loss: 0.177\n",
            "Epoch: [2/2] | Iterations: [1818/1870] | Training loss: 0.064\n",
            "Epoch: [2/2] | Iterations: [1819/1870] | Training loss: 0.168\n",
            "Epoch: [2/2] | Iterations: [1820/1870] | Training loss: 0.076\n",
            "Epoch: [2/2] | Iterations: [1821/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [1822/1870] | Training loss: 0.128\n",
            "Epoch: [2/2] | Iterations: [1823/1870] | Training loss: 0.179\n",
            "Epoch: [2/2] | Iterations: [1824/1870] | Training loss: 0.207\n",
            "Epoch: [2/2] | Iterations: [1825/1870] | Training loss: 0.178\n",
            "Epoch: [2/2] | Iterations: [1826/1870] | Training loss: 0.200\n",
            "Epoch: [2/2] | Iterations: [1827/1870] | Training loss: 0.110\n",
            "Epoch: [2/2] | Iterations: [1828/1870] | Training loss: 0.159\n",
            "Epoch: [2/2] | Iterations: [1829/1870] | Training loss: 0.161\n",
            "Epoch: [2/2] | Iterations: [1830/1870] | Training loss: 0.132\n",
            "Epoch: [2/2] | Iterations: [1831/1870] | Training loss: 0.233\n",
            "Epoch: [2/2] | Iterations: [1832/1870] | Training loss: 0.233\n",
            "Epoch: [2/2] | Iterations: [1833/1870] | Training loss: 0.074\n",
            "Epoch: [2/2] | Iterations: [1834/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1835/1870] | Training loss: 0.134\n",
            "Epoch: [2/2] | Iterations: [1836/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [1837/1870] | Training loss: 0.236\n",
            "Epoch: [2/2] | Iterations: [1838/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [1839/1870] | Training loss: 0.174\n",
            "Epoch: [2/2] | Iterations: [1840/1870] | Training loss: 0.094\n",
            "Epoch: [2/2] | Iterations: [1841/1870] | Training loss: 0.302\n",
            "Epoch: [2/2] | Iterations: [1842/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1843/1870] | Training loss: 0.124\n",
            "Epoch: [2/2] | Iterations: [1844/1870] | Training loss: 0.104\n",
            "Epoch: [2/2] | Iterations: [1845/1870] | Training loss: 0.154\n",
            "Epoch: [2/2] | Iterations: [1846/1870] | Training loss: 0.089\n",
            "Epoch: [2/2] | Iterations: [1847/1870] | Training loss: 0.139\n",
            "Epoch: [2/2] | Iterations: [1848/1870] | Training loss: 0.176\n",
            "Epoch: [2/2] | Iterations: [1849/1870] | Training loss: 0.101\n",
            "Epoch: [2/2] | Iterations: [1850/1870] | Training loss: 0.097\n",
            "Epoch: [2/2] | Iterations: [1851/1870] | Training loss: 0.158\n",
            "Epoch: [2/2] | Iterations: [1852/1870] | Training loss: 0.090\n",
            "Epoch: [2/2] | Iterations: [1853/1870] | Training loss: 0.206\n",
            "Epoch: [2/2] | Iterations: [1854/1870] | Training loss: 0.056\n",
            "Epoch: [2/2] | Iterations: [1855/1870] | Training loss: 0.175\n",
            "Epoch: [2/2] | Iterations: [1856/1870] | Training loss: 0.102\n",
            "Epoch: [2/2] | Iterations: [1857/1870] | Training loss: 0.114\n",
            "Epoch: [2/2] | Iterations: [1858/1870] | Training loss: 0.108\n",
            "Epoch: [2/2] | Iterations: [1859/1870] | Training loss: 0.170\n",
            "Epoch: [2/2] | Iterations: [1860/1870] | Training loss: 0.068\n",
            "Epoch: [2/2] | Iterations: [1861/1870] | Training loss: 0.152\n",
            "Epoch: [2/2] | Iterations: [1862/1870] | Training loss: 0.116\n",
            "Epoch: [2/2] | Iterations: [1863/1870] | Training loss: 0.118\n",
            "Epoch: [2/2] | Iterations: [1864/1870] | Training loss: 0.087\n",
            "Epoch: [2/2] | Iterations: [1865/1870] | Training loss: 0.089\n",
            "Epoch: [2/2] | Iterations: [1866/1870] | Training loss: 0.226\n",
            "Epoch: [2/2] | Iterations: [1867/1870] | Training loss: 0.171\n",
            "Epoch: [2/2] | Iterations: [1868/1870] | Training loss: 0.153\n",
            "Epoch: [2/2] | Iterations: [1869/1870] | Training loss: 0.151\n",
            "Epoch: [2/2] | Iterations: [1870/1870] | Training loss: 0.127\n",
            "Training done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCnWSyVkbPqO"
      },
      "source": [
        "if train_model = False:\n",
        "    model.load_state_dict(torch.load(\"modelConvNet.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOXLq8OZUfmW"
      },
      "source": [
        "#This function makes predictions of the toxicity level, for all the comments in the test iterator.\n",
        "def predictions(model, test_i, first_idx, sec_idx):\n",
        "    result = []\n",
        "    #put model into evaluation mode, so that the weights are not affected.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_i):\n",
        "            batch_size = len(batch)\n",
        "            text = batch.text.view(batch_size,-1).long()\n",
        "            ids = batch.id.squeeze().cpu()\n",
        "            output = model(text)\n",
        "            output = torch.sigmoid(output).cpu()*10\n",
        "            for i,j in zip(ids,output):\n",
        "                result.append([id2.vocab.itos[i.numpy()],j.numpy()])\n",
        "    return result[first_idx][sec_idx], text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NEFqjv7_hfr",
        "outputId": "0f2c8a9b-ea07-47b9-c9fc-967e6111997c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tqdm(test_i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2394 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  0%|          | 0/2394 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y55IDbECtBqc",
        "outputId": "d3f77295-fbda-42a1-c571-8e17a7fd20f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "results, text_comment = predictions(model, test_i, 4,1)\n",
        "print(results)\n",
        "print(text_comment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 2394/2394 [03:07<00:00, 12.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-df32b10db289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_comment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_comment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGZDyE_QUwLE"
      },
      "source": [
        "minimum_text_len = 5\n",
        "def pred_toxicity(model, sentence):\n",
        "    model.eval()\n",
        "    tokens = [i.text for i in nlp.tokenizer(sentence)]\n",
        "    if len(tokens) < minimum_text_len:\n",
        "        tokens += ['<pad>'] * (minimum_text_len - len(tokens))\n",
        "    indexed = [text.vocab.stoi[w] for w in tokens]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFa_e27MuCgd",
        "outputId": "7eee6c01-e035-46fa-bf92-833a4aa634bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "comment = processing('Fuck you, Smith. Please have me notified when you die. I want to dance on your grave.')\n",
        "pred_toxicity(model,comment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2518, 0.2185, 0.1761, 0.0939, 0.1315, 0.0797]],\n",
              "       grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD7TliZTwMt9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}